[
  {
    "id": 2021016687,
    "number": 6791,
    "user": {
      "login": "holmanb",
      "id": 16310367
    },
    "diff_url": "https://github.com/psf/requests/pull/6791.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6791",
    "title": "Do not load ssl context during import",
    "body": "Possible fix for https://github.com/psf/requests/issues/6790.\r\n\r\nThis removes the ssl context load from the import path, and instead caches the context the first time that it is requested.",
    "diff": "diff --git a/src/requests/adapters.py b/src/requests/adapters.py\nindex 9a58b16025..d78f31a17e 100644\n--- a/src/requests/adapters.py\n+++ b/src/requests/adapters.py\n@@ -74,17 +74,20 @@ def SOCKSProxyManager(*args, **kwargs):\n DEFAULT_POOL_TIMEOUT = None\n \n \n-try:\n-    import ssl  # noqa: F401\n-\n-    _preloaded_ssl_context = create_urllib3_context()\n-    _preloaded_ssl_context.load_verify_locations(\n-        extract_zipped_paths(DEFAULT_CA_BUNDLE_PATH)\n-    )\n-except ImportError:\n-    # Bypass default SSLContext creation when Python\n-    # interpreter isn't built with the ssl module.\n-    _preloaded_ssl_context = None\n+_cached_ssl_context = None\n+def load_cached_ssl_context():\n+    global _cached_ssl_context\n+    try:\n+        import ssl  # noqa: F401\n+\n+        _cached_ssl_context = create_urllib3_context()\n+        _cached_ssl_context.load_verify_locations(\n+            extract_zipped_paths(DEFAULT_CA_BUNDLE_PATH)\n+        )\n+    except ImportError:\n+        # Bypass default SSLContext creation when Python\n+        # interpreter isn't built with the ssl module.\n+        _cached_ssl_context = None\n \n \n def _urllib3_request_context(\n@@ -103,15 +106,16 @@ def _urllib3_request_context(\n     # to optimize performance on standard requests.\n     poolmanager_kwargs = getattr(poolmanager, \"connection_pool_kw\", {})\n     has_poolmanager_ssl_context = poolmanager_kwargs.get(\"ssl_context\")\n+    load_cached_ssl_context()\n     should_use_default_ssl_context = (\n-        _preloaded_ssl_context is not None and not has_poolmanager_ssl_context\n+        _cached_ssl_context is not None and not has_poolmanager_ssl_context\n     )\n \n     cert_reqs = \"CERT_REQUIRED\"\n     if verify is False:\n         cert_reqs = \"CERT_NONE\"\n     elif verify is True and should_use_default_ssl_context:\n-        pool_kwargs[\"ssl_context\"] = _preloaded_ssl_context\n+        pool_kwargs[\"ssl_context\"] = _cached_ssl_context\n     elif isinstance(verify, str):\n         if not os.path.isdir(verify):\n             pool_kwargs[\"ca_certs\"] = verify\n"
  },
  {
    "id": 2008617816,
    "number": 6785,
    "user": {
      "login": "nothingface0",
      "id": 8612771
    },
    "diff_url": "https://github.com/psf/requests/pull/6785.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6785",
    "title": "Update conf.py to fix roman year",
    "body": "Very minor fix, which did however catch my eye: Fix for the invalid roman year in the documentation footer.\r\n\r\nSame as #6389, which seems to be closed. \r\n",
    "diff": "diff --git a/docs/conf.py b/docs/conf.py\nindex edbd72ba82..6c8cc85eaa 100644\n--- a/docs/conf.py\n+++ b/docs/conf.py\n@@ -58,7 +58,7 @@\n \n # General information about the project.\n project = u\"Requests\"\n-copyright = u'MMXVIX. A <a href=\"https://kenreitz.org/projects\">Kenneth Reitz</a> Project'\n+copyright = u'MMXXIV. A <a href=\"https://kenreitz.org/projects\">Kenneth Reitz</a> Project'\n author = u\"Kenneth Reitz\"\n \n # The version info for the project you're documenting, acts as replacement for\n"
  },
  {
    "id": 1990893773,
    "number": 6773,
    "user": {
      "login": "anodo123",
      "id": 40581230
    },
    "diff_url": "https://github.com/psf/requests/pull/6773.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6773",
    "title": "Clarify error description in cloning instructions",
    "body": "This clarifies the error description in the 'Cloning the repository' section of the README. The current wording mentions 'an error about a bad commit', which is somewhat vague. I've updated it to 'an error about a bad commit timestamp', which more accurately describes the nature of the error referenced in issue #2690. This small change improves the clarity of the documentation, helping users better understand the potential issue they might encounter when cloning the repository and why they need to use the specified Git flag.",
    "diff": "diff --git a/README.md b/README.md\nindex 79cf54d1e1..18d9723c7d 100644\n--- a/README.md\n+++ b/README.md\n@@ -60,7 +60,7 @@ Requests is ready for the demands of building robust and reliable HTTP\u2013speakin\n ## Cloning the repository\n \n When cloning the Requests repository, you may need to add the `-c\n-fetch.fsck.badTimezone=ignore` flag to avoid an error about a bad commit (see\n+fetch.fsck.badTimezone=ignore` flag to avoid an error about a bad commit timestamp (see\n [this issue](https://github.com/psf/requests/issues/2690) for more background):\n \n ```shell\n"
  },
  {
    "id": 1988598150,
    "number": 6772,
    "user": {
      "login": "kalingth",
      "id": 75703762
    },
    "diff_url": "https://github.com/psf/requests/pull/6772.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6772",
    "title": "chore(charset issue): Resolution for issue 6102 ",
    "body": "Resolution for the issue https://github.com/psf/requests/issues/6102\r\n\r\nNow, If a username and password are passed already encoded, they will not be affected by being expected as a string. In this case, the function will encode the string-formatted attributes into bytes using the latin-1 charset by default due to convention.",
    "diff": "diff --git a/src/requests/auth.py b/src/requests/auth.py\nindex 4a7ce6dc14..da086f6d4a 100644\n--- a/src/requests/auth.py\n+++ b/src/requests/auth.py\n@@ -123,6 +123,45 @@ def init_per_thread_state(self):\n             self._thread_local.pos = None\n             self._thread_local.num_401_calls = None\n \n+    @staticmethod\n+    def _encode_data(data, codec=\"latin-1\"):\n+        \"\"\"\n+        This function encodes input data to bytes using the specified\n+        encoding (default is Latin-1). It returns the encoded data as bytes.\n+        :rtype: tuple[bytes, Optional[str]]\n+        \"\"\"\n+        if type(data) is bytes:\n+            return data, None\n+        try:\n+            return str(data).encode(codec), codec\n+        except UnicodeEncodeError:\n+            warnings.warn(\n+                \"This data will be encoded with UTF-8 because the provided \"\n+                \"encoding could not handle some characters.\",\n+                category=UnicodeWarning,\n+            )\n+            if codec != \"utf-8\":\n+                return HTTPDigestAuth._encode_data(data, \"utf-8\")\n+            else:\n+                raise UnicodeEncodeError(\"Cannot encode the provided data...\")\n+\n+    @staticmethod\n+    def _decode_data(data, codec):\n+        \"\"\"\n+        This function decodes input data from bytes using the specified\n+        encoding. It returns the decoded data as a string.\n+        :rtype: str\n+        \"\"\"\n+        if type(data) is not bytes:\n+            return data\n+        if codec is None:\n+            warnings.warn(\n+                \"No encoding provided. The data will be decoded using UTF-8.\",\n+                category=UnicodeWarning,\n+            )\n+            codec = \"utf-8\"\n+        return data.decode(codec)\n+\n     def build_digest_header(self, method, url):\n         \"\"\"\n         :rtype: str\n@@ -186,7 +225,11 @@ def sha512_utf8(x):\n         if p_parsed.query:\n             path += f\"?{p_parsed.query}\"\n \n-        A1 = f\"{self.username}:{realm}:{self.password}\"\n+        username, username_codec = self._encode_data(self.username)\n+        realm, realm_codec = self._encode_data(realm)\n+        password, _ = self._encode_data(self.password)\n+\n+        A1 = b\":\".join([username, realm, password])\n         A2 = f\"{method}:{path}\"\n \n         HA1 = hash_utf8(A1)\n@@ -218,9 +261,11 @@ def sha512_utf8(x):\n         self._thread_local.last_nonce = nonce\n \n         # XXX should the partial digests be encoded too?\n+        decoded_username = self._decode_data(username, username_codec)\n+        decoded_realm = self._decode_data(realm, realm_codec)\n         base = (\n-            f'username=\"{self.username}\", realm=\"{realm}\", nonce=\"{nonce}\", '\n-            f'uri=\"{path}\", response=\"{respdig}\"'\n+            f'username=\"{decoded_username}\", realm=\"{decoded_realm}\", '\n+            f'nonce=\"{nonce}\", uri=\"{path}\", response=\"{respdig}\"'\n         )\n         if opaque:\n             base += f', opaque=\"{opaque}\"'\ndiff --git a/tests/test_auth.py b/tests/test_auth.py\nnew file mode 100644\nindex 0000000000..a0328e7f31\n--- /dev/null\n+++ b/tests/test_auth.py\n@@ -0,0 +1,27 @@\n+import pytest\n+\n+# from requests.auth import HTTPDigestAuth\n+from src.requests.auth import HTTPDigestAuth\n+\n+\n+class TestDigestAuth:\n+    def _build_a_digest_auth(self, user, password):\n+        auth = HTTPDigestAuth(user, password)\n+        auth.init_per_thread_state()\n+        auth._thread_local.chal[\"realm\"] = \"eggs\"\n+        auth._thread_local.chal[\"nonce\"] = \"chips\"\n+        return auth.build_digest_header(\"GET\", \"https://www.example.com/\")\n+\n+    @pytest.mark.parametrize(\n+        \"username, password\",\n+        (\n+            (\"spam\", \"ham\"),\n+            (\"\u0438\u043c\u044f\", \"\u043f\u0430\u0440\u043e\u043b\u044c\"),\n+        ),\n+    )\n+    def test_digestauth_encode_consistency(self, username, password):\n+        auth = username, password\n+        str_auth = self._build_a_digest_auth(*auth)\n+        bauth = username.encode(\"utf-8\"), password.encode(\"utf-8\")\n+        bin_auth = self._build_a_digest_auth(*bauth)\n+        assert str_auth == bin_auth\n"
  },
  {
    "id": 1986576459,
    "number": 6770,
    "user": {
      "login": "vilhelmen",
      "id": 5308250
    },
    "diff_url": "https://github.com/psf/requests/pull/6770.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6770",
    "title": "Header-based auth class",
    "body": "This adds a short and simple header-based auth backend (and tests!). This allows greater flexibility in auth mechanisms out of the box.\r\n\r\nFor example, I'm working on on an internal library to interface with OGC services and three providers use three different auth mechanisms. Being able to consolidate this via the auth parameter seems like the correct solution.\r\n\r\nIt's worth noting that because of when this executes, auth objects override the headers provided both by the session and request call. This has been appropriately documented, and I don't believe any other option would be preferable.\r\n\r\nLooks like someone proposed this in 2018 (#4823) but was rejected because of a feature freeze at the time.\r\n\r\nIf this is acceptable, would the team be interested in something similar like HTTPParamAuth that uses a dict of parameters in a similar fashion? Going back to my library example, one provider uses oauth, which requests supports, one uses headers, and one uses a custom api key parameter. Totally willing to add it to this PR if it simplifies approval :)",
    "diff": "diff --git a/docs/user/authentication.rst b/docs/user/authentication.rst\nindex 0737bd319a..5d29bb6094 100644\n--- a/docs/user/authentication.rst\n+++ b/docs/user/authentication.rst\n@@ -32,6 +32,27 @@ for using it::\n Providing the credentials in a tuple like this is exactly the same as the\n ``HTTPBasicAuth`` example above.\n \n+Header Authentication\n+--------------------\n+\n+Some services require authentication data in the header of the request.\n+Multiple headers can be added to an authentication object to keep them separate\n+from request data::\n+\n+    >>> from requests.auth import HTTPHeaderAuth\n+    >>> auth = HTTPHeaderAuth({'Api-Key': '1234567890ABCDEF'}})\n+    >>> response = requests.get('https://httpbin.org/headers', auth=auth)\n+    >>> response.json()['headers']['Api-Key']\n+    '1234567890abcdef'\n+\n+Be aware that keys in the authentication object will override headers set by\n+the current request or session parameters::\n+\n+    >>> from requests.auth import HTTPHeaderAuth\n+    >>> auth = HTTPHeaderAuth({'Api-Key': '1234567890ABCDEF'}})\n+    >>> response = requests.get('https://httpbin.org/headers', headers={'Api-Key': '0000000000'}, auth=auth)\n+    >>> response.json()['headers']['Api-Key']\n+    '1234567890ABCDEF'\n \n netrc Authentication\n ~~~~~~~~~~~~~~~~~~~~\ndiff --git a/src/requests/auth.py b/src/requests/auth.py\nindex 4a7ce6dc14..5a00b63264 100644\n--- a/src/requests/auth.py\n+++ b/src/requests/auth.py\n@@ -104,6 +104,17 @@ def __call__(self, r):\n         return r\n \n \n+class HTTPHeaderAuth(AuthBase):\n+    \"\"\"Attaches authentication headers to the given Request object.\"\"\"\n+\n+    def __init__(self, headers):\n+        self.headers = headers\n+\n+    def __call__(self, r):\n+        r.headers.update(self.headers)\n+        return r\n+\n+\n class HTTPDigestAuth(AuthBase):\n     \"\"\"Attaches HTTP Digest Authentication to the given Request object.\"\"\"\n \ndiff --git a/tests/test_requests.py b/tests/test_requests.py\nindex b4e9fe92ae..bf347c6175 100644\n--- a/tests/test_requests.py\n+++ b/tests/test_requests.py\n@@ -17,7 +17,7 @@\n \n import requests\n from requests.adapters import HTTPAdapter\n-from requests.auth import HTTPDigestAuth, _basic_auth_str\n+from requests.auth import HTTPDigestAuth, HTTPHeaderAuth, _basic_auth_str\n from requests.compat import (\n     JSONDecodeError,\n     Morsel,\n@@ -703,6 +703,43 @@ def get_netrc_auth_mock(url):\n         finally:\n             requests.sessions.get_netrc_auth = old_auth\n \n+    def test_header_auth(self, httpbin):\n+        header_key = \"Test-Header\"\n+        header_value = \"1234567890ABCDEF\"\n+        auth = HTTPHeaderAuth({header_key: header_value})\n+        url = httpbin(\"headers\")\n+\n+        # Check header exists\n+        r = requests.get(url, auth=auth)\n+        assert r.json()[\"headers\"][header_key] == header_value\n+\n+        # Make sure it's not a fluke\n+        r = requests.get(url)\n+        assert header_key not in r.json()[\"headers\"]\n+\n+        # Verify auth header overrides provided headers\n+        #  (not strictly a feature, but it's current behavior)\n+        second_header_value = \"NOT_RETURNED\"\n+        r = requests.get(url, headers={header_key: second_header_value}, auth=auth)\n+        assert r.json()[\"headers\"][header_key] == header_value\n+\n+        # Test with session\n+        s = requests.session()\n+        s.auth = auth\n+        r = s.get(url)\n+        assert r.json()[\"headers\"][header_key] == header_value\n+\n+        # verify session header override\n+        r = s.get(url, headers={header_key: second_header_value})\n+        assert r.json()[\"headers\"][header_key] == header_value\n+\n+        # Check sure multiple headers works\n+        header_keys = (\"Header-One\", \"Header-Two\")\n+        auth = HTTPHeaderAuth({key: key for key in header_keys})\n+        r = requests.get(url, auth=auth)\n+        returned_keys = r.json()[\"headers\"].keys()\n+        assert all(key in returned_keys for key in header_keys)\n+\n     def test_DIGEST_HTTP_200_OK_GET(self, httpbin):\n         for authtype in self.digest_auth_algo:\n             auth = HTTPDigestAuth(\"user\", \"pass\")\n"
  },
  {
    "id": 1976560446,
    "number": 6767,
    "user": {
      "login": "nateprewitt",
      "id": 5271761
    },
    "diff_url": "https://github.com/psf/requests/pull/6767.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6767",
    "title": "Revert caching a default SSLContext",
    "body": "This PR reverts the changes from #6667 to the previous behavior. Due to the number of edge cases and concurrency issues we've encountered with this change, we've decided the benefit doesn't currently outweigh the pain to existing infrastructure. We've iterated on a few tries to keep this functionality in place, but are still receiving reports of novel issues with this behavior.\r\n\r\nWe may be able to revisit this in a later version of Requests but we'll need a much more comprehensive test plan.",
    "diff": "diff --git a/src/requests/adapters.py b/src/requests/adapters.py\nindex 9a58b16025..523505a02a 100644\n--- a/src/requests/adapters.py\n+++ b/src/requests/adapters.py\n@@ -27,7 +27,6 @@\n from urllib3.util import Timeout as TimeoutSauce\n from urllib3.util import parse_url\n from urllib3.util.retry import Retry\n-from urllib3.util.ssl_ import create_urllib3_context\n \n from .auth import _basic_auth_str\n from .compat import basestring, urlparse\n@@ -74,19 +73,6 @@ def SOCKSProxyManager(*args, **kwargs):\n DEFAULT_POOL_TIMEOUT = None\n \n \n-try:\n-    import ssl  # noqa: F401\n-\n-    _preloaded_ssl_context = create_urllib3_context()\n-    _preloaded_ssl_context.load_verify_locations(\n-        extract_zipped_paths(DEFAULT_CA_BUNDLE_PATH)\n-    )\n-except ImportError:\n-    # Bypass default SSLContext creation when Python\n-    # interpreter isn't built with the ssl module.\n-    _preloaded_ssl_context = None\n-\n-\n def _urllib3_request_context(\n     request: \"PreparedRequest\",\n     verify: \"bool | str | None\",\n@@ -99,19 +85,9 @@ def _urllib3_request_context(\n     scheme = parsed_request_url.scheme.lower()\n     port = parsed_request_url.port\n \n-    # Determine if we have and should use our default SSLContext\n-    # to optimize performance on standard requests.\n-    poolmanager_kwargs = getattr(poolmanager, \"connection_pool_kw\", {})\n-    has_poolmanager_ssl_context = poolmanager_kwargs.get(\"ssl_context\")\n-    should_use_default_ssl_context = (\n-        _preloaded_ssl_context is not None and not has_poolmanager_ssl_context\n-    )\n-\n     cert_reqs = \"CERT_REQUIRED\"\n     if verify is False:\n         cert_reqs = \"CERT_NONE\"\n-    elif verify is True and should_use_default_ssl_context:\n-        pool_kwargs[\"ssl_context\"] = _preloaded_ssl_context\n     elif isinstance(verify, str):\n         if not os.path.isdir(verify):\n             pool_kwargs[\"ca_certs\"] = verify\n@@ -314,26 +290,27 @@ def cert_verify(self, conn, url, verify, cert):\n         :param cert: The SSL certificate to verify.\n         \"\"\"\n         if url.lower().startswith(\"https\") and verify:\n-            conn.cert_reqs = \"CERT_REQUIRED\"\n+            cert_loc = None\n \n-            # Only load the CA certificates if 'verify' is a string indicating the CA bundle to use.\n-            # Otherwise, if verify is a boolean, we don't load anything since\n-            # the connection will be using a context with the default certificates already loaded,\n-            # and this avoids a call to the slow load_verify_locations()\n+            # Allow self-specified cert location.\n             if verify is not True:\n-                # `verify` must be a str with a path then\n                 cert_loc = verify\n \n-                if not os.path.exists(cert_loc):\n-                    raise OSError(\n-                        f\"Could not find a suitable TLS CA certificate bundle, \"\n-                        f\"invalid path: {cert_loc}\"\n-                    )\n+            if not cert_loc:\n+                cert_loc = extract_zipped_paths(DEFAULT_CA_BUNDLE_PATH)\n \n-                if not os.path.isdir(cert_loc):\n-                    conn.ca_certs = cert_loc\n-                else:\n-                    conn.ca_cert_dir = cert_loc\n+            if not cert_loc or not os.path.exists(cert_loc):\n+                raise OSError(\n+                    f\"Could not find a suitable TLS CA certificate bundle, \"\n+                    f\"invalid path: {cert_loc}\"\n+                )\n+\n+            conn.cert_reqs = \"CERT_REQUIRED\"\n+\n+            if not os.path.isdir(cert_loc):\n+                conn.ca_certs = cert_loc\n+            else:\n+                conn.ca_cert_dir = cert_loc\n         else:\n             conn.cert_reqs = \"CERT_NONE\"\n             conn.ca_certs = None\n"
  },
  {
    "id": 1935569735,
    "number": 6750,
    "user": {
      "login": "eaglegai",
      "id": 31752768
    },
    "diff_url": "https://github.com/psf/requests/pull/6750.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6750",
    "title": "catch FileNotFoundError if there aren't any certificates",
    "body": "fix https://github.com/psf/requests/issues/6749",
    "diff": "diff --git a/src/requests/adapters.py b/src/requests/adapters.py\nindex 9a58b16025..d7f8716e4e 100644\n--- a/src/requests/adapters.py\n+++ b/src/requests/adapters.py\n@@ -85,6 +85,9 @@ def SOCKSProxyManager(*args, **kwargs):\n     # Bypass default SSLContext creation when Python\n     # interpreter isn't built with the ssl module.\n     _preloaded_ssl_context = None\n+except FileNotFoundError:\n+    # Bypass if there aren't any certificates\n+    _preloaded_ssl_context = None\n \n \n def _urllib3_request_context(\n"
  },
  {
    "id": 1898493939,
    "number": 6731,
    "user": {
      "login": "nateprewitt",
      "id": 5271761
    },
    "diff_url": "https://github.com/psf/requests/pull/6731.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6731",
    "title": "Address certificate loading regression",
    "body": "## Overview\r\nThis PR is intended to address two distinct issues introduced with the default cert optimizations originally introduced in 2.32.0. While we continue to refine the settings considered when opting into our optimized context, we'll no longer use the new default if any custom cert values are supplied. This addresses the concurrency issues raised in #6726.\r\n\r\nThe second piece of this will be ensuring that when opting out of the default SSLContext, we're still supplying to the default CA Cert bundle correctly. This addresses the problems noticed in https://github.com/psf/requests/pull/6710#issuecomment-2137802782 and #6730.\r\n\r\n## Considerations\r\n\r\nWe're now duplicating a decent chunk of the [logic from cert_verify](https://github.com/psf/requests/blob/0e322af87745eff34caffe4df68456ebc20d9068/src/requests/adapters.py#L324-L357) inside [_urllib3_request_context](https://github.com/psf/requests/blob/0e322af87745eff34caffe4df68456ebc20d9068/src/requests/adapters.py#L115-L128) but without our validation exceptions. That's a potential vector for behavioral shifts in the future. We _could_ consolidate some of this behavior in one place but it's going to require constructing a dict and using `setattr` on our `conn` in `cert_verify` while setting `pool_kwargs` in `_urllib3_request_context`. I started writing that up but it feels clunky. This is probably going to be a tradeoff of risking drift like we have with Session settings and binding the two behaviors together too tightly.\r\n\r\n## Testing\r\nI'd like to codify the issues we've encountered through the whole 2.32.x saga in tests to hopefully avoid this in the future. Doing it cleanly without relying on external endpoints is proving to be a bit more involved than I'd like. I think we can harness some of the infrastructure added in #6662, but I haven't had a chance to really dig into that.",
    "diff": "diff --git a/src/requests/adapters.py b/src/requests/adapters.py\nindex 9a58b16025..54143f9e6b 100644\n--- a/src/requests/adapters.py\n+++ b/src/requests/adapters.py\n@@ -87,6 +87,23 @@ def SOCKSProxyManager(*args, **kwargs):\n     _preloaded_ssl_context = None\n \n \n+def _should_use_default_context(\n+    verify: \"bool | str | None\",\n+    client_cert: \"typing.Tuple[str, str] | str | None\",\n+    poolmanager_kwargs: typing.Dict[str, typing.Any],\n+) -> bool:\n+    # Determine if we have and should use our default SSLContext\n+    # to optimize performance on standard requests.\n+    has_poolmanager_ssl_context = poolmanager_kwargs.get(\"ssl_context\")\n+    should_use_default_ssl_context = (\n+        verify is True\n+        and _preloaded_ssl_context is not None\n+        and not has_poolmanager_ssl_context\n+        and client_cert is None\n+    )\n+    return should_use_default_ssl_context\n+\n+\n def _urllib3_request_context(\n     request: \"PreparedRequest\",\n     verify: \"bool | str | None\",\n@@ -98,25 +115,26 @@ def _urllib3_request_context(\n     parsed_request_url = urlparse(request.url)\n     scheme = parsed_request_url.scheme.lower()\n     port = parsed_request_url.port\n-\n-    # Determine if we have and should use our default SSLContext\n-    # to optimize performance on standard requests.\n     poolmanager_kwargs = getattr(poolmanager, \"connection_pool_kw\", {})\n-    has_poolmanager_ssl_context = poolmanager_kwargs.get(\"ssl_context\")\n-    should_use_default_ssl_context = (\n-        _preloaded_ssl_context is not None and not has_poolmanager_ssl_context\n-    )\n \n     cert_reqs = \"CERT_REQUIRED\"\n+    cert_loc = None\n     if verify is False:\n         cert_reqs = \"CERT_NONE\"\n-    elif verify is True and should_use_default_ssl_context:\n+    elif _should_use_default_context(verify, client_cert, poolmanager_kwargs):\n         pool_kwargs[\"ssl_context\"] = _preloaded_ssl_context\n+    elif verify is True:\n+        # Set default ca cert location if none provided\n+        cert_loc = extract_zipped_paths(DEFAULT_CA_BUNDLE_PATH)\n     elif isinstance(verify, str):\n-        if not os.path.isdir(verify):\n-            pool_kwargs[\"ca_certs\"] = verify\n+        cert_loc = verify\n+\n+    if cert_loc is not None:\n+        if not os.path.isdir(cert_loc):\n+            pool_kwargs[\"ca_certs\"] = cert_loc\n         else:\n-            pool_kwargs[\"ca_cert_dir\"] = verify\n+            pool_kwargs[\"ca_cert_dir\"] = cert_loc\n+\n     pool_kwargs[\"cert_reqs\"] = cert_reqs\n     if client_cert is not None:\n         if isinstance(client_cert, tuple) and len(client_cert) == 2:\n@@ -316,10 +334,8 @@ def cert_verify(self, conn, url, verify, cert):\n         if url.lower().startswith(\"https\") and verify:\n             conn.cert_reqs = \"CERT_REQUIRED\"\n \n-            # Only load the CA certificates if 'verify' is a string indicating the CA bundle to use.\n-            # Otherwise, if verify is a boolean, we don't load anything since\n-            # the connection will be using a context with the default certificates already loaded,\n-            # and this avoids a call to the slow load_verify_locations()\n+            # Only load the CA certificates if `verify` is a\n+            # string indicating the CA bundle to use.\n             if verify is not True:\n                 # `verify` must be a str with a path then\n                 cert_loc = verify\n"
  },
  {
    "id": 1885254790,
    "number": 6720,
    "user": {
      "login": "nateprewitt",
      "id": 5271761
    },
    "diff_url": "https://github.com/psf/requests/pull/6720.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6720",
    "title": "Start testing on 3.13 beta",
    "body": "It looks like we're currently blocked on wheels for a few testing dependencies so this isn't mergable currently but we'll use this PR to monitor upstream support from other projects. Once those blockers are moved, we can merge this into our CI to prepare for the upcoming release candidates.\r\n\r\nCurrently it looks like `flsgger` and `greenlet` are having issues from httpbin. I think we have an open proposal/request to make `flsgger` optional which seems fine. `greenlet` currently has a proposed patch from Victor Stinner so I would assume that's resolved sooner than later.",
    "diff": "diff --git a/.github/workflows/run-tests.yml b/.github/workflows/run-tests.yml\nindex c35af968c4..674df73120 100644\n--- a/.github/workflows/run-tests.yml\n+++ b/.github/workflows/run-tests.yml\n@@ -12,7 +12,7 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        python-version: [\"3.8\", \"3.9\", \"3.10\", \"3.11\", \"3.12\", \"pypy-3.9\", \"pypy-3.10\"]\n+        python-version: [\"3.8\", \"3.9\", \"3.10\", \"3.11\", \"3.12\", \"3.13-dev\", \"pypy-3.9\", \"pypy-3.10\"]\n         os: [ubuntu-22.04, macOS-latest, windows-latest]\n         # Python 3.8 and 3.9 do not run on macOS-latest which\n         # is now using arm64 hardware.\n"
  },
  {
    "id": 1884365001,
    "number": 6718,
    "user": {
      "login": "domx4q",
      "id": 39739545
    },
    "diff_url": "https://github.com/psf/requests/pull/6718.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6718",
    "title": "grammar fixes",
    "body": "This pull request contains a few grammatical corrections. The changes aim to improve the clarity and readability of the documentation and comments within the code.",
    "diff": "diff --git a/.github/CONTRIBUTING.md b/.github/CONTRIBUTING.md\nindex 3470dfee83..2f163060e9 100644\n--- a/.github/CONTRIBUTING.md\n+++ b/.github/CONTRIBUTING.md\n@@ -50,5 +50,5 @@ Please be aware of the following things when filing bug reports:\n      ship patches on top of the code we supply.\n \n    If you do not provide all of these things, it will take us much longer to\n-   fix your problem. If we ask you to clarify these and you never respond, we\n+   fix your problem. If we ask you to clarify these, and you never respond, we\n    will close your issue without fixing it.\ndiff --git a/.github/SECURITY.md b/.github/SECURITY.md\nindex 9021d429d8..d149b9d192 100644\n--- a/.github/SECURITY.md\n+++ b/.github/SECURITY.md\n@@ -16,7 +16,7 @@ profiles above if desired.\n \n If English is not your first language, please try to describe the\n problem and its impact to the best of your ability. For greater detail,\n-please use your native language and we will try our best to translate it\n+please use your native language, and we will try our best to translate it\n using online services.\n \n Please also include the code you used to find the problem and the\n@@ -65,11 +65,11 @@ number if one is required, providing you with full credit for the\n discovery. We will also decide on a planned release date, and let you\n know when it is. This release date will *always* be on a weekday.\n \n-At this point we will reach out to our major downstream packagers to\n-notify them of an impending security-related patch so they can make\n+At this point, we will reach out to our major downstream packagers to\n+notify them of an impending security-related patch, so they can make\n arrangements. In addition, these packagers will be provided with the\n intended patch ahead of time, to ensure that they are able to promptly\n-release their downstream packages. Currently the list of people we\n+release their downstream packages. Currently, the list of people we\n actively contact *ahead of a public release* is:\n \n -   Jeremy Cline, Red Hat (@jeremycline)\ndiff --git a/.github/workflows/codeql-analysis.yml b/.github/workflows/codeql-analysis.yml\nindex b6d544640b..917180eaa3 100644\n--- a/.github/workflows/codeql-analysis.yml\n+++ b/.github/workflows/codeql-analysis.yml\n@@ -35,7 +35,7 @@ jobs:\n       uses: actions/checkout@8ade135a41bc03ea155e62e844d188df1ea18608 # v4.1.0\n       with:\n         # We must fetch at least the immediate parents so that if this is\n-        # a pull request then we can checkout the head.\n+        # a pull request, then we can checkout the head.\n         fetch-depth: 2\n \n     # If this run was triggered by a pull request event, then checkout\ndiff --git a/docs/conf.py b/docs/conf.py\nindex edbd72ba82..5e65d1f780 100644\n--- a/docs/conf.py\n+++ b/docs/conf.py\n@@ -335,7 +335,7 @@\n # The scheme of the identifier. Typical schemes are ISBN or URL.\n # epub_scheme = ''\n \n-# The unique identifier of the text. This can be a ISBN number\n+# The unique identifier of the text. This can be an ISBN\n # or the project homepage.\n # epub_identifier = ''\n \ndiff --git a/src/requests/adapters.py b/src/requests/adapters.py\nindex 42fabe527c..266982a91e 100644\n--- a/src/requests/adapters.py\n+++ b/src/requests/adapters.py\n@@ -407,7 +407,7 @@ def get_connection_with_tls_context(self, request, verify, proxies=None, cert=No\n                 **host_params, pool_kwargs=pool_kwargs\n             )\n         else:\n-            # Only scheme should be lower case\n+            # Only scheme should be lowercase\n             conn = self.poolmanager.connection_from_host(\n                 **host_params, pool_kwargs=pool_kwargs\n             )\n@@ -448,7 +448,7 @@ def get_connection(self, url, proxies=None):\n             proxy_manager = self.proxy_manager_for(proxy)\n             conn = proxy_manager.connection_from_url(url)\n         else:\n-            # Only scheme should be lower case\n+            # Only scheme should be lowercase\n             parsed = urlparse(url)\n             url = parsed.geturl()\n             conn = self.poolmanager.connection_from_url(url)\ndiff --git a/src/requests/auth.py b/src/requests/auth.py\nindex 4a7ce6dc14..ba98e9fac4 100644\n--- a/src/requests/auth.py\n+++ b/src/requests/auth.py\n@@ -26,7 +26,7 @@ def _basic_auth_str(username, password):\n     \"\"\"Returns a Basic Auth string.\"\"\"\n \n     # \"I want us to put a big-ol' comment on top of it that\n-    # says that this behaviour is dumb but we need to preserve\n+    # says that this behaviour is dumb, but we need to preserve\n     # it because people are relying on it.\"\n     #    - Lukasa\n     #\ndiff --git a/src/requests/models.py b/src/requests/models.py\nindex 8f56ca7d23..ee5bf6f42a 100644\n--- a/src/requests/models.py\n+++ b/src/requests/models.py\n@@ -422,7 +422,7 @@ def prepare_url(self, url, params):\n         url = url.lstrip()\n \n         # Don't do any URL preparation for non-HTTP schemes like `mailto`,\n-        # `data` etc to work around exceptions from `url_parse`, which\n+        # `data` etc. to work around exceptions from `url_parse`, which\n         # handles RFC 3986 only.\n         if \":\" in url and not url.lower().startswith(\"http\"):\n             self.url = url\n@@ -494,8 +494,8 @@ def prepare_headers(self, headers):\n     def prepare_body(self, data, files, json=None):\n         \"\"\"Prepares the given HTTP body data.\"\"\"\n \n-        # Check if file, fo, generator, iterator.\n-        # If not, run through normal process.\n+        # Check if it's a file, fo, generator, iterator.\n+        # If not, run through the normal process.\n \n         # Nottin' on you.\n         body = None\n@@ -956,7 +956,7 @@ def json(self, **kwargs):\n             # No encoding set. JSON RFC 4627 section 3 states we should expect\n             # UTF-8, -16 or -32. Detect which one to use; If the detection or\n             # decoding fails, fall back to `self.text` (using charset_normalizer to make\n-            # a best guess).\n+            # the best guess).\n             encoding = guess_json_utf(self.content)\n             if encoding is not None:\n                 try:\ndiff --git a/src/requests/sessions.py b/src/requests/sessions.py\nindex b387bc36df..230dd67e26 100644\n--- a/src/requests/sessions.py\n+++ b/src/requests/sessions.py\n@@ -438,7 +438,7 @@ def __init__(self):\n         self.trust_env = True\n \n         #: A CookieJar containing all currently outstanding cookies set on this\n-        #: session. By default it is a\n+        #: session. By default, it is a\n         #: :class:`RequestsCookieJar <requests.cookies.RequestsCookieJar>`, but\n         #: may be any other ``cookielib.CookieJar`` compatible object.\n         self.cookies = cookiejar_from_dict({})\ndiff --git a/src/requests/structures.py b/src/requests/structures.py\nindex 188e13e482..9f6005587b 100644\n--- a/src/requests/structures.py\n+++ b/src/requests/structures.py\n@@ -21,7 +21,7 @@ class CaseInsensitiveDict(MutableMapping):\n     case of the last key to be set, and ``iter(instance)``,\n     ``keys()``, ``items()``, ``iterkeys()``, and ``iteritems()``\n     will contain case-sensitive keys. However, querying and contains\n-    testing is case insensitive::\n+    testing is case-insensitive::\n \n         cid = CaseInsensitiveDict()\n         cid['Accept'] = 'application/json'\ndiff --git a/src/requests/utils.py b/src/requests/utils.py\nindex ae6c42f6cb..41b3a05498 100644\n--- a/src/requests/utils.py\n+++ b/src/requests/utils.py\n@@ -449,16 +449,16 @@ def unquote_header_value(value, is_filename=False):\n     \"\"\"\n     if value and value[0] == value[-1] == '\"':\n         # this is not the real unquoting, but fixing this so that the\n-        # RFC is met will result in bugs with internet explorer and\n-        # probably some other browsers as well.  IE for example is\n+        # RFC is met will result in bugs with Internet Explorer and\n+        # probably some other browsers as well. IE, for example, is\n         # uploading files with \"C:\\foo\\bar.txt\" as filename\n         value = value[1:-1]\n \n-        # if this is a filename and the starting characters look like\n-        # a UNC path, then just return the value without quotes.  Using the\n+        # If this is a filename and the starting characters look like\n+        # a UNC path, then just return the value without quotes. Using the\n         # replace sequence below on a UNC path has the effect of turning\n         # the leading double slash into a single slash and then\n-        # _fix_ie_filename() doesn't work correctly.  See #458.\n+        # _fix_ie_filename() doesn't work correctly. See #458.\n         if not is_filename or value[:2] != \"\\\\\\\\\":\n             return value.replace(\"\\\\\\\\\", \"\\\\\").replace('\\\\\"', '\"')\n     return value\n@@ -675,7 +675,7 @@ def requote_uri(uri):\n     except InvalidURL:\n         # We couldn't unquote the given URI, so let's try quoting it, but\n         # there may be unquoted '%'s in the URI. We need to make sure they're\n-        # properly quoted so they do not cause issues elsewhere.\n+        # properly quoted, so they do not cause issues elsewhere.\n         return quote(uri, safe=safe_without_percent)\n \n \n@@ -1004,7 +1004,7 @@ def prepend_scheme_if_needed(url, new_scheme):\n         netloc, path = path, netloc\n \n     if auth:\n-        # parse_url doesn't provide the netloc with auth\n+        # parse_url doesn't provide the netloc with auth,\n         # so we'll add it ourselves.\n         netloc = \"@\".join([auth, netloc])\n     if scheme is None:\n@@ -1016,8 +1016,8 @@ def prepend_scheme_if_needed(url, new_scheme):\n \n \n def get_auth_from_url(url):\n-    \"\"\"Given a url with authentication components, extract them into a tuple of\n-    username,password.\n+    \"\"\"Given a URL with authentication components, extract them into a tuple of\n+    (username, password).\n \n     :rtype: (str,str)\n     \"\"\"\n@@ -1063,7 +1063,7 @@ def _validate_header_part(header, header_part, header_validator_index):\n \n def urldefragauth(url):\n     \"\"\"\n-    Given a url remove the fragment and the authentication part.\n+    Given a URL remove the fragment and the authentication part.\n \n     :rtype: str\n     \"\"\"\n@@ -1079,7 +1079,7 @@ def urldefragauth(url):\n \n \n def rewind_body(prepared_request):\n-    \"\"\"Move file pointer back to its recorded starting position\n+    \"\"\"Move file pointer back to its recorded starting position,\n     so it can be read again on redirect.\n     \"\"\"\n     body_seek = getattr(prepared_request.body, \"seek\", None)\ndiff --git a/tests/certs/README.md b/tests/certs/README.md\nindex 4bf7002e0b..e947ec160d 100644\n--- a/tests/certs/README.md\n+++ b/tests/certs/README.md\n@@ -6,5 +6,5 @@ behaviour.\n The certificates include:\n \n * [expired](./expired) server certificate with a valid certificate authority\n-* [mtls](./mtls) provides a valid client certificate with a 2 year validity\n+* [mtls](./mtls) provides a valid client certificate with 2-year validity\n * [valid](./valid) has a valid server certificate\ndiff --git a/tests/test_requests.py b/tests/test_requests.py\nindex b4e9fe92ae..9254e7a286 100644\n--- a/tests/test_requests.py\n+++ b/tests/test_requests.py\n@@ -583,7 +583,7 @@ def test_errors(self, url, exception):\n             requests.get(url, timeout=1)\n \n     def test_proxy_error(self):\n-        # any proxy related error (address resolution, no route to host, etc) should result in a ProxyError\n+        # any proxy related error (address resolution, no route to host, etc.) should result in a ProxyError\n         with pytest.raises(ProxyError):\n             requests.get(\n                 \"http://localhost:1\", proxies={\"http\": \"non-resolvable-address\"}\n@@ -1470,7 +1470,7 @@ def test_response_chunk_size_type(self):\n     def test_iter_content_wraps_exceptions(self, httpbin, exception, args, expected):\n         r = requests.Response()\n         r.raw = mock.Mock()\n-        # ReadTimeoutError can't be initialized by mock\n+        # ReadTimeoutError can't be initialized by mock,\n         # so we'll manually create the instance with args\n         r.raw.stream.side_effect = exception(*args)\n \n@@ -2216,7 +2216,7 @@ def get_redirect_target(self, resp):\n                 # default behavior\n                 if resp.is_redirect:\n                     return resp.headers[\"location\"]\n-                # edge case - check to see if 'location' is in headers anyways\n+                # edge case - check to see if 'location' is in headers anyway\n                 location = resp.headers.get(\"location\")\n                 if location and (location != resp.url):\n                     return location\n@@ -2431,7 +2431,7 @@ def test_max_age_valid_int(self):\n         assert isinstance(cookie.expires, int)\n \n     def test_max_age_invalid_str(self):\n-        \"\"\"Test case where a invalid max age is passed.\"\"\"\n+        \"\"\"Test case where an invalid max age is passed.\"\"\"\n \n         morsel = Morsel()\n         morsel[\"max-age\"] = \"woops\"\n@@ -2465,7 +2465,7 @@ def test_none_timeout(self, httpbin, timeout):\n         To actually test this behavior, we'd want to check that setting the\n         timeout to None actually lets the request block past the system default\n         timeout. However, this would make the test suite unbearably slow.\n-        Instead we verify that setting the timeout to None does not prevent the\n+        Instead, we verify that setting the timeout to None does not prevent the\n         request from succeeding.\n         \"\"\"\n         r = requests.get(httpbin(\"get\"), timeout=timeout)\n@@ -2887,7 +2887,7 @@ def response_handler(sock):\n             r1 = s.get(url, verify=False)\n             assert r1.status_code == 200\n \n-            # Has right trust bundle, but certificate expired\n+            # Has the right trust bundle, but the certificate is expired\n             with pytest.raises(requests.exceptions.SSLError):\n                 s.get(url, verify=\"tests/certs/expired/ca/ca.crt\")\n \n"
  },
  {
    "id": 1880276222,
    "number": 6709,
    "user": {
      "login": "sigmavirus24",
      "id": 240830
    },
    "diff_url": "https://github.com/psf/requests/pull/6709.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6709",
    "title": "Add default timeout",
    "body": "This adds a default connect and read timeout value for all usage of Requests. This is to solve a long-standing issue where some systems do not have a sufficiently low default value.\r\n\r\nPersonally, I'd want these values to be much lower, but a 10 second connection timeout and a 30 second read timeout seem like they should be enough to avoid problems for the edge cases of users while also not being so large that they're basically ineffective.\r\n\r\nCloses #3070",
    "diff": "diff --git a/HISTORY.md b/HISTORY.md\nindex 4fca5894d7..a65a5e673e 100644\n--- a/HISTORY.md\n+++ b/HISTORY.md\n@@ -6,6 +6,10 @@ dev\n \n - \\[Short description of non-trivial change.\\]\n \n+**Security**\n+- Add a default timeout value to Requests. The default connect timeout is 10.0\n+  seconds and the default read timeout is 30.0 seconds. (#3070)\n+\n 2.32.2 (2024-05-21)\n -------------------\n \ndiff --git a/src/requests/sessions.py b/src/requests/sessions.py\nindex b387bc36df..2bf4aa8e8f 100644\n--- a/src/requests/sessions.py\n+++ b/src/requests/sessions.py\n@@ -58,6 +58,10 @@\n     preferred_clock = time.time\n \n \n+# (connect timeout, read timeout)\n+_DEFAULT_TIMEOUT = (10.0, 30.0)\n+\n+\n def merge_setting(request_setting, session_setting, dict_class=OrderedDict):\n     \"\"\"Determines appropriate setting for a given request, taking into account\n     the explicit setting on that request, and the setting in the session. If a\n@@ -582,7 +586,7 @@ def request(\n \n         # Send the request.\n         send_kwargs = {\n-            \"timeout\": timeout,\n+            \"timeout\": timeout if timeout is not None else _DEFAULT_TIMEOUT,\n             \"allow_redirects\": allow_redirects,\n         }\n         send_kwargs.update(settings)\n"
  },
  {
    "id": 1860510790,
    "number": 6696,
    "user": {
      "login": "StefanKopieczek",
      "id": 1242316
    },
    "diff_url": "https://github.com/psf/requests/pull/6696.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6696",
    "title": "Docs update: warn that Session.verify is ignored if REQUESTS_CA_BUNDLE is set",
    "body": "Issue #3829 tracks that Session.verify is ignored whenever `REQUESTS_CA_BUNDLES` or `CURL_CA_BUNDLES` is set. The underlying behavior will apparently be changed in 3.x, but in the meantime it would be helpful to call out the behavior in the docs to avoid catching people out (the issue has been locked due to a large number of comments).\r\n\r\nI'm happy to rework this PR as needed, but it'd be great to get something in the docs about this in one form or another to help prevent people hitting this.\r\n\r\nThanks in advance for your thoughts!\r\n\r\n---\r\n\r\n## Change Details\r\n\r\n* Added a `warning::` callout to the relevant section of the docs.\r\n\r\n## Alternate Approaches Considered\r\n\r\nI initially wrote out a longer section detailing the semantics for how the various options interact \u2014 but all the other interactions are very intuitive, so this felt like overkill. Let me know if you'd prefer that approach.\r\n\r\nThis section of the docs is also a bit callout heavy, so I'm happy to inline the text rather than put it in a callout, if you prefer.",
    "diff": "diff --git a/docs/user/advanced.rst b/docs/user/advanced.rst\nindex ff3a3d0f26..f3b8bb4451 100644\n--- a/docs/user/advanced.rst\n+++ b/docs/user/advanced.rst\n@@ -238,6 +238,9 @@ or persistent::\n This list of trusted CAs can also be specified through the ``REQUESTS_CA_BUNDLE`` environment variable.\n If ``REQUESTS_CA_BUNDLE`` is not set, ``CURL_CA_BUNDLE`` will be used as fallback.\n \n+.. warning:: Changes to ``session.verify`` will be ignored if either \n+  ``REQUESTS_CA_BUNDLE`` or ``CURL_CA_BUNDLE`` is set.\n+\n Requests can also ignore verifying the SSL certificate if you set ``verify`` to False::\n \n     >>> requests.get('https://kennethreitz.org', verify=False)\n"
  },
  {
    "id": 1856587369,
    "number": 6693,
    "user": {
      "login": "alvieirajr",
      "id": 5341637
    },
    "diff_url": "https://github.com/psf/requests/pull/6693.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6693",
    "title": "fix: enviroment variable PYTHONHTTPSVERIFY isn't take in consideration on https requests.",
    "body": "In case of the ssl certificate of destination host has expired isn't possible to ignore the cert validation using env PYTHONHTTPSVERIFY=0.",
    "diff": "diff --git a/src/requests/adapters.py b/src/requests/adapters.py\nindex 84ec48fc70..83298b7c8c 100644\n--- a/src/requests/adapters.py\n+++ b/src/requests/adapters.py\n@@ -6,6 +6,7 @@\n and maintain connections.\n \"\"\"\n \n+import os\n import os.path\n import socket  # noqa: F401\n import typing\n@@ -522,7 +523,14 @@ def send(\n             conn = self._get_connection(request, verify, proxies=proxies, cert=cert)\n         except LocationValueError as e:\n             raise InvalidURL(e, request=request)\n-\n+            \n+        python_https_verify = '1'       \n+        if 'PYTHONHTTPSVERIFY' in os.environ:\n+            python_https_verify = os.environ['PYTHONHTTPSVERIFY']        \n+            if ((python_https_verify is None) or (python_https_verify == '') or (python_https_verify != '0') or (python_https_verify != '1')):\n+                python_https_verify = '0'\n+        verify = True if python_https_verify == '1' else False\n+        \n         self.cert_verify(conn, request.url, verify, cert)\n         url = self.request_url(request, proxies)\n         self.add_headers(\n"
  },
  {
    "id": 1793353482,
    "number": 6675,
    "user": {
      "login": "vcapparelli",
      "id": 125774763
    },
    "diff_url": "https://github.com/psf/requests/pull/6675.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6675",
    "title": "Fix utils functions with invalid ip/cidr inputs",
    "body": "- add test cases reported in #5131\r\n- fix utils functions `address_in_network`, `is_ipv4_address` and `is_valid_cidr`\r\n- replace socket with `ipaddress` module\r\n- prepare ipv6 support\r\n\r\nFixes #5131",
    "diff": "diff --git a/src/requests/utils.py b/src/requests/utils.py\nindex ae6c42f6cb..5e7f80be5f 100644\n--- a/src/requests/utils.py\n+++ b/src/requests/utils.py\n@@ -9,6 +9,7 @@\n import codecs\n import contextlib\n import io\n+import ipaddress\n import os\n import re\n import socket\n@@ -687,11 +688,12 @@ def address_in_network(ip, net):\n \n     :rtype: bool\n     \"\"\"\n-    ipaddr = struct.unpack(\"=L\", socket.inet_aton(ip))[0]\n-    netaddr, bits = net.split(\"/\")\n-    netmask = struct.unpack(\"=L\", socket.inet_aton(dotted_netmask(int(bits))))[0]\n-    network = struct.unpack(\"=L\", socket.inet_aton(netaddr))[0] & netmask\n-    return (ipaddr & netmask) == (network & netmask)\n+    try:\n+        ip_address = ipaddress.ip_address(ip)\n+        network = ipaddress.ip_network(net)\n+        return ip_address in network\n+    except (ipaddress.AddressValueError, ValueError):\n+        return False\n \n \n def dotted_netmask(mask):\n@@ -710,8 +712,8 @@ def is_ipv4_address(string_ip):\n     :rtype: bool\n     \"\"\"\n     try:\n-        socket.inet_aton(string_ip)\n-    except OSError:\n+        ipaddress.IPv4Address(string_ip)\n+    except ipaddress.AddressValueError:\n         return False\n     return True\n \n@@ -722,22 +724,11 @@ def is_valid_cidr(string_network):\n \n     :rtype: bool\n     \"\"\"\n-    if string_network.count(\"/\") == 1:\n-        try:\n-            mask = int(string_network.split(\"/\")[1])\n-        except ValueError:\n-            return False\n-\n-        if mask < 1 or mask > 32:\n-            return False\n-\n-        try:\n-            socket.inet_aton(string_network.split(\"/\")[0])\n-        except OSError:\n-            return False\n-    else:\n+    try:\n+        interface = ipaddress.ip_interface(string_network)\n+    except (ipaddress.AddressValueError, ValueError):\n         return False\n-    return True\n+    return string_network in (interface.compressed, interface.exploded)\n \n \n @contextlib.contextmanager\ndiff --git a/tests/test_utils.py b/tests/test_utils.py\nindex 5e9b56ea64..31f7416441 100644\n--- a/tests/test_utils.py\n+++ b/tests/test_utils.py\n@@ -257,7 +257,14 @@ class TestIsIPv4Address:\n     def test_valid(self):\n         assert is_ipv4_address(\"8.8.8.8\")\n \n-    @pytest.mark.parametrize(\"value\", (\"8.8.8.8.8\", \"localhost.localdomain\"))\n+    @pytest.mark.parametrize(\n+        \"value\",\n+        (\n+            \"8.8.8.8.8\",\n+            \"1.1.1.1 someone was here...\",\n+            \"localhost.localdomain\",\n+        ),\n+    )\n     def test_invalid(self, value):\n         assert not is_ipv4_address(value)\n \n@@ -274,6 +281,7 @@ def test_valid(self):\n             \"192.168.1.0/128\",\n             \"192.168.1.0/-1\",\n             \"192.168.1.999/24\",\n+            \"1.1.1.1 something/24\",\n         ),\n     )\n     def test_invalid(self, value):\n@@ -284,8 +292,17 @@ class TestAddressInNetwork:\n     def test_valid(self):\n         assert address_in_network(\"192.168.1.1\", \"192.168.1.0/24\")\n \n-    def test_invalid(self):\n-        assert not address_in_network(\"172.16.0.1\", \"192.168.1.0/24\")\n+    @pytest.mark.parametrize(\n+        \"ip, net\",\n+        (\n+            (\"172.16.0.1\", \"192.168.1.0/24\"),\n+            (\"1.1.1.1\", \"1.1.1.1/24\"),\n+            (\"1.1.1.1wtf\", \"1.1.1.1/24\"),\n+            (\"1.1.1.1 wtf\", \"1.1.1.1/24\"),\n+        ),\n+    )\n+    def test_invalid(self, ip, net):\n+        assert not address_in_network(ip, net)\n \n \n class TestGuessFilename:\n"
  },
  {
    "id": 1763404110,
    "number": 6659,
    "user": {
      "login": "alain-khalil",
      "id": 66105100
    },
    "diff_url": "https://github.com/psf/requests/pull/6659.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6659",
    "title": "get_encoding_from_headers fails if charset name not specified #6646",
    "body": null,
    "diff": "diff --git a/src/requests/utils.py b/src/requests/utils.py\nindex a603a8638c..44fc70d251 100644\n--- a/src/requests/utils.py\n+++ b/src/requests/utils.py\n@@ -549,7 +549,15 @@ def get_encoding_from_headers(headers):\n     content_type, params = _parse_content_type_header(content_type)\n \n     if \"charset\" in params:\n-        return params[\"charset\"].strip(\"'\\\"\")\n+        charset = params[\"charset\"]\n+        # Check if charset is a boolean value\n+        if charset is True:\n+            return \"ISO-8859-1\"\n+        # Check if charset is explicitly False\n+        elif charset is False:\n+            return None\n+        else:\n+            return charset.strip(\"'\\\"\")\n \n     if \"text\" in content_type:\n         return \"ISO-8859-1\"\n@@ -558,6 +566,8 @@ def get_encoding_from_headers(headers):\n         # Assume UTF-8 based on RFC 4627: https://www.ietf.org/rfc/rfc4627.txt since the charset was unset\n         return \"utf-8\"\n \n+    return None\n+\n \n def stream_decode_response_unicode(iterator, r):\n     \"\"\"Stream decodes an iterator.\"\"\"\ndiff --git a/tests/test_utils.py b/tests/test_utils.py\nindex 8988eaf69c..4043a1928f 100644\n--- a/tests/test_utils.py\n+++ b/tests/test_utils.py\n@@ -617,6 +617,11 @@ def test__parse_content_type_header(value, expected):\n             \"utf-8\",\n         ),\n         (CaseInsensitiveDict({\"content-type\": \"text/plain\"}), \"ISO-8859-1\"),\n+        (CaseInsensitiveDict({\"content-type\": \"text/html; charset\"}), \"ISO-8859-1\"),\n+        (\n+            CaseInsensitiveDict({\"content-type\": \"application/json; charset\"}),\n+            \"ISO-8859-1\",\n+        ),\n     ),\n )\n def test_get_encoding_from_headers(value, expected):\n"
  },
  {
    "id": 1745710438,
    "number": 6651,
    "user": {
      "login": "jschfflr",
      "id": 4992440
    },
    "diff_url": "https://github.com/psf/requests/pull/6651.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6651",
    "title": "Add test coverage for api.py",
    "body": "This change adds tests for the highlevel api that exposes the http methods directly.",
    "diff": "diff --git a/tests/test_api.py b/tests/test_api.py\nnew file mode 100644\nindex 0000000000..117b9ea8e6\n--- /dev/null\n+++ b/tests/test_api.py\n@@ -0,0 +1,12 @@\n+import pytest\n+\n+import requests\n+\n+\n+@pytest.mark.parametrize(\n+    \"method\", (\"get\", \"head\", \"options\", \"delete\", \"put\", \"post\", \"patch\")\n+)\n+def test_highlevel_api(httpbin, method):\n+    function = getattr(requests, method)\n+    response = function(httpbin(\"/status/200\"))\n+    assert response.status_code == 200\n"
  },
  {
    "id": 1741846373,
    "number": 6649,
    "user": {
      "login": "sigmavirus24",
      "id": 240830
    },
    "diff_url": "https://github.com/psf/requests/pull/6649.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6649",
    "title": "Measure and report test coverage",
    "body": "This runs pytest within coverage, enables pytest-xdist to improve test run times, and reports the coverage on GitHub Actions for us.\r\n\r\nThis also ensures that the parallel mode coverage files are ignored and adds things to pytest configuration to make tox command simpler",
    "diff": "diff --git a/.coveragerc b/.coveragerc\nindex b5008b2b2f..d68944724e 100644\n--- a/.coveragerc\n+++ b/.coveragerc\n@@ -1,2 +1,14 @@\n [run]\n-omit = requests/packages/*\n+include = */requests/*.py\n+omit = */requests/packages/*\n+parallel = True\n+\n+[report]\n+omit =\n+    */requests/packages/*\n+\n+[paths]\n+source =\n+    src/requests/\n+    */requests/\n+    *\\requests\ndiff --git a/.github/workflows/run-tests.yml b/.github/workflows/run-tests.yml\nindex 3275080233..2349228b8a 100644\n--- a/.github/workflows/run-tests.yml\n+++ b/.github/workflows/run-tests.yml\n@@ -28,3 +28,50 @@ jobs:\n     - name: Run tests\n       run: |\n         make ci\n+    - name: \"Upload artifact\"\n+      uses: actions/upload-artifact@0b7f8abb1508181956e8e162db84b466c27e18ce # v3.1.2\n+      with:\n+        name: coverage-data-${{ matrix.python-version }}\n+        path: \".coverage.*\"\n+        if-no-files-found: error\n+\n+  coverage:\n+    name: \"Combine & check coverage\"\n+    needs: build\n+    runs-on: \"ubuntu-latest\"\n+\n+    steps:\n+      - name: \"Checkout repository\"\n+        uses: actions/checkout@3df4ab11eba7bda6032a0b82a6bb43b11571feac # v4.0.0\n+\n+      - name: \"Setup Python\"\n+        uses: actions/setup-python@0a5c61591373683505ea898e09a3ea4f39ef2b9c # v5.0.0\n+        with:\n+          python-version: \"3.x\"\n+\n+      - name: \"Install Coverage\"\n+        run: python -Im pip install --upgrade coverage\n+\n+      - name: \"Download coverage artifacts\"\n+        uses: actions/download-artifact@v4\n+        with:\n+          pattern: coverage-data-*\n+          merge-multiple: true\n+\n+      - name: Combine coverage & fail if it's <100%.\n+        run: |\n+          python -Im coverage combine\n+          python -Im coverage html --skip-covered --skip-empty\n+\n+          # Report and write to summary.\n+          python -Im coverage report --format=markdown >> $GITHUB_STEP_SUMMARY\n+\n+          # Report again and fail if under 100%.\n+          python -Im coverage report --show-missing --fail-under=85\n+\n+      - name: \"Upload HTML report if check failed.\"\n+        uses: actions/upload-artifact@v4\n+        with:\n+          name: html-report\n+          path: htmlcov\n+        if: ${{ failure() }}\ndiff --git a/.gitignore b/.gitignore\nindex de61154e3e..b4a25663c7 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -1,4 +1,4 @@\n-.coverage\n+.coverage*\n MANIFEST\n coverage.xml\n nosetests.xml\ndiff --git a/Makefile b/Makefile\nindex 192b926853..92e1d52719 100644\n--- a/Makefile\n+++ b/Makefile\n@@ -5,22 +5,23 @@ test:\n \t# This runs all of the tests on all supported Python versions.\n \ttox -p\n ci:\n-\tpython -m pytest tests --junitxml=report.xml\n+\tpython -m coverage run -m pytest --junitxml=report.xml\n \n test-readme:\n-\tpython setup.py check --restructuredtext --strict && ([ $$? -eq 0 ] && echo \"README.rst and HISTORY.rst ok\") || echo \"Invalid markup in README.rst or HISTORY.rst!\"\n+\tpython -m pip install twine build\n+\tpython -m build\n+\tpython -m twine check dist/*\n \n flake8:\n \tpython -m flake8 src/requests\n \n coverage:\n-\tpython -m pytest --cov-config .coveragerc --verbose --cov-report term --cov-report xml --cov=src/requests tests\n+\tpython -m coverage run -m pytest\n \n publish:\n-\tpython -m pip install 'twine>=1.5.0'\n-\tpython setup.py sdist bdist_wheel\n+\tpython -m pip install 'twine>=1.5.0' build\n+\tpython -m build\n \ttwine upload dist/*\n-\trm -fr build dist .egg requests.egg-info\n \n docs:\n \tcd docs && make html\ndiff --git a/pyproject.toml b/pyproject.toml\nindex 1b7901e155..a946b6b0f3 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -4,7 +4,7 @@ src_paths = [\"src/requests\", \"test\"]\n honor_noqa = true\n \n [tool.pytest.ini_options]\n-addopts = \"--doctest-modules\"\n+addopts = \"--doctest-modules -n auto\"\n doctest_optionflags = \"NORMALIZE_WHITESPACE ELLIPSIS\"\n minversion = \"6.2\"\n testpaths = [\"tests\"]\ndiff --git a/requirements-dev.txt b/requirements-dev.txt\nindex 13173f3ae5..4e68d3c46b 100644\n--- a/requirements-dev.txt\n+++ b/requirements-dev.txt\n@@ -1,7 +1,8 @@\n -e .[socks]\n-pytest>=2.8.0,<=6.2.5\n-pytest-cov\n-pytest-httpbin==2.0.0\n+pytest>=2.8.0,!=6.2.5\n+coverage\n+pytest-xdist\n httpbin~=0.10.0\n+pytest-httpbin==2.0.0\n trustme\n wheel\ndiff --git a/tox.ini b/tox.ini\nindex c438ef316a..434bcb9643 100644\n--- a/tox.ini\n+++ b/tox.ini\n@@ -7,7 +7,8 @@ extras =\n     security\n     socks\n commands =\n-    pytest {posargs:tests}\n+    coverage run --parallel-mode -m pytest\n+    coverage report\n \n [testenv:default]\n \n"
  },
  {
    "id": 1596877436,
    "number": 6570,
    "user": {
      "login": "bemoody",
      "id": 7748246
    },
    "diff_url": "https://github.com/psf/requests/pull/6570.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6570",
    "title": "Document that Response.iter_lines is broken and should be avoided",
    "body": "The `Response.iter_lines` method is seriously broken (it inserts fake blank lines in unpredictable places.)\n\nThe earliest report of this I've found is pull #2431 (January 2015).\n\nThe behavior was apparently fixed in the 3.0.0 branch, by pulls #3923 and #3984.  (I think #3923 fixes the `delimiter!=None` case and #3984 fixes the `delimiter=None` case.  But I haven't tested it.)\n\nThe problem was raised again in issues #3980, #4121, and #5540.\n\nPull #4629 attempted to partially fix the issue in the master branch, and was rejected.\n\nPersonally, I am skeptical that there is any benefit in preserving this broken behavior.  But folks who know better than I do have said that it needs to be maintained.\n\nAs long as this *isn't* fixed, people using the library should be informed that method is broken and they shouldn't use it.\n",
    "diff": "diff --git a/src/requests/models.py b/src/requests/models.py\nindex 44556394ec..6ccb76489c 100644\n--- a/src/requests/models.py\n+++ b/src/requests/models.py\n@@ -852,9 +852,14 @@ def generate():\n     def iter_lines(\n         self, chunk_size=ITER_CHUNK_SIZE, decode_unicode=False, delimiter=None\n     ):\n-        \"\"\"Iterates over the response data, one line at a time.  When\n-        stream=True is set on the request, this avoids reading the\n-        content at once into memory for large responses.\n+        \"\"\"Iterates over the response data, one line at a time.\n+\n+        This method is broken and should not be used. It inserts fake\n+        blank lines in unpredictable locations. Furthermore, if\n+        stream=True is set on the request, the results may not be\n+        reproducible. However, the behavior of this method is frozen\n+        and cannot be fixed while preserving strict backward\n+        compatibility. This behavior should be fixed in Requests 3.0.\n \n         .. note:: This method is not reentrant safe.\n         \"\"\"\n"
  },
  {
    "id": 1588102920,
    "number": 6568,
    "user": {
      "login": "eivindt",
      "id": 1540378
    },
    "diff_url": "https://github.com/psf/requests/pull/6568.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6568",
    "title": "Bug/6294 zero bytes files are chunked",
    "body": "See bug #6294 for original bug report.\r\n\r\nThis PR provides a possible fix for this, reverting the change in #2896.\r\n\r\nThe change in #2896 definitely causes some bad side effects, since not all web servers handle \"chunked\" transfer encoding (i.e. some don't handle it, some don't handle it well).\r\n\r\n### Problem demonstration\r\n\r\nGiven a simple bottle application:\r\n```python\r\nfrom bottle import run, put, request, default_app\r\n\r\napp = default_app()\r\n\r\n@app.put(\"/\")\r\ndef receive_file():\r\n    upload = request.body.read()\r\n    for head, val in request.headers.items():\r\n        print(\"%-30s:\\t%s\" % (head, val))\r\n    clength = request.headers.get(\"Content-Length\", \"not-set\")\r\n    cencoding = request.headers.get(\"Transfer-Encoding\", \"not-set\")\r\n    return { 'size': len(upload), 'content-length': clength, 'transfer-encoding': cencoding }\r\n\r\nif __name__ == '__main__':\r\n    app.run(host='localhost', port=8880)\r\n```\r\n\r\nAnd a simple requests script:\r\n```python\r\nimport requests\r\nimport io\r\nempty_obj = io.BytesIO(b'')\r\nresp = requests.put('http://localhost:8880/', data=empty_obj)\r\nprint(resp.status_code)\r\nif resp.status_code == 200:\r\n    print(resp.json())\r\n```\r\n\r\n## Working Scenario\r\n\r\n```\r\n$ python server.py\r\nBottle v0.12.25 server starting up (using WSGIRefServer())...\r\nListening on http://localhost:8880/\r\nHit Ctrl-C to quit.\r\n\r\nContent-Length                :\t\r\nContent-Type                  :\ttext/plain\r\nHost                          :\tlocalhost:8880\r\nUser-Agent                    :\tpython-requests/2.31.0\r\nAccept-Encoding               :\tgzip, deflate, br\r\nAccept                        :\t*/*\r\nConnection                    :\tkeep-alive\r\nTransfer-Encoding             :\tchunked\r\n127.0.0.1 - - [03/Nov/2023 15:51:51] \"PUT / HTTP/1.1\" 200 65\r\n```\r\nTest script output:\r\n```\r\n200\r\n{'size': 0, 'content-length': '', 'transfer-encoding': 'chunked'}\r\n```\r\n\r\n## Failing Scenario\r\n```\r\n$ gunicorn -b localhost:8880 server:app\r\n[2023-11-03 15:52:05 +0100] [1326049] [INFO] Starting gunicorn 21.2.0\r\n[2023-11-03 15:52:05 +0100] [1326049] [INFO] Listening at: http://127.0.0.1:8880 (1326049)\r\n[2023-11-03 15:52:05 +0100] [1326049] [INFO] Using worker: sync\r\n[2023-11-03 15:52:05 +0100] [1326050] [INFO] Booting worker with pid: 1326050\r\n```\r\nTest script output:\r\n```\r\n400\r\n```\r\n\r\n### The Original Issue\r\n\r\nThe change causing this problem in the first place was due to failing to upload data read from a subprocess pipe.\r\n\r\nThis patch adds a check to `super_len` that avoids returning length 0 for file handles that are not regular files.",
    "diff": "diff --git a/src/requests/models.py b/src/requests/models.py\nindex 44556394ec..de985109b6 100644\n--- a/src/requests/models.py\n+++ b/src/requests/models.py\n@@ -545,7 +545,7 @@ def prepare_body(self, data, files, json=None):\n                     \"Streamed bodies and files are mutually exclusive.\"\n                 )\n \n-            if length:\n+            if length is not None:\n                 self.headers[\"Content-Length\"] = builtin_str(length)\n             else:\n                 self.headers[\"Transfer-Encoding\"] = \"chunked\"\n@@ -573,7 +573,7 @@ def prepare_content_length(self, body):\n         \"\"\"Prepare Content-Length header based on request method and body\"\"\"\n         if body is not None:\n             length = super_len(body)\n-            if length:\n+            if length is not None:\n                 # If length exists, set it. Otherwise, we fallback\n                 # to Transfer-Encoding: chunked.\n                 self.headers[\"Content-Length\"] = builtin_str(length)\ndiff --git a/src/requests/utils.py b/src/requests/utils.py\nindex a603a8638c..1b2c8afc33 100644\n--- a/src/requests/utils.py\n+++ b/src/requests/utils.py\n@@ -12,6 +12,7 @@\n import os\n import re\n import socket\n+import stat\n import struct\n import sys\n import tempfile\n@@ -131,7 +132,14 @@ def dict_to_sequence(d):\n \n \n def super_len(o):\n+    \"\"\"Returns the length of the object or None if the length cannot be measured.\n+\n+    Tries looking for length attributes, file handles and seek/tell in order\n+    to figure out the object length.  If the length cannot be found, None\n+    is returned.\n+    \"\"\"\n     total_length = None\n+    length_undefined = False\n     current_position = 0\n \n     if isinstance(o, str):\n@@ -146,11 +154,18 @@ def super_len(o):\n     elif hasattr(o, \"fileno\"):\n         try:\n             fileno = o.fileno()\n+            if not stat.S_ISREG(os.fstat(fileno).st_mode):\n+                raise BufferError(\"Cannot tell size of non regular file\")\n         except (io.UnsupportedOperation, AttributeError):\n             # AttributeError is a surprising exception, seeing as how we've just checked\n             # that `hasattr(o, 'fileno')`.  It happens for objects obtained via\n             # `Tarfile.extractfile()`, per issue 5229.\n             pass\n+        except BufferError:\n+            # Telling size of non regular files does is not realiable\n+            # if it is a socket or a pipe, they need to be handled as\n+            # streams with no known end.\n+            length_undefined = True\n         else:\n             total_length = os.fstat(fileno).st_size\n \n@@ -169,7 +184,13 @@ def super_len(o):\n                     FileModeWarning,\n                 )\n \n-    if hasattr(o, \"tell\"):\n+    if hasattr(o, \"tell\") and not length_undefined:\n+        # pipes have tell/seek, tell() will fail on Linux/MacOS,\n+        # but not in windows, causing super_len to report the\n+        # length as the number of bytes currently written to\n+        # the pipe, which will be wrong if the pipe is continuously\n+        # being written to. The length_underfined check will\n+        # prevent trying to use tell/seek for a pipe.\n         try:\n             current_position = o.tell()\n         except OSError:\n@@ -191,10 +212,10 @@ def super_len(o):\n                     # partially read file-like objects\n                     o.seek(current_position or 0)\n                 except OSError:\n-                    total_length = 0\n+                    total_length = None\n \n     if total_length is None:\n-        total_length = 0\n+        return total_length\n \n     return max(0, total_length - current_position)\n \ndiff --git a/tests/test_requests.py b/tests/test_requests.py\nindex 32b5e6700c..308deb580c 100644\n--- a/tests/test_requests.py\n+++ b/tests/test_requests.py\n@@ -130,6 +130,22 @@ def test_empty_content_length(self, httpbin, method):\n         req = requests.Request(method, httpbin(method.lower()), data=\"\").prepare()\n         assert req.headers[\"Content-Length\"] == \"0\"\n \n+    @pytest.mark.parametrize(\"method\", (\"POST\", \"PUT\", \"PATCH\", \"OPTIONS\"))\n+    def test_empty_file_content_length(self, httpbin, method):\n+        data = io.BytesIO(b\"\")\n+        req = requests.Request(method, httpbin(method.lower()), data=data).prepare()\n+        assert req.headers[\"Content-Length\"] == \"0\"\n+\n+    @pytest.mark.parametrize(\"method\", (\"POST\", \"PUT\", \"PATCH\", \"OPTIONS\"))\n+    def test_not_readable_content_length_pipe(self, httpbin, method):\n+        pipe_r, pipe_w = os.pipe()\n+        pipe_rf = os.fdopen(pipe_r, \"rb\")\n+        pipe_wf = os.fdopen(pipe_w, \"wb\")\n+        pipe_wf.write(b\"hello\")\n+        req = requests.Request(method, httpbin(method.lower()), data=pipe_rf).prepare()\n+        assert req.headers.get(\"Content-Length\") is None\n+        assert req.headers[\"Transfer-Encoding\"] == \"chunked\"\n+\n     def test_override_content_length(self, httpbin):\n         headers = {\"Content-Length\": \"not zero\"}\n         r = requests.Request(\"POST\", httpbin(\"post\"), headers=headers).prepare()\n@@ -2151,7 +2167,9 @@ def test_response_without_release_conn(self):\n         resp.close()\n         assert resp.raw.closed\n \n-    def test_empty_stream_with_auth_does_not_set_content_length_header(self, httpbin):\n+    def test_empty_stream_with_auth_does_not_set_transfer_encoding_header(\n+        self, httpbin\n+    ):\n         \"\"\"Ensure that a byte stream with size 0 will not set both a Content-Length\n         and Transfer-Encoding header.\n         \"\"\"\n@@ -2160,8 +2178,8 @@ def test_empty_stream_with_auth_does_not_set_content_length_header(self, httpbin\n         file_obj = io.BytesIO(b\"\")\n         r = requests.Request(\"POST\", url, auth=auth, data=file_obj)\n         prepared_request = r.prepare()\n-        assert \"Transfer-Encoding\" in prepared_request.headers\n-        assert \"Content-Length\" not in prepared_request.headers\n+        assert \"Transfer-Encoding\" not in prepared_request.headers\n+        assert \"Content-Length\" in prepared_request.headers\n \n     def test_stream_with_auth_does_not_set_transfer_encoding_header(self, httpbin):\n         \"\"\"Ensure that a byte stream with size > 0 will not set both a Content-Length\ndiff --git a/tests/test_utils.py b/tests/test_utils.py\nindex 8988eaf69c..6f2b83657e 100644\n--- a/tests/test_utils.py\n+++ b/tests/test_utils.py\n@@ -92,7 +92,7 @@ def tell(self):\n             def seek(self, offset, whence):\n                 pass\n \n-        assert super_len(NoLenBoomFile()) == 0\n+        assert super_len(NoLenBoomFile()) is None\n \n     def test_string(self):\n         assert super_len(\"Test\") == 4\n@@ -148,8 +148,14 @@ def test_super_len_with_fileno(self):\n         assert length == len(file_data)\n \n     def test_super_len_with_no_matches(self):\n-        \"\"\"Ensure that objects without any length methods default to 0\"\"\"\n-        assert super_len(object()) == 0\n+        \"\"\"Ensure that objects without any length methods default to None\"\"\"\n+        assert super_len(object()) is None\n+\n+    def test_super_len_with_pipe(self):\n+        \"\"\"Ensure that ojects with a fileno that are not regular files default to length None\"\"\"\n+        r, w = os.pipe()\n+        rf = os.fdopen(r, \"rb\")\n+        assert super_len(rf) is None\n \n \n class TestToKeyValList:\n"
  },
  {
    "id": 1577600868,
    "number": 6561,
    "user": {
      "login": "amkarn258",
      "id": 55189266
    },
    "diff_url": "https://github.com/psf/requests/pull/6561.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6561",
    "title": "url param gets added when empty param is passed",
    "body": "Have added the fix for the issue mentioned here - https://github.com/psf/requests/issues/6557\r\n\r\nSince empty lists, dicts, tuples aren't of none type they should be added in the url string. ",
    "diff": "diff --git a/src/requests/models.py b/src/requests/models.py\nindex 44556394ec..79cb2bab9a 100644\n--- a/src/requests/models.py\n+++ b/src/requests/models.py\n@@ -119,6 +119,8 @@ def _encode_params(data):\n         elif hasattr(data, \"__iter__\"):\n             result = []\n             for k, vs in to_key_val_list(data):\n+                if not isinstance(vs, basestring) and not vs:\n+                    vs = ''\n                 if isinstance(vs, basestring) or not hasattr(vs, \"__iter__\"):\n                     vs = [vs]\n                 for v in vs:\n"
  },
  {
    "id": 1562611314,
    "number": 6553,
    "user": {
      "login": "magsen",
      "id": 28842337
    },
    "diff_url": "https://github.com/psf/requests/pull/6553.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6553",
    "title": "docs(socks): same block as other sections",
    "body": "strange formatting:\r\nhttps://docs.python-requests.org/en/latest/user/advanced/#socks",
    "diff": "diff --git a/docs/user/advanced.rst b/docs/user/advanced.rst\nindex c90a13dc6d..57ad8c3cac 100644\n--- a/docs/user/advanced.rst\n+++ b/docs/user/advanced.rst\n@@ -675,9 +675,7 @@ In addition to basic HTTP proxies, Requests also supports proxies using the\n SOCKS protocol. This is an optional feature that requires that additional\n third-party libraries be installed before use.\n \n-You can get the dependencies for this feature from ``pip``:\n-\n-.. code-block:: bash\n+You can get the dependencies for this feature from ``pip``::\n \n     $ python -m pip install requests[socks]\n \n"
  },
  {
    "id": 1515287871,
    "number": 6529,
    "user": {
      "login": "anupam-arista",
      "id": 118899211
    },
    "diff_url": "https://github.com/psf/requests/pull/6529.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6529",
    "title": "Update models.Response.json docstring to be clearer",
    "body": null,
    "diff": "diff --git a/src/requests/models.py b/src/requests/models.py\nindex 8f56ca7d23..31ac5ea102 100644\n--- a/src/requests/models.py\n+++ b/src/requests/models.py\n@@ -945,7 +945,7 @@ def text(self):\n         return content\n \n     def json(self, **kwargs):\n-        r\"\"\"Returns the json-encoded content of a response, if any.\n+        r\"\"\"Returns the json-decoded dict of a response, if any.\n \n         :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n         :raises requests.exceptions.JSONDecodeError: If the response body does not\n"
  },
  {
    "id": 1508834119,
    "number": 6523,
    "user": {
      "login": "joren485",
      "id": 7031489
    },
    "diff_url": "https://github.com/psf/requests/pull/6523.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6523",
    "title": "Pass response_kw to HTTPConnectionPool through HTTPAdapter.send",
    "body": "### Summary\r\nThis PR adds kwargs arguments to `HTTPAdapter.send`, which it passes to `HTTPConnectionPool.urlopen` in urllib3.\r\n\r\n### Description\r\nAs discussed in #4956, urrllib3 recently changed the default value of `enforce_content_length` from `False` to `True`. The new default seems like a sane choice, but in some use cases the `Content-Length` header should not be enforced. To change the default behavior, urllib3 allows application code to set the `enforce_content_length` argument. As far as I know, `requests` does not have a way to pass this argument to urllib3.\r\n\r\nThe `HTTPConnectionPool.urlopen` method in urllib3 has the `**response_kw` kwargs argument to pass extra arguments down to the response parser. This PR adds a similar to argument to `HTTPAdapter.send`.\r\n\r\nWith this PR, users can override the `HTTPAdapter.send` method to pass extra arguments to `HTTPConnectionPool.urlopen`. For example, this enables users to explicitly set the `enforce_content_length`.\r\n\r\n### Example\r\n```Python\r\nimport requests\r\n\r\nfrom requests.adapters import HTTPAdapter\r\n\r\nclass EnforceContentLengthAdapter(HTTPAdapter):\r\n    def send(self, *args, **kwargs):\r\n        kwargs[\"enforce_content_length\"] = False\r\n        return super().send(*args, **kwargs)\r\n\r\ns = requests.Session()\r\ns.mount(\"http://\", EnforceContentLengthAdapter())\r\ns.mount(\"https://\", EnforceContentLengthAdapter())\r\n\r\nr = s.get(\"http://localhost:8080/\")\r\nprint(r.raw.enforce_content_length) # Returns False\r\n```",
    "diff": "diff --git a/src/requests/adapters.py b/src/requests/adapters.py\nindex eb240fa954..56a247477b 100644\n--- a/src/requests/adapters.py\n+++ b/src/requests/adapters.py\n@@ -130,6 +130,7 @@ class HTTPAdapter(BaseAdapter):\n         \"_pool_connections\",\n         \"_pool_maxsize\",\n         \"_pool_block\",\n+        \"urllib3_response_options\",\n     ]\n \n     def __init__(\n@@ -154,6 +155,8 @@ def __init__(\n \n         self.init_poolmanager(pool_connections, pool_maxsize, block=pool_block)\n \n+        self.urllib3_response_options = {}\n+\n     def __getstate__(self):\n         return {attr: getattr(self, attr, None) for attr in self.__attrs__}\n \n@@ -435,6 +438,9 @@ def send(\n     ):\n         \"\"\"Sends PreparedRequest object. Returns Response object.\n \n+        It is possible to pass additional arguments to :meth:`urllib3.poolmanager.PoolManager.urlopen`\n+        (e.g. `enforce_content_length`) by populating :attr:`HTTPAdapter.urllib3_response_options`.\n+\n         :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.\n         :param stream: (optional) Whether to stream the request content.\n         :param timeout: (optional) How long to wait for the server to send\n@@ -481,20 +487,25 @@ def send(\n         else:\n             timeout = TimeoutSauce(connect=timeout, read=timeout)\n \n+        urlopen_kwargs = self.urllib3_response_options.copy()\n+        urlopen_kwargs.update(\n+            {\n+                \"method\": request.method,\n+                \"url\": url,\n+                \"body\": request.body,\n+                \"headers\": request.headers,\n+                \"redirect\": False,\n+                \"assert_same_host\": False,\n+                \"preload_content\": False,\n+                \"decode_content\": False,\n+                \"retries\": self.max_retries,\n+                \"timeout\": timeout,\n+                \"chunked\": chunked,\n+            }\n+        )\n+\n         try:\n-            resp = conn.urlopen(\n-                method=request.method,\n-                url=url,\n-                body=request.body,\n-                headers=request.headers,\n-                redirect=False,\n-                assert_same_host=False,\n-                preload_content=False,\n-                decode_content=False,\n-                retries=self.max_retries,\n-                timeout=timeout,\n-                chunked=chunked,\n-            )\n+            resp = conn.urlopen(**urlopen_kwargs)\n \n         except (ProtocolError, OSError) as err:\n             raise ConnectionError(err, request=request)\n"
  },
  {
    "id": 1473462816,
    "number": 6504,
    "user": {
      "login": "nateprewitt",
      "id": 5271761
    },
    "diff_url": "https://github.com/psf/requests/pull/6504.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6504",
    "title": "Add minimum PEP518 build-system specification",
    "body": "We've been missing the minimum PEP 518 build specification in our pyproject.toml. This PR adds that, along with an explicit backend to remove any ambiguity for alternative build tools.",
    "diff": "diff --git a/MANIFEST.in b/MANIFEST.in\nindex 633be369e2..ecde3b8ac4 100644\n--- a/MANIFEST.in\n+++ b/MANIFEST.in\n@@ -1,2 +1,9 @@\n-include README.md LICENSE NOTICE HISTORY.md pytest.ini requirements-dev.txt\n+include README.md\n+include LICENSE\n+include NOTICE\n+include HISTORY.md\n+include pytest.ini\n+include pyproject.toml\n+include requirements-dev.txt\n+\n recursive-include tests *.py\ndiff --git a/pyproject.toml b/pyproject.toml\nindex d3ab7bd9bb..194c0d5ed2 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -1,3 +1,7 @@\n+[build-system]\n+requires = [\"setuptools\", \"wheel\"]\n+build-backend = \"setuptools.build_meta\"\n+\n [tool.isort]\n profile = \"black\"\n src_paths = [\"requests\", \"test\"]\n"
  },
  {
    "id": 1346323043,
    "number": 6454,
    "user": {
      "login": "steveberdy",
      "id": 86739818
    },
    "diff_url": "https://github.com/psf/requests/pull/6454.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6454",
    "title": "Add os.PathLike support for cert files",
    "body": "## Summary\r\n\r\nAdds support for any path-like object, including `pathlib.Path`. See https://github.com/psf/requests/issues/5936#issuecomment-931758148 for more details.\r\nSome things have changed since the linked issue was created, so it may be a good time to add support now.\r\n\r\nFixes #5936 ",
    "diff": "diff --git a/requests/adapters.py b/requests/adapters.py\nindex 78e3bb6ecf..33d13cc5e0 100644\n--- a/requests/adapters.py\n+++ b/requests/adapters.py\n@@ -276,8 +276,16 @@ def cert_verify(self, conn, url, verify, cert):\n \n         if cert:\n             if not isinstance(cert, basestring):\n-                conn.cert_file = cert[0]\n-                conn.key_file = cert[1]\n+                # a subscriptable object\n+                if hasattr(cert, \"__getitem__\"):\n+                    conn.cert_file = cert[0]\n+                    conn.key_file = cert[1]\n+                elif hasattr(cert, \"__fspath__\"):\n+                    # a path-like object implements __fspath__\n+                    # see https://docs.python.org/3/library/os.html#os.PathLike\n+                    conn.cert_file = cert.__fspath__()\n+                else:\n+                    conn.cert_file = str(cert)\n             else:\n                 conn.cert_file = cert\n                 conn.key_file = None\n"
  },
  {
    "id": 1302219875,
    "number": 6400,
    "user": {
      "login": "yarikoptic",
      "id": 39889
    },
    "diff_url": "https://github.com/psf/requests/pull/6400.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6400",
    "title": "Codespell: workflow, config + 1 typo fixed",
    "body": "My humble contribution to keep requests so typos free as it is proudly is now.",
    "diff": "diff --git a/.github/workflows/codespell.yml b/.github/workflows/codespell.yml\nnew file mode 100644\nindex 0000000000..3ebbf5504b\n--- /dev/null\n+++ b/.github/workflows/codespell.yml\n@@ -0,0 +1,22 @@\n+---\n+name: Codespell\n+\n+on:\n+  push:\n+    branches: [main]\n+  pull_request:\n+    branches: [main]\n+\n+permissions:\n+  contents: read\n+\n+jobs:\n+  codespell:\n+    name: Check for spelling errors\n+    runs-on: ubuntu-latest\n+\n+    steps:\n+      - name: Checkout\n+        uses: actions/checkout@v3\n+      - name: Codespell\n+        uses: codespell-project/actions-codespell@v2\ndiff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\nindex 0a0515cf87..284970a1df 100644\n--- a/.pre-commit-config.yaml\n+++ b/.pre-commit-config.yaml\n@@ -26,3 +26,10 @@ repos:\n   rev: 6.1.0\n   hooks:\n     - id: flake8\n+\n+- repo: https://github.com/codespell-project/codespell\n+  rev: v2.2.6\n+  hooks:\n+  - id: codespell\n+    additional_dependencies:\n+    - tomli\ndiff --git a/pyproject.toml b/pyproject.toml\nindex 1b7901e155..2bddd6e22d 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -8,3 +8,9 @@ addopts = \"--doctest-modules\"\n doctest_optionflags = \"NORMALIZE_WHITESPACE ELLIPSIS\"\n minversion = \"6.2\"\n testpaths = [\"tests\"]\n+\n+[tool.codespell]\n+skip = '.git,*.pdf,*.svg,*.css,*.ai'\n+check-hidden = true\n+# ignore-regex = ''\n+ignore-words-list = 'fo'\n"
  },
  {
    "id": 1286835033,
    "number": 6388,
    "user": {
      "login": "Hawk777",
      "id": 365236
    },
    "diff_url": "https://github.com/psf/requests/pull/6388.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6388",
    "title": "Document that a Response is a context manager",
    "body": "This is mentioned in the Advanced Usage page, but not in the API reference. It is mentioned in the API reference for `Session`, so it seems reasonable that it should be mentioned for `Response` as well. It looks like this capability was added in GH-4137, which made the change to the former documentation but not the latter.",
    "diff": "diff --git a/requests/models.py b/requests/models.py\nindex 617a4134e5..30a0a8e8d4 100644\n--- a/requests/models.py\n+++ b/requests/models.py\n@@ -641,6 +641,8 @@ def prepare_hooks(self, hooks):\n class Response:\n     \"\"\"The :class:`Response <Response>` object, which contains a\n     server's response to an HTTP request.\n+\n+    A :class:`Response <Response>` object can be used as a context manager.\n     \"\"\"\n \n     __attrs__ = [\n"
  },
  {
    "id": 1281787480,
    "number": 6383,
    "user": {
      "login": "SyntaxColoring",
      "id": 3236864
    },
    "diff_url": "https://github.com/psf/requests/pull/6383.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6383",
    "title": "Clarify docs for multipart file uploads",
    "body": "This makes some documentation changes that clarify the `files` argument of `requests.post()` et. al.\r\n\r\n## Be more precise when we say \"multiple files\"\r\n\r\nThe `files` argument can either be a dict:\r\n\r\n```python\r\nfiles = {\r\n    \"field_1\": file_1,\r\n    \"field_2\": file_2,\r\n}\r\n```\r\n\r\nOr a list of tuples:\r\n\r\n```python\r\nfiles = [\r\n    (\"field_1\": file_1),\r\n    (\"field_2\": file_2),\r\n]\r\n```\r\n\r\nA few places implied that to \"upload multiple files in one request,\" you had to use the list-of-tuples syntax. But the dict syntax supports multiple files just fine. The added power in the list-of-tuples syntax seems to be that you can upload multiple files *to the same form field.* So, we now say that.\r\n\r\n## Update the reference docs\r\n\r\nThe API reference documentation did not completely describe what values were acceptable for `files`, and in some cases it was misleading.\r\n\r\n* It didn't mention the list-of-tuples syntax at all.\r\n* It said you could pass a \"dictionary of `'name': file-like-objects`\". The pluralization there makes it look like you can do something like `{\"name\": [file_1, file_2]}`, but that's not correct.\r\n* It didn't describe what you were allowed to provide as a \"`fileobj`\". (It can be a file-like object, or `str` contents, or `bytes` contents.)\r\n* It said `name` to refer to the value that Requests uses as the form field name. This could be misconstrued as the file name. [RFC 7578](https://www.rfc-editor.org/rfc/rfc7578) and other examples within Requests call it the \"field name\" or \"form field name.\"\r\n\r\nMy sources for these changes:\r\n\r\n* The existing [POST a Multipart-Encoded File](https://github.com/psf/requests/blob/7f694b79e114c06fac5ec06019cada5a61e5570f/docs/user/quickstart.rst?plain=1#L300) section.\r\n* The existing [POST Multiple Multipart-Encoded Files](https://github.com/psf/requests/blob/7f694b79e114c06fac5ec06019cada5a61e5570f/docs/user/advanced.rst?plain=1#L391) section.\r\n* [Request's type stubs](https://github.com/python/typeshed/blob/8080e491d2fa92b1b1390c87fac23a1a38dcd919/stubs/requests/requests/sessions.pyi#L91).\r\n\r\nIt's difficult to describe this API concisely because it accepts so many different input shapes, and these additions do make the rendered docs feel a bit crowded.",
    "diff": "diff --git a/docs/user/advanced.rst b/docs/user/advanced.rst\nindex c664a83d30..b7430fae13 100644\n--- a/docs/user/advanced.rst\n+++ b/docs/user/advanced.rst\n@@ -386,13 +386,13 @@ parameter of ``None``. If you want to set a maximum size of the chunk,\n you can set a ``chunk_size`` parameter to any integer.\n \n \n-.. _multipart:\n+.. _multiple_multipart_files_same_field:\n \n-POST Multiple Multipart-Encoded Files\n--------------------------------------\n+POST Multiple Multipart-Encoded Files to the Same Field\n+-------------------------------------------------------\n \n-You can send multiple files in one request. For example, suppose you want to\n-upload image files to an HTML form with a multiple file field 'images'::\n+You can send multiple files to the same form field in one request. For example, suppose you want to\n+upload image files to an HTML form with a multiple-file field 'images'::\n \n     <input type=\"file\" name=\"images\" multiple=\"true\" required=\"true\"/>\n \ndiff --git a/docs/user/quickstart.rst b/docs/user/quickstart.rst\nindex 464e4f5fa5..f5d9c7e24b 100644\n--- a/docs/user/quickstart.rst\n+++ b/docs/user/quickstart.rst\n@@ -297,6 +297,9 @@ and it will be encoded automatically:\n \n Note, the ``json`` parameter is ignored if either ``data`` or ``files`` is passed.\n \n+\n+.. _multipart_file:\n+\n POST a Multipart-Encoded File\n -----------------------------\n \n@@ -351,8 +354,8 @@ support this, but there is a separate package which does -\n ``requests-toolbelt``. You should read `the toolbelt's documentation\n <https://toolbelt.readthedocs.io>`_ for more details about how to use it.\n \n-For sending multiple files in one request refer to the :ref:`advanced <advanced>`\n-section.\n+For sending multiple files to the same form field in one request, refer to\n+:ref:`multiple_multipart_files_same_field`.\n \n .. warning:: It is strongly recommended that you open files in :ref:`binary\n              mode <tut-files>`. This is because Requests may attempt to provide\ndiff --git a/requests/api.py b/requests/api.py\nindex cd0b3eeac3..4a22aee54b 100644\n--- a/requests/api.py\n+++ b/requests/api.py\n@@ -23,11 +23,25 @@ def request(method, url, **kwargs):\n     :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.\n     :param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.\n     :param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`.\n-    :param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``{'name': file-tuple}``) for multipart encoding upload.\n-        ``file-tuple`` can be a 2-tuple ``('filename', fileobj)``, 3-tuple ``('filename', fileobj, 'content_type')``\n-        or a 4-tuple ``('filename', fileobj, 'content_type', custom_headers)``, where ``'content-type'`` is a string\n-        defining the content type of the given file and ``custom_headers`` a dict-like object containing additional headers\n-        to add for the file.\n+    :param files: (optional) For uploading files with ``multipart/form-data`` encoding,\n+        either a dictionary of ``'field_name': file_info`` items,\n+        or a list of ``('field_name', file_info)`` tuples. Each ``file_info`` can be:\n+\n+        * A ``file_obj``.\n+        * A 2-tuple ``('filename', file_obj)``.\n+        * A 3-tuple ``('filename', file_obj, 'content_type')``.\n+        * A 4-tuple ``('filename', file_obj, 'content_type', custom_headers)``.\n+\n+        Where:\n+\n+        * ``file_obj`` is a binary-mode file-like object to read the file contents from.\n+          or a ``str`` or ``bytes`` containing the file contents.\n+        * ``'content_type'`` is a string defining the content type of the given file.\n+        * ``custom_headers`` is a dict-like object containing additional headers\n+          to add for the file.\n+\n+        For an example of the dict syntax, see :ref:`multipart_file`.\n+        For an example of the list-of-tuples syntax, see :ref:`multiple_multipart_files_same_field`.\n     :param auth: (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth.\n     :param timeout: (optional) How many seconds to wait for the server to send data\n         before giving up, as a float, or a :ref:`(connect timeout, read\n"
  },
  {
    "id": 1231924796,
    "number": 6347,
    "user": {
      "login": "RichieB2B",
      "id": 1461970
    },
    "diff_url": "https://github.com/psf/requests/pull/6347.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6347",
    "title": "Use context manager to open files",
    "body": "In the documentation for `POST a Multipart-Encoded File` the example code opens the file inline but never closes it. This leaves the file open until the script ends. It is better to use a context manager even in example code.\r\n\r\nOriginal:\r\n```\r\n>>> url = 'https://httpbin.org/post'\r\n>>> files = {'file': open('report.xls', 'rb')}\r\n\r\n>>> r = requests.post(url, files=files)\r\n>>> r.text\r\n```\r\n\r\nReplace with:\r\n```\r\n>>> url = 'https://httpbin.org/post'\r\n>>> with open('report.xls', 'rb') as fd:\r\n...     files = {'file': fd}\r\n\r\n...     r = requests.post(url, files=files)\r\n...     r.text\r\n```",
    "diff": "diff --git a/docs/user/advanced.rst b/docs/user/advanced.rst\nindex c664a83d30..1a721c36d1 100644\n--- a/docs/user/advanced.rst\n+++ b/docs/user/advanced.rst\n@@ -399,10 +399,11 @@ upload image files to an HTML form with a multiple file field 'images'::\n To do that, just set files to a list of tuples of ``(form_field_name, file_info)``::\n \n     >>> url = 'https://httpbin.org/post'\n-    >>> multiple_files = [\n-    ...     ('images', ('foo.png', open('foo.png', 'rb'), 'image/png')),\n-    ...     ('images', ('bar.png', open('bar.png', 'rb'), 'image/png'))]\n-    >>> r = requests.post(url, files=multiple_files)\n+    >>> with open('foo.png', 'rb') as fd1, open('bar.png', 'rb') as fd2:\n+    ...     multiple_files = [\n+    ...         ('images', ('foo.png', fd1, 'image/png')),\n+    ...         ('images', ('bar.png', fd2, 'image/png'))]\n+    ...     r = requests.post(url, files=multiple_files)\n     >>> r.text\n     {\n       ...\ndiff --git a/docs/user/quickstart.rst b/docs/user/quickstart.rst\nindex 7fac5ce735..54fe0b9ade 100644\n--- a/docs/user/quickstart.rst\n+++ b/docs/user/quickstart.rst\n@@ -303,9 +303,10 @@ POST a Multipart-Encoded File\n Requests makes it simple to upload Multipart-encoded files::\n \n     >>> url = 'https://httpbin.org/post'\n-    >>> files = {'file': open('report.xls', 'rb')}\n+    >>> with open('report.xls', 'rb') as fd:\n+    ...     files = {'file': fd}\n \n-    >>> r = requests.post(url, files=files)\n+    ...     r = requests.post(url, files=files)\n     >>> r.text\n     {\n       ...\n@@ -318,9 +319,10 @@ Requests makes it simple to upload Multipart-encoded files::\n You can set the filename, content_type and headers explicitly::\n \n     >>> url = 'https://httpbin.org/post'\n-    >>> files = {'file': ('report.xls', open('report.xls', 'rb'), 'application/vnd.ms-excel', {'Expires': '0'})}\n+    >>> with open('report.xls', 'rb') as fd:\n+    ...     files = {'file': ('report.xls', fd, 'application/vnd.ms-excel', {'Expires': '0'})}\n \n-    >>> r = requests.post(url, files=files)\n+    ...     r = requests.post(url, files=files)\n     >>> r.text\n     {\n       ...\n"
  },
  {
    "id": 1100344664,
    "number": 6270,
    "user": {
      "login": "ErikBjare",
      "id": 1405370
    },
    "diff_url": "https://github.com/psf/requests/pull/6270.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6270",
    "title": "fix: pass response (self) to ConnectionError constructor",
    "body": "As far as I can tell, this is the only internal construction of `ConnectionError` that doesn't include `request` or `response`.\r\n\r\nPart of improving typing for exceptions in requests: https://github.com/python/typeshed/pull/8989",
    "diff": "diff --git a/requests/models.py b/requests/models.py\nindex 617a4134e5..cdc5aaaa1a 100644\n--- a/requests/models.py\n+++ b/requests/models.py\n@@ -819,7 +819,7 @@ def generate():\n                 except DecodeError as e:\n                     raise ContentDecodingError(e)\n                 except ReadTimeoutError as e:\n-                    raise ConnectionError(e)\n+                    raise ConnectionError(e, response=self)\n                 except SSLError as e:\n                     raise RequestsSSLError(e)\n             else:\n"
  },
  {
    "id": 1096454591,
    "number": 6265,
    "user": {
      "login": "mgorny",
      "id": 110765
    },
    "diff_url": "https://github.com/psf/requests/pull/6265.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6265",
    "title": "Fix setuptools deprecation warnings",
    "body": "Update keys used in `setup.cfg` in order to fix the following setuptools deprecation warnings:\r\n\r\n> The license_file parameter is deprecated, use license_files instead.\r\n\r\n> Usage of dash-separated 'provides-extra' will not be supported\r\n> in future versions. Please use the underscore name 'provides_extra'\r\n> instead\r\n\r\n> Usage of dash-separated 'requires-dist' will not be supported\r\n> in future versions. Please use the underscore name 'requires_dist'\r\n> instead",
    "diff": "diff --git a/setup.cfg b/setup.cfg\nindex bf21c81cc0..4cee707024 100644\n--- a/setup.cfg\n+++ b/setup.cfg\n@@ -1,9 +1,9 @@\n [metadata]\n-license_file = LICENSE\n-provides-extra =\n+license_files = LICENSE\n+provides_extra =\n     socks\n     use_chardet_on_py3\n-requires-dist =\n+requires_dist =\n     certifi>=2017.4.17\n     charset_normalizer>=2,<4\n     idna>=2.5,<4\n"
  },
  {
    "id": 1052498822,
    "number": 6237,
    "user": {
      "login": "mhayworth",
      "id": 73895101
    },
    "diff_url": "https://github.com/psf/requests/pull/6237.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6237",
    "title": "Improved documentation of exceptions in api.rst",
    "body": "Added use cases and inheritance information for the common types of exceptions that are listed in docs/api.rst. Documentation already exists for the main interface, request sessions, and lower-level classes on the website, so those may want to be moved into api.rst as well for consistent formatting. Alternatively, this information could also be chosen to move to the website. I am open to any changes we might want to make to improve this documentation and help users better understand the exceptions their code may be throwing.",
    "diff": "diff --git a/docs/api.rst b/docs/api.rst\nindex 83eb58787a..17a0dc00ed 100644\n--- a/docs/api.rst\n+++ b/docs/api.rst\n@@ -4,7 +4,6 @@ Developer Interface\n ===================\n \n .. module:: requests\n-\n This part of the documentation covers all the interfaces of Requests. For\n parts where Requests depends on external libraries, we document the most\n important right here and provide links to the canonical documentation.\n@@ -17,7 +16,6 @@ All of Requests' functionality can be accessed by these 7 methods.\n They all return an instance of the :class:`Response <Response>` object.\n \n .. autofunction:: request\n-\n .. autofunction:: head\n .. autofunction:: get\n .. autofunction:: post\n@@ -27,15 +25,67 @@ They all return an instance of the :class:`Response <Response>` object.\n \n Exceptions\n ----------\n+This list contains some of the most commonly encountered exceptions and their descriptions.\n+This list is not exhaustive. More types of niche exceptions and the error message\n+that will accompany them can be found at requests-exceptions_. Errors may be thrown\n+automatically through API functions such as the ``requests.HTTPError`` or thrown\n+manually through user-specified conditions such as in the ``requests.ReadTimeout``.\n+Descriptions for both aforementioned exceptions are below.\n+\n+.. _requests-exceptions: https://github.com/psf/requests/blob/main/requests/exceptions.py\n \n .. autoexception:: requests.RequestException\n+Serves as a generic exception to catch any errors thrown by the Requests library. \n+Useful for preventing uncatched exceptions due to sending HTTP requests.\n+Can be used in tandem with Python's ``type()`` to check the specific\n+exception and respond accordingly.\n+\n .. autoexception:: requests.ConnectionError\n+Serves as a generic exception to catch any errors related to establishing a\n+connection between the client and the server. Is inherited by the\n+``requests.ConnectTimeout`` exception and could be the result of an\n+unstable connection from either the client or the server. Requests that\n+result in this exception are known to be safe to try again.\n+Multiple requests that result in this error could mean either the\n+client lacks an internet connection or the server is offline.\n+\n .. autoexception:: requests.HTTPError\n+Comes from the urllib3_ library and is a generic exception to catch errors\n+related to HTTP warnings, invalid SSL certificates, or proxy server errors. \n+Errors of this type are usually on the server's side or invalid use of the server's API.\n+\n+.. _urllib3: https://github.com/urllib3/urllib3/blob/main/src/urllib3/exceptions.py#L16\n+\n .. autoexception:: requests.URLRequired\n+A derived exception from the ``requests.RequestException`` class. Check the spelling\n+on the URL and try again. Could also arise due to a domain becoming\n+inactive and needing to be renewed.\n+\n .. autoexception:: requests.TooManyRedirects\n+A derived exception from the ``requests.RequestException`` class. Happens when the\n+page requested redirects to another page too many times. Usually happens when\n+two or more pages redirect to each other in an infinite loop. If this is not the case,\n+consider requesting a page further down the request pipeline to prevent this error.\n+\n .. autoexception:: requests.ConnectTimeout\n+A derived exception from the ``requests.ConnectionError`` class. Happens when\n+the request takes too long to establish a connection to the server.\n+This exception could be the result of unstable internet and this type of\n+request is safe to try again.\n+\n .. autoexception:: requests.ReadTimeout\n+An exception that occurs when a connection is established with the server,\n+but the server does not send any data in a given amount of time. The time\n+for this excpetion can be set by the user and thrown manually using\n+Python's Time-Library_.\n+\n+.. _Time-Library: https://docs.python.org/3/library/time.html\n+\n .. autoexception:: requests.Timeout\n+Serves as a generic exception to catch any errors related to timeouts.\n+Catching this exception will be able to catch both ``requests.ConnectTimeout``\n+and ``requests.ReadTimeout`` errors. The ``type()`` of the error\n+can then be checked and responded to accordingly.\n \n \n Request Sessions\n"
  },
  {
    "id": 991608482,
    "number": 6192,
    "user": {
      "login": "kianelbo",
      "id": 34425507
    },
    "diff_url": "https://github.com/psf/requests/pull/6192.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6192",
    "title": "Fix the popitem bug in cookies",
    "body": "Closes #6190 ",
    "diff": "diff --git a/src/requests/cookies.py b/src/requests/cookies.py\nindex f69d0cda9e..a1b51a4082 100644\n--- a/src/requests/cookies.py\n+++ b/src/requests/cookies.py\n@@ -346,6 +346,13 @@ def __delitem__(self, name):\n         \"\"\"\n         remove_cookie_by_name(self, name)\n \n+    def popitem(self):\n+        if not self:\n+            raise KeyError(\"Cookie jar is empty\")\n+        next_cookie = next(iter(self))\n+        self.clear(next_cookie.domain, next_cookie.path, next_cookie.name)\n+        return next_cookie, next_cookie.value\n+\n     def set_cookie(self, cookie, *args, **kwargs):\n         if (\n             hasattr(cookie.value, \"startswith\")\ndiff --git a/tests/test_requests.py b/tests/test_requests.py\nindex 5d8d2cd3af..1f5d05c173 100644\n--- a/tests/test_requests.py\n+++ b/tests/test_requests.py\n@@ -1364,6 +1364,26 @@ def test_cookie_duplicate_names_raises_cookie_conflict_error(self):\n         with pytest.raises(requests.cookies.CookieConflictError):\n             jar.get(key)\n \n+    def test_cookies_popitem(self):\n+        jar = requests.cookies.RequestsCookieJar()\n+        with pytest.raises(KeyError):\n+            jar.popitem()\n+\n+        jar.set(\"1st_key\", \"1st_value\")\n+        jar.set(\"2nd_key\", \"2nd_value\")\n+        cookie = next(iter(jar))\n+        assert jar.popitem() == (cookie, cookie.value)\n+\n+        expected_jar = requests.cookies.RequestsCookieJar()\n+        expected_jar.set(\"2nd_key\", \"2nd_value\")\n+        assert jar == expected_jar\n+\n+        jar.set(\"2nd_key\", \"2nd_value\", path=\"another_path\")\n+        jar.set(\"2nd_key\", \"2nd_value\", path=\"another_path\", domain=\"another_dom\")\n+        cookie = next(iter(jar))\n+        assert jar.popitem() == (cookie, cookie.value)\n+        assert len(jar) == 2\n+\n     def test_cookie_policy_copy(self):\n         class MyCookiePolicy(cookielib.DefaultCookiePolicy):\n             pass\n"
  },
  {
    "id": 982773285,
    "number": 6186,
    "user": {
      "login": "kkirsche",
      "id": 947110
    },
    "diff_url": "https://github.com/psf/requests/pull/6186.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6186",
    "title": "refactor: prefer f-strings to format where fits within black lengths",
    "body": "Python added f-strings in version 3.6, with [PEP 498](https://www.python.org/dev/peps/pep-0498/). F-strings are a flexible and powerful way to format strings. They make the code shorter and more readable, since the code now looks more like the output.\r\n\r\nF-Strings are also more performant than using `.format` (e.g. see [F-String Speed Considerations](https://realpython.com/lessons/f-strings-speed-considerations-performance/), [F-String Benchmarks](https://www.scivision.dev/python-f-string-speed/) for two quick examples) because the python runtime can insert the variables into the string while parsing rather than backtracking to insert them into the placeholders, reducing the number of times the lines need to be processed",
    "diff": "diff --git a/requests/__init__.py b/requests/__init__.py\nindex 7ac8e297b8..3d2df2a821 100644\n--- a/requests/__init__.py\n+++ b/requests/__init__.py\n@@ -94,9 +94,10 @@ def _check_cryptography(cryptography_version):\n         return\n \n     if cryptography_version < [1, 3, 4]:\n-        warning = \"Old version of cryptography ({}) may cause slowdown.\".format(\n-            cryptography_version\n+        warning = (\n+            f\"Old version of cryptography ({cryptography_version}) may cause slowdown.\"\n         )\n+\n         warnings.warn(warning, RequestsDependencyWarning)\n \n \ndiff --git a/requests/auth.py b/requests/auth.py\nindex 9733686ddb..7a13af9d84 100644\n--- a/requests/auth.py\n+++ b/requests/auth.py\n@@ -59,8 +59,8 @@ def _basic_auth_str(username, password):\n     if isinstance(password, str):\n         password = password.encode(\"latin1\")\n \n-    authstr = \"Basic \" + to_native_string(\n-        b64encode(b\":\".join((username, password))).strip()\n+    authstr = (\n+        f'Basic {to_native_string(b64encode(b\":\".join((username, password))).strip())}'\n     )\n \n     return authstr\ndiff --git a/requests/utils.py b/requests/utils.py\nindex ad5358381a..7457a80a4a 100644\n--- a/requests/utils.py\n+++ b/requests/utils.py\n@@ -840,9 +840,9 @@ def select_proxy(url, proxies):\n         return proxies.get(urlparts.scheme, proxies.get(\"all\"))\n \n     proxy_keys = [\n-        urlparts.scheme + \"://\" + urlparts.hostname,\n+        f\"{urlparts.scheme}://{urlparts.hostname}\",\n         urlparts.scheme,\n-        \"all://\" + urlparts.hostname,\n+        f\"all://{urlparts.hostname}\",\n         \"all\",\n     ]\n     proxy = None\n"
  },
  {
    "id": 982763586,
    "number": 6185,
    "user": {
      "login": "kkirsche",
      "id": 947110
    },
    "diff_url": "https://github.com/psf/requests/pull/6185.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6185",
    "title": "refactor: use contextlib.suppress instead of empty except",
    "body": "Use [contextlib](https://docs.python.org/3/library/contextlib.html)'s suppress method to silence a specific error, instead of passing in an exception handler.\r\n\r\nThe context manager slightly shortens the code and significantly clarifies the author's intention to ignore the specific errors. The standard library feature was introduced following a [discussion](https://bugs.python.org/issue15806), where the consensus was that\r\n\r\n> A key benefit here is in the priming effect for readers... The with statement form makes it clear before you start reading the code that certain exceptions won't propagate.",
    "diff": "diff --git a/requests/__init__.py b/requests/__init__.py\nindex 7ac8e297b8..bd5df28583 100644\n--- a/requests/__init__.py\n+++ b/requests/__init__.py\n@@ -38,6 +38,7 @@\n :license: Apache 2.0, see LICENSE for more details.\n \"\"\"\n \n+import contextlib\n import warnings\n \n import urllib3\n@@ -117,7 +118,7 @@ def _check_cryptography(cryptography_version):\n # Attempt to enable urllib3's fallback for SNI support\n # if the standard library doesn't support SNI or the\n # 'ssl' library isn't available.\n-try:\n+with contextlib.suppress(ImportError):\n     try:\n         import ssl\n     except ImportError:\n@@ -132,8 +133,6 @@ def _check_cryptography(cryptography_version):\n         from cryptography import __version__ as cryptography_version\n \n         _check_cryptography(cryptography_version)\n-except ImportError:\n-    pass\n \n # urllib3's DependencyWarnings should be silenced.\n from urllib3.exceptions import DependencyWarning\ndiff --git a/requests/sessions.py b/requests/sessions.py\nindex 6cb3b4dae3..0f65bc08e5 100644\n--- a/requests/sessions.py\n+++ b/requests/sessions.py\n@@ -5,6 +5,7 @@\n This module provides a Session object to manage and persist settings across\n requests (cookies, auth, proxies).\n \"\"\"\n+import contextlib\n import os\n import sys\n import time\n@@ -734,12 +735,10 @@ def send(self, request, **kwargs):\n \n         # If redirects aren't being followed, store the response on the Request for Response.next().\n         if not allow_redirects:\n-            try:\n+            with contextlib.suppress(StopIteration):\n                 r._next = next(\n                     self.resolve_redirects(r, request, yield_requests=True, **kwargs)\n                 )\n-            except StopIteration:\n-                pass\n \n         if not stream:\n             r.content\ndiff --git a/requests/utils.py b/requests/utils.py\nindex ad5358381a..54cf0fc5a2 100644\n--- a/requests/utils.py\n+++ b/requests/utils.py\n@@ -200,7 +200,8 @@ def get_netrc_auth(url, raise_errors=False):\n     else:\n         netrc_locations = (f\"~/{f}\" for f in NETRC_FILES)\n \n-    try:\n+    # App Engine hack requires suppressing these errors\n+    with contextlib.suppress(ImportError, AttributeError):\n         from netrc import NetrcParseError, netrc\n \n         netrc_path = None\n@@ -243,10 +244,6 @@ def get_netrc_auth(url, raise_errors=False):\n             if raise_errors:\n                 raise\n \n-    # App Engine hackiness.\n-    except (ImportError, AttributeError):\n-        pass\n-\n \n def guess_filename(obj):\n     \"\"\"Tries to guess the filename of the given object.\"\"\"\n"
  },
  {
    "id": 974000651,
    "number": 6172,
    "user": {
      "login": "RichieB2B",
      "id": 1461970
    },
    "diff_url": "https://github.com/psf/requests/pull/6172.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6172",
    "title": "Clarify setting proxies exceptions",
    "body": "This adds an example for the `NO_PROXY` environment variable and explains making exceptions to the proxy settings using per-host proxy entries.",
    "diff": "diff --git a/docs/user/advanced.rst b/docs/user/advanced.rst\nindex 2a91401a25..2c45ed3930 100644\n--- a/docs/user/advanced.rst\n+++ b/docs/user/advanced.rst\n@@ -621,6 +621,7 @@ to your needs)::\n \n     $ export HTTP_PROXY=\"http://10.10.1.10:3128\"\n     $ export HTTPS_PROXY=\"http://10.10.1.10:1080\"\n+    $ export NO_PROXY=\"www.example.com,10.20.1.128,10.20.2.0/24\"\n     $ export ALL_PROXY=\"socks5://10.10.1.10:3434\"\n \n     $ python\n@@ -649,6 +650,20 @@ any request to the given scheme and exact hostname.\n \n Note that proxy URLs must include the scheme.\n \n+To exclude a specific scheme and host from a proxy configuration,\n+set it to an empty string so a direct connection is used:\n+\n+::\n+\n+    proxies = {\n+      'http': 'http://10.10.1.10:3128',\n+      'https': 'http://10.10.1.10:1080',\n+      'http://10.20.1.128': '',\n+    }\n+\n+Note that adding ``no_proxy`` to the ``proxies`` dictionary for this purpose is not\n+supported.\n+\n Finally, note that using a proxy for https connections typically requires your\n local machine to trust the proxy's root certificate. By default the list of\n certificates trusted by Requests can be found with::\n"
  },
  {
    "id": 971256771,
    "number": 6166,
    "user": {
      "login": "Cubicpath",
      "id": 24359380
    },
    "diff_url": "https://github.com/psf/requests/pull/6166.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6166",
    "title": "Add union operator support to CaseInsensitiveDict (PEP 584)",
    "body": "Adds support for the `|` and `|=` operators for `CaseInsensitiveDict`.\r\n\r\nThis copies the functionality of `dict` from [PEP 584](https://peps.python.org/pep-0584). Even though this PEP was implemented in 3.9, the functionality implemented to `CaseInsensitiveDict` still works in all currently supported versions (3.7+).\r\n\r\nThis also brings a level of consistency with other Mapping types, i.e. `OrderedDict`, `MappingProxyType`, `ChainMap`, `WeakKeyDictionary`, and more have this as supported behavior.\r\n\r\nThe code itself is a slightly modified version of the [reference implementation](https://peps.python.org/pep-0584/#reference-implementation).\r\n\r\nExample usage:\r\n```python3\r\nsession = Session()\r\nsession.headers |= {'Accept', 'application/json'}\r\n```",
    "diff": "diff --git a/requests/structures.py b/requests/structures.py\nindex 188e13e482..7550b70dc5 100644\n--- a/requests/structures.py\n+++ b/requests/structures.py\n@@ -60,6 +60,28 @@ def __iter__(self):\n     def __len__(self):\n         return len(self._store)\n \n+    def __or__(self, other):\n+        if isinstance(other, Mapping):\n+            new = CaseInsensitiveDict(self)\n+        else:\n+            return NotImplemented\n+\n+        new.update(other)\n+        return new\n+\n+    def __ror__(self, other):\n+        if isinstance(other, Mapping):\n+            new = CaseInsensitiveDict(other)\n+        else:\n+            return NotImplemented\n+\n+        new.update(self)\n+        return new\n+\n+    def __ior__(self, other):\n+        self.update(other)\n+        return self\n+\n     def lower_items(self):\n         \"\"\"Like iteritems(), but with all lowercase keys.\"\"\"\n         return ((lowerkey, keyval[1]) for (lowerkey, keyval) in self._store.items())\ndiff --git a/tests/test_structures.py b/tests/test_structures.py\nindex e2fd5baaf2..99c3c2725d 100644\n--- a/tests/test_structures.py\n+++ b/tests/test_structures.py\n@@ -26,6 +26,29 @@ def test_delitem(self, key):\n         del self.case_insensitive_dict[key]\n         assert key not in self.case_insensitive_dict\n \n+    def test_or(self):\n+        assert self.case_insensitive_dict | {\"Accept\": \"application/xml\"} == {\n+            \"Accept\": \"application/xml\"\n+        }\n+        assert self.case_insensitive_dict | {\"Accept-Encoding\": \"gzip\"} == {\n+            \"Accept\": \"application/json\",\n+            \"Accept-Encoding\": \"gzip\",\n+        }\n+\n+    def test_ror(self):\n+        assert {\"Accept\": \"application/xml\"} | self.case_insensitive_dict == {\n+            \"Accept\": \"application/json\"\n+        }\n+        assert {\"Accept-Encoding\": \"gzip\"} | self.case_insensitive_dict == {\n+            \"Accept\": \"application/json\",\n+            \"Accept-Encoding\": \"gzip\",\n+        }\n+\n+    def test_ior(self):\n+        copy = self.case_insensitive_dict.copy()\n+        copy |= {\"Accept\": \"application/xml\"}\n+        assert copy == {\"Accept\": \"application/xml\"}\n+\n     def test_lower_items(self):\n         assert list(self.case_insensitive_dict.lower_items()) == [\n             (\"accept\", \"application/json\")\n"
  },
  {
    "id": 964798792,
    "number": 6162,
    "user": {
      "login": "ValdikSS",
      "id": 3054729
    },
    "diff_url": "https://github.com/psf/requests/pull/6162.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6162",
    "title": "Fix HTTPS websites with system-wide HTTP proxy on Windows",
    "body": "Due to urllib bug, requests fails to open any HTTPS websites on Windows\r\nif there's system-wide HTTP proxy configured. This is because urllib\r\nincorrectly prepends the protocol to the proxy host and port, as in:\r\n\r\n```\r\n{'http':  'http://host:port',\r\n 'https': 'https://host:port',\r\n 'ftp':   'ftp://host:port'}\r\n```\r\n\r\nSuch configuration forces urllib3 to use HTTPS proxy (Secure Web Proxy,\r\nalso known as TLS Proxy) for HTTPS URLs, which the configured proxy\r\nmost likely does not support.\r\n\r\nDetect incorrect behavior and rewrite the protocol to http.",
    "diff": "diff --git a/requests/utils.py b/requests/utils.py\nindex ad5358381a..cf6a6abe2b 100644\n--- a/requests/utils.py\n+++ b/requests/utils.py\n@@ -816,6 +816,25 @@ def get_proxy(key):\n     return False\n \n \n+def getproxies_fix_system_proxies_windows():\n+    \"\"\"Fix for urllib getproxies() bug on Windows with Python < 3.10.5\n+    See: https://bugs.python.org/issue42627\n+\n+    :rtype: dict\n+    \"\"\"\n+    proxy_hosts = []\n+    protocols = {\"http\", \"https\", \"ftp\"}\n+    proxies = getproxies()\n+    if proxies.keys() != protocols:\n+        return proxies\n+    for protocol in proxies:\n+        proxy_hosts.append(proxies[protocol].lstrip(protocol + \"://\"))\n+    proxy_hosts = list(set(proxy_hosts))\n+    if len(proxy_hosts) == 1:\n+        return {protocol: \"http://\" + proxy_hosts[0] for protocol in protocols}\n+    return proxies\n+\n+\n def get_environ_proxies(url, no_proxy=None):\n     \"\"\"\n     Return a dict of environment proxies.\n@@ -824,6 +843,8 @@ def get_environ_proxies(url, no_proxy=None):\n     \"\"\"\n     if should_bypass_proxies(url, no_proxy=no_proxy):\n         return {}\n+    elif os.name == 'nt':\n+        return getproxies_fix_system_proxies_windows()\n     else:\n         return getproxies()\n \n"
  },
  {
    "id": 941779042,
    "number": 6137,
    "user": {
      "login": "ogayot",
      "id": 4038023
    },
    "diff_url": "https://github.com/psf/requests/pull/6137.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6137",
    "title": "Allow tests to run if HTTP proxy env variables are already present,",
    "body": "Hello,\r\n\r\nThe following tests fail if HTTP proxy environment variables are already set:\r\n\r\n```\r\nFAILED tests/test_requests.py::TestRequests::test_mixed_case_scheme_acceptable\r\nFAILED tests/test_requests.py::TestRequests::test_HTTP_302_ALLOW_REDIRECT_GET\r\nFAILED tests/test_requests.py::TestRequests::test_errors[http://doesnotexist.google.com-ConnectionError]\r\nFAILED tests/test_requests.py::TestRequests::test_respect_proxy_env_on_send_self_prepared_request\r\nFAILED tests/test_requests.py::TestRequests::test_respect_proxy_env_on_send_session_prepared_request\r\nFAILED tests/test_requests.py::TestRequests::test_respect_proxy_env_on_send_with_redirects\r\nFAILED tests/test_requests.py::TestRequests::test_respect_proxy_env_on_get - ...\r\nFAILED tests/test_requests.py::TestRequests::test_respect_proxy_env_on_request\r\n```\r\n\r\nThe variables affecting the tests are:\r\n\r\n * `http_proxy` (and `https_proxy` potentially)\r\n * `no_proxy`\r\n\r\nFixed by overriding their value if they are already present in the environment.\r\n\r\nThe `override_environ` function would fail if attempting to delete a variable that is not in the environment. Added a `suppress(KeyError)` construct around the `del` instruction to fix it.\r\n\r\nThanks,\r\nOilvier",
    "diff": "diff --git a/tests/test_requests.py b/tests/test_requests.py\nindex b724264166..68602d7b22 100644\n--- a/tests/test_requests.py\n+++ b/tests/test_requests.py\n@@ -189,21 +189,23 @@ def test_whitespaces_are_removed_from_url(self):\n     @pytest.mark.parametrize(\"scheme\", (\"http://\", \"HTTP://\", \"hTTp://\", \"HttP://\"))\n     def test_mixed_case_scheme_acceptable(self, httpbin, scheme):\n         s = requests.Session()\n-        s.proxies = getproxies()\n-        parts = urlparse(httpbin(\"get\"))\n-        url = scheme + parts.netloc + parts.path\n-        r = requests.Request(\"GET\", url)\n-        r = s.send(r.prepare())\n-        assert r.status_code == 200, f\"failed for scheme {scheme}\"\n+        with override_environ(http_proxy=None, https_proxy=None):\n+            s.proxies = getproxies()\n+            parts = urlparse(httpbin(\"get\"))\n+            url = scheme + parts.netloc + parts.path\n+            r = requests.Request(\"GET\", url)\n+            r = s.send(r.prepare())\n+            assert r.status_code == 200, f\"failed for scheme {scheme}\"\n \n     def test_HTTP_200_OK_GET_ALTERNATIVE(self, httpbin):\n         r = requests.Request(\"GET\", httpbin(\"get\"))\n         s = requests.Session()\n-        s.proxies = getproxies()\n+        with override_environ(http_proxy=None, https_proxy=None):\n+            s.proxies = getproxies()\n \n-        r = s.send(r.prepare())\n+            r = s.send(r.prepare())\n \n-        assert r.status_code == 200\n+            assert r.status_code == 200\n \n     def test_HTTP_302_ALLOW_REDIRECT_GET(self, httpbin):\n         r = requests.get(httpbin(\"redirect\", \"1\"))\n@@ -578,8 +580,9 @@ def test_basicauth_encodes_byte_strings(self):\n         ),\n     )\n     def test_errors(self, url, exception):\n-        with pytest.raises(exception):\n-            requests.get(url, timeout=1)\n+        with override_environ(http_proxy=None, https_proxy=None):\n+            with pytest.raises(exception):\n+                requests.get(url, timeout=1)\n \n     def test_proxy_error(self):\n         # any proxy related error (address resolution, no route to host, etc) should result in a ProxyError\n@@ -602,14 +605,14 @@ def test_proxy_error_on_bad_url(self, httpbin, httpbin_secure):\n             requests.get(httpbin(), proxies={\"http\": \"http:///example.com:8080\"})\n \n     def test_respect_proxy_env_on_send_self_prepared_request(self, httpbin):\n-        with override_environ(http_proxy=INVALID_PROXY):\n+        with override_environ(no_proxy=None, http_proxy=INVALID_PROXY):\n             with pytest.raises(ProxyError):\n                 session = requests.Session()\n                 request = requests.Request(\"GET\", httpbin())\n                 session.send(request.prepare())\n \n     def test_respect_proxy_env_on_send_session_prepared_request(self, httpbin):\n-        with override_environ(http_proxy=INVALID_PROXY):\n+        with override_environ(no_proxy=None, http_proxy=INVALID_PROXY):\n             with pytest.raises(ProxyError):\n                 session = requests.Session()\n                 request = requests.Request(\"GET\", httpbin())\n@@ -617,7 +620,7 @@ def test_respect_proxy_env_on_send_session_prepared_request(self, httpbin):\n                 session.send(prepared)\n \n     def test_respect_proxy_env_on_send_with_redirects(self, httpbin):\n-        with override_environ(http_proxy=INVALID_PROXY):\n+        with override_environ(no_proxy=None, http_proxy=INVALID_PROXY):\n             with pytest.raises(ProxyError):\n                 session = requests.Session()\n                 url = httpbin(\"redirect/1\")\n@@ -626,13 +629,13 @@ def test_respect_proxy_env_on_send_with_redirects(self, httpbin):\n                 session.send(request.prepare())\n \n     def test_respect_proxy_env_on_get(self, httpbin):\n-        with override_environ(http_proxy=INVALID_PROXY):\n+        with override_environ(no_proxy=None, http_proxy=INVALID_PROXY):\n             with pytest.raises(ProxyError):\n                 session = requests.Session()\n                 session.get(httpbin())\n \n     def test_respect_proxy_env_on_request(self, httpbin):\n-        with override_environ(http_proxy=INVALID_PROXY):\n+        with override_environ(no_proxy=None, http_proxy=INVALID_PROXY):\n             with pytest.raises(ProxyError):\n                 session = requests.Session()\n                 session.request(method=\"GET\", url=httpbin())\ndiff --git a/tests/utils.py b/tests/utils.py\nindex 6cb75bfb6a..7871a32254 100644\n--- a/tests/utils.py\n+++ b/tests/utils.py\n@@ -7,7 +7,8 @@ def override_environ(**kwargs):\n     save_env = dict(os.environ)\n     for key, value in kwargs.items():\n         if value is None:\n-            del os.environ[key]\n+            with contextlib.suppress(KeyError):\n+                del os.environ[key]\n         else:\n             os.environ[key] = value\n     try:\n"
  },
  {
    "id": 929884272,
    "number": 6123,
    "user": {
      "login": "deosrc",
      "id": 50599779
    },
    "diff_url": "https://github.com/psf/requests/pull/6123.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6123",
    "title": "Fix error caused by invalid charset in response headers",
    "body": "Ran into an error where we were having an exception raised from the requests library: `'bool' object has no attribute 'strip'`\r\n\r\nSeems the server was returning an empty charset value in the Content-Type response header of `application/json;charset`. This was causing the line below to default the value to a boolean `True`:\r\n\r\nhttps://github.com/psf/requests/blob/2a6f290bc09324406708a4d404a88a45d848ddf9/requests/utils.py#L524-L529\r\n\r\nThis meant that attempting format the charset on the line below, the value was a boolean rather than a string:\r\n\r\nhttps://github.com/psf/requests/blob/2a6f290bc09324406708a4d404a88a45d848ddf9/requests/utils.py#L545-L548\r\n\r\nThis change adds a type check so that the charset is ignored if a value is not provided. The unit test has been modified for coverage, along with expanding the other test scenarios slightly.",
    "diff": "diff --git a/requests/utils.py b/requests/utils.py\nindex 7a881b6444..c82095f84f 100644\n--- a/requests/utils.py\n+++ b/requests/utils.py\n@@ -544,7 +544,8 @@ def get_encoding_from_headers(headers):\n \n     content_type, params = _parse_content_type_header(content_type)\n \n-    if \"charset\" in params:\n+    # Charset will be a bool if it was not provided with a value. Invalid so ignore it.\n+    if \"charset\" in params and not isinstance(params[\"charset\"], bool):\n         return params[\"charset\"].strip(\"'\\\"\")\n \n     if \"text\" in content_type:\ndiff --git a/tests/test_utils.py b/tests/test_utils.py\nindex a714b4700e..544ea7a4ec 100644\n--- a/tests/test_utils.py\n+++ b/tests/test_utils.py\n@@ -610,15 +610,15 @@ def test__parse_content_type_header(value, expected):\n @pytest.mark.parametrize(\n     \"value, expected\",\n     (\n-        (CaseInsensitiveDict(), None),\n-        (\n-            CaseInsensitiveDict({\"content-type\": \"application/json; charset=utf-8\"}),\n-            \"utf-8\",\n-        ),\n-        (CaseInsensitiveDict({\"content-type\": \"text/plain\"}), \"ISO-8859-1\"),\n+        ({}, None),\n+        ({\"content-type\": \"application/json; charset=ISO-8859-1\"},\"ISO-8859-1\",), # Check overriding the charset\n+        ({\"content-type\": \"application/json\"},\"utf-8\",), # application/json default\n+        ({\"content-type\": \"text/plain\"}, \"ISO-8859-1\"), # text default\n+        ({\"content-type\": \"some/type;charset\"}, None), # Ignore an invalid charset\n     ),\n )\n def test_get_encoding_from_headers(value, expected):\n+    value = CaseInsensitiveDict(value)\n     assert get_encoding_from_headers(value) == expected\n \n \n"
  },
  {
    "id": 928535043,
    "number": 6122,
    "user": {
      "login": "romanyakovlev",
      "id": 20498421
    },
    "diff_url": "https://github.com/psf/requests/pull/6122.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6122",
    "title": "Request with data which consists of empty values only sends bad request",
    "body": "Case - request with data which consists of empty values only\r\n```python\r\nget('http://localhost:80', data={'foo': None})\r\n```\r\nResponse in nginx:\r\n```\r\n172.17.0.1 - - [05/May/2022:19:34:30 +0000] \"GET / HTTP/1.1\" 200 615 \"-\" \"python-requests/2.27.1\" \"-\"\r\n172.17.0.1 - - [05/May/2022:19:34:30 +0000] \"0\" 400 157 \"-\" \"-\" \"-\"\r\n```\r\nSo it sends second request with bad status code.\r\nHere https://github.com/psf/requests/blob/main/requests/models.py#L576 `length` will be `0` so there is no `Content-Length: 0` header in request.\r\nThe problem occurs there https://github.com/psf/requests/blob/main/requests/adapters.py#L471 .\r\nBecause `request.body` is `''` and 'Content-Length' not in `request.headers` it counts as `chunk=True`.\r\nBecause of that it acts like it has `Transfer-Encoding: chunked` header, and here https://github.com/psf/requests/blob/main/requests/adapters.py#L523-L528 it does not send nothing but `low_conn.send(b'0\\r\\n\\r\\n')`.\r\nI guess thats why It has bad request like this:\r\n```\r\napache_1  | 172.21.0.1 - - [05/May/2022:23:05:44 +0000] \"0\" 400 226\r\n```\r\nThe same behavior is happening on POST request.\r\n```python\r\npost('http://localhost:80', data={'foo': None})\r\n```\r\ngives:\r\n```\r\napache_1  | 172.21.0.1 - - [05/May/2022:23:05:44 +0000] \"POST / HTTP/1.1\" 200 45\r\napache_1  | 172.21.0.1 - - [05/May/2022:23:05:44 +0000] \"0\" 400 226\r\n```\r\nThe raw request will be something like this:\r\n```python\r\nimport socket\r\ns = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\r\ns.connect((\"localhost\", 80))\r\ns.send(\r\n    b'GET / HTTP/1.1\\r\\n'\r\n    b'Host: localhost:80\\r\\n'\r\n    b'User-Agent: python-requests/2.27.1\\r\\n'\r\n    b'Accept-Encoding: gzip, deflate, br\\r\\n'\r\n    b'Accept: */*\\r\\n'\r\n    b'Connection: keep-alive\\r\\n'\r\n    b'\\r\\n'\r\n    b'0\\r\\n\\r\\n' # this thing is added here https://github.com/psf/requests/blob/main/requests/adapters.py#L528\r\n)\r\n\r\nresponse = s.recv(4096)\r\nprint(response)\r\n```\r\nso `0\\r\\n\\r\\n` is the reason of\r\n\r\n`apache_1  | 172.21.0.1 - - [05/May/2022:23:05:44 +0000] \"0\" 400 226`\r\n\r\n\r\nThis PR fixes the problem. Tests for this case are created.\r\n",
    "diff": "diff --git a/requests/models.py b/requests/models.py\nindex 7e1522837f..fcdfb5e3d3 100644\n--- a/requests/models.py\n+++ b/requests/models.py\n@@ -556,7 +556,7 @@ def prepare_body(self, data, files, json=None):\n                 (body, content_type) = self._encode_files(files, data)\n             else:\n                 if data:\n-                    body = self._encode_params(data)\n+                    body = self._encode_params(data) or None\n                     if isinstance(data, basestring) or hasattr(data, \"read\"):\n                         content_type = None\n                     else:\ndiff --git a/tests/test_lowlevel.py b/tests/test_lowlevel.py\nindex 859d07e8a5..d073e7fcc5 100644\n--- a/tests/test_lowlevel.py\n+++ b/tests/test_lowlevel.py\n@@ -426,3 +426,29 @@ def response_handler(sock):\n     assert isinstance(excinfo.value, requests.exceptions.RequestException)\n     assert isinstance(excinfo.value, JSONDecodeError)\n     assert r.text not in str(excinfo.value)\n+\n+\n+@pytest.mark.parametrize(\n+    \"method,include,exclude\",\n+    (\n+        (requests.get, [], [b\"Content-Length:\", b\"Transfer-Encoding:\"]),\n+        (requests.post, [b\"Content-Length: 0\\r\\n\"], [b\"Transfer-Encoding:\"]),\n+    )\n+)\n+def test_empty_urlencoded_form_body(method, include, exclude):\n+    \"\"\"Ensure we use only the specified Host header for chunked requests.\"\"\"\n+    close_server = threading.Event()\n+    server = Server(echo_response_handler, wait_to_close_event=close_server)\n+\n+    with server as (host, port):\n+        url = f\"http://{host}:{port}/\"\n+        resp = method(url, data=((\"a\", None,),))\n+        close_server.set()  # release server block\n+\n+    assert not resp.content.endswith(b\"\\r\\n0\\r\\n\\r\\n\")\n+\n+    for header in include:\n+        assert header in resp.content\n+\n+    for header in exclude:\n+        assert header not in resp.content\n"
  },
  {
    "id": 831484507,
    "number": 6048,
    "user": {
      "login": "mftb",
      "id": 1322800
    },
    "diff_url": "https://github.com/psf/requests/pull/6048.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/6048",
    "title": "Fixing unicode surrogates in POST data lead to exception",
    "body": "Closes #6008\r\nFixing the unicode surrogates issue",
    "diff": "diff --git a/requests/adapters.py b/requests/adapters.py\nindex fe22ff450e..71a94365fc 100644\n--- a/requests/adapters.py\n+++ b/requests/adapters.py\n@@ -435,12 +435,19 @@ def send(self, request, stream=False, timeout=None, verify=True, cert=None, prox\n         else:\n             timeout = TimeoutSauce(connect=timeout, read=timeout)\n \n+\n+        try:\n+            body = request.body.encode(\n+                'utf-8', 'surrogatepass') if isinstance(request.body, str) else request.body\n+        except:\n+            body = request.body\n+\n         try:\n             if not chunked:\n                 resp = conn.urlopen(\n                     method=request.method,\n                     url=url,\n-                    body=request.body,\n+                    body=body,\n                     headers=request.headers,\n                     redirect=False,\n                     assert_same_host=False,\ndiff --git a/requests/models.py b/requests/models.py\nindex dfbea854f9..d160907305 100644\n--- a/requests/models.py\n+++ b/requests/models.py\n@@ -102,8 +102,8 @@ def _encode_params(data):\n                 for v in vs:\n                     if v is not None:\n                         result.append(\n-                            (k.encode('utf-8') if isinstance(k, str) else k,\n-                             v.encode('utf-8') if isinstance(v, str) else v))\n+                            (k.encode('utf-8', 'surrogatepass') if isinstance(k, str) else k,\n+                             v.encode('utf-8', 'surrogatepass') if isinstance(v, str) else v))\n             return urlencode(result, doseq=True)\n         else:\n             return data\ndiff --git a/tests/test_requests.py b/tests/test_requests.py\nindex 328da4bd43..b7aa88c56e 100644\n--- a/tests/test_requests.py\n+++ b/tests/test_requests.py\n@@ -525,6 +525,24 @@ def test_basicauth_encodes_byte_strings(self):\n \n         assert p.headers['Authorization'] == 'Basic xa9zZXJuYW1lOnRlc3TGtg=='\n \n+    def test_post_encodes_surrogate_dict(self):\n+        \"\"\"Ensure that a POST request with a dict containing a utf-8 string with\n+        surrogates in its payload is correctly sent in Python 3.\n+        \"\"\"\n+        data = {'name': 'test\\udced\\udcb3\\udc83.pdf'}\n+        r = requests.post('https://example.com/', data=data)\n+\n+        assert r.status_code == 200\n+\n+    def test_post_encodes_surrogate_string(self):\n+        \"\"\"Ensure that a POST request with a dict containing a utf-8 string with\n+        surrogates in its payload is correctly sent in Python 3.\n+        \"\"\"\n+        payload = 'test\\udced\\udcb3\\udc83.pdf'\n+        r = requests.post('https://example.com/', payload)\n+\n+        assert r.status_code == 200\n+\n     @pytest.mark.parametrize(\n         'url, exception', (\n             # Connecting to an unknown domain should raise a ConnectionError\n"
  },
  {
    "id": 754102555,
    "number": 5953,
    "user": {
      "login": "derekhiggins",
      "id": 883848
    },
    "diff_url": "https://github.com/psf/requests/pull/5953.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/5953",
    "title": "Add ipv6 support to should_bypass_proxies",
    "body": "Add support to should_bypass_proxies to support\r\nIPv6 ipaddresses and CIDRs in no_proxy. Includes\r\nadding IPv6 support to various other helper functions.",
    "diff": "diff --git a/src/requests/utils.py b/src/requests/utils.py\nindex ae6c42f6cb..0363698659 100644\n--- a/src/requests/utils.py\n+++ b/src/requests/utils.py\n@@ -679,18 +679,46 @@ def requote_uri(uri):\n         return quote(uri, safe=safe_without_percent)\n \n \n+def _get_mask_bits(mask, totalbits=32):\n+    \"\"\"Converts a mask from /xx format to a int\n+    to be used as a mask for IP's in int format\n+\n+    Example: if mask is 24 function returns 0xFFFFFF00\n+             if mask is 24 and totalbits=128 function\n+                returns 0xFFFFFF00000000000000000000000000\n+\n+    :rtype: int\n+    \"\"\"\n+    bits = ((1 << mask) - 1) << (totalbits - mask)\n+    return bits\n+\n+\n def address_in_network(ip, net):\n     \"\"\"This function allows you to check if an IP belongs to a network subnet\n \n     Example: returns True if ip = 192.168.1.1 and net = 192.168.1.0/24\n              returns False if ip = 192.168.1.1 and net = 192.168.100.0/24\n+             returns True if ip = 1:2:3:4::1 and net = 1:2:3:4::/64\n \n     :rtype: bool\n     \"\"\"\n-    ipaddr = struct.unpack(\"=L\", socket.inet_aton(ip))[0]\n     netaddr, bits = net.split(\"/\")\n-    netmask = struct.unpack(\"=L\", socket.inet_aton(dotted_netmask(int(bits))))[0]\n-    network = struct.unpack(\"=L\", socket.inet_aton(netaddr))[0] & netmask\n+    if is_ipv4_address(ip) and is_ipv4_address(netaddr):\n+        ipaddr = struct.unpack(\">L\", socket.inet_aton(ip))[0]\n+        netmask = _get_mask_bits(int(bits))\n+        network = struct.unpack(\">L\", socket.inet_aton(netaddr))[0]\n+    elif is_ipv6_address(ip) and is_ipv6_address(netaddr):\n+        ipaddr_msb, ipaddr_lsb = struct.unpack(\n+            \">QQ\", socket.inet_pton(socket.AF_INET6, ip)\n+        )\n+        ipaddr = (ipaddr_msb << 64) ^ ipaddr_lsb\n+        netmask = _get_mask_bits(int(bits), 128)\n+        network_msb, network_lsb = struct.unpack(\n+            \">QQ\", socket.inet_pton(socket.AF_INET6, netaddr)\n+        )\n+        network = (network_msb << 64) ^ network_lsb\n+    else:\n+        return False\n     return (ipaddr & netmask) == (network & netmask)\n \n \n@@ -710,12 +738,39 @@ def is_ipv4_address(string_ip):\n     :rtype: bool\n     \"\"\"\n     try:\n-        socket.inet_aton(string_ip)\n+        socket.inet_pton(socket.AF_INET, string_ip)\n+    except OSError:\n+        return False\n+    return True\n+\n+\n+def is_ipv6_address(string_ip):\n+    \"\"\"\n+    :rtype: bool\n+    \"\"\"\n+    try:\n+        socket.inet_pton(socket.AF_INET6, string_ip)\n     except OSError:\n         return False\n     return True\n \n \n+def compare_ips(a, b):\n+    \"\"\"\n+    Compare 2 IP's, uses socket.inet_pton to normalize IPv6 IPs\n+\n+    :rtype: bool\n+    \"\"\"\n+    if a == b:\n+        return True\n+    try:\n+        return socket.inet_pton(socket.AF_INET6, a) == socket.inet_pton(\n+            socket.AF_INET6, b\n+        )\n+    except OSError:\n+        return False\n+\n+\n def is_valid_cidr(string_network):\n     \"\"\"\n     Very simple check of the cidr format in no_proxy variable.\n@@ -723,17 +778,19 @@ def is_valid_cidr(string_network):\n     :rtype: bool\n     \"\"\"\n     if string_network.count(\"/\") == 1:\n+        address, mask = string_network.split(\"/\")\n         try:\n-            mask = int(string_network.split(\"/\")[1])\n+            mask = int(mask)\n         except ValueError:\n             return False\n \n-        if mask < 1 or mask > 32:\n-            return False\n-\n-        try:\n-            socket.inet_aton(string_network.split(\"/\")[0])\n-        except OSError:\n+        if is_ipv4_address(address):\n+            if mask < 1 or mask > 32:\n+                return False\n+        elif is_ipv6_address(address):\n+            if mask < 1 or mask > 128:\n+                return False\n+        else:\n             return False\n     else:\n         return False\n@@ -790,12 +847,12 @@ def get_proxy(key):\n         # the end of the hostname, both with and without the port.\n         no_proxy = (host for host in no_proxy.replace(\" \", \"\").split(\",\") if host)\n \n-        if is_ipv4_address(parsed.hostname):\n+        if is_ipv4_address(parsed.hostname) or is_ipv6_address(parsed.hostname):\n             for proxy_ip in no_proxy:\n                 if is_valid_cidr(proxy_ip):\n                     if address_in_network(parsed.hostname, proxy_ip):\n                         return True\n-                elif parsed.hostname == proxy_ip:\n+                elif compare_ips(parsed.hostname, proxy_ip):\n                     # If no_proxy ip was defined in plain IP notation instead of cidr notation &\n                     # matches the IP of the index\n                     return True\ndiff --git a/tests/test_utils.py b/tests/test_utils.py\nindex 5e9b56ea64..befbb46b3b 100644\n--- a/tests/test_utils.py\n+++ b/tests/test_utils.py\n@@ -14,9 +14,11 @@\n from requests.cookies import RequestsCookieJar\n from requests.structures import CaseInsensitiveDict\n from requests.utils import (\n+    _get_mask_bits,\n     _parse_content_type_header,\n     add_dict_to_cookiejar,\n     address_in_network,\n+    compare_ips,\n     dotted_netmask,\n     extract_zipped_paths,\n     get_auth_from_url,\n@@ -263,8 +265,15 @@ def test_invalid(self, value):\n \n \n class TestIsValidCIDR:\n-    def test_valid(self):\n-        assert is_valid_cidr(\"192.168.1.0/24\")\n+    @pytest.mark.parametrize(\n+        \"value\",\n+        (\n+            \"192.168.1.0/24\",\n+            \"1:2:3:4::/64\",\n+        ),\n+    )\n+    def test_valid(self, value):\n+        assert is_valid_cidr(value)\n \n     @pytest.mark.parametrize(\n         \"value\",\n@@ -274,6 +283,11 @@ def test_valid(self):\n             \"192.168.1.0/128\",\n             \"192.168.1.0/-1\",\n             \"192.168.1.999/24\",\n+            \"1:2:3:4::1\",\n+            \"1:2:3:4::/a\",\n+            \"1:2:3:4::0/321\",\n+            \"1:2:3:4::/-1\",\n+            \"1:2:3:4::12211/64\",\n         ),\n     )\n     def test_invalid(self, value):\n@@ -287,6 +301,12 @@ def test_valid(self):\n     def test_invalid(self):\n         assert not address_in_network(\"172.16.0.1\", \"192.168.1.0/24\")\n \n+    def test_valid_v6(self):\n+        assert address_in_network(\"1:2:3:4::1111\", \"1:2:3:4::/64\")\n+\n+    def test_invalid_v6(self):\n+        assert not address_in_network(\"1:2:3:4:1111\", \"1:2:3:4::/124\")\n+\n \n class TestGuessFilename:\n     @pytest.mark.parametrize(\n@@ -722,6 +742,11 @@ def test_urldefragauth(url, expected):\n         (\"http://172.16.1.12:5000/\", False),\n         (\"http://google.com:5000/v1.0/\", False),\n         (\"file:///some/path/on/disk\", True),\n+        (\"http://[1:2:3:4:5:6:7:8]:5000/\", True),\n+        (\"http://[1:2:3:4::1]/\", True),\n+        (\"http://[1:2:3:9::1]/\", True),\n+        (\"http://[1:2:3:9:0:0:0:1]/\", True),\n+        (\"http://[1:2:3:9::2]/\", False),\n     ),\n )\n def test_should_bypass_proxies(url, expected, monkeypatch):\n@@ -730,11 +755,11 @@ def test_should_bypass_proxies(url, expected, monkeypatch):\n     \"\"\"\n     monkeypatch.setenv(\n         \"no_proxy\",\n-        \"192.168.0.0/24,127.0.0.1,localhost.localdomain,172.16.1.1, google.com:6000\",\n+        \"192.168.0.0/24,127.0.0.1,localhost.localdomain,1:2:3:4::/64,1:2:3:9::1,172.16.1.1, google.com:6000\",\n     )\n     monkeypatch.setenv(\n         \"NO_PROXY\",\n-        \"192.168.0.0/24,127.0.0.1,localhost.localdomain,172.16.1.1, google.com:6000\",\n+        \"192.168.0.0/24,127.0.0.1,localhost.localdomain,1:2:3:4::/64,1:2:3:9::1,172.16.1.1, google.com:6000\",\n     )\n     assert should_bypass_proxies(url, no_proxy=None) == expected\n \n@@ -956,3 +981,36 @@ def QueryValueEx(key, value_name):\n     monkeypatch.setattr(winreg, \"OpenKey\", OpenKey)\n     monkeypatch.setattr(winreg, \"QueryValueEx\", QueryValueEx)\n     assert should_bypass_proxies(\"http://example.com/\", None) is False\n+\n+\n+@pytest.mark.parametrize(\n+    \"mask, totalbits, maskbits\",\n+    (\n+        (24, None, 0xFFFFFF00),\n+        (31, None, 0xFFFFFFFE),\n+        (0, None, 0x0),\n+        (4, 4, 0xF),\n+        (24, 128, 0xFFFFFF00000000000000000000000000),\n+    ),\n+)\n+def test__get_mask_bits(mask, totalbits, maskbits):\n+    args = {\"mask\": mask}\n+    if totalbits:\n+        args[\"totalbits\"] = totalbits\n+    assert _get_mask_bits(**args) == maskbits\n+\n+\n+@pytest.mark.parametrize(\n+    \"a, b, expected\",\n+    (\n+        (\"1.2.3.4\", \"1.2.3.4\", True),\n+        (\"1.2.3.4\", \"2.2.3.4\", False),\n+        (\"1::4\", \"1.2.3.4\", False),\n+        (\"1::4\", \"1::4\", True),\n+        (\"1::4\", \"1:0:0:0:0:0:0:4\", True),\n+        (\"1::4\", \"1:0:0:0:0:0::4\", True),\n+        (\"1::4\", \"1:0:0:0:0:0:1:4\", False),\n+    ),\n+)\n+def test_compare_ips(a, b, expected):\n+    assert compare_ips(a, b) == expected\n"
  },
  {
    "id": 724438664,
    "number": 5922,
    "user": {
      "login": "brmzkw",
      "id": 795841
    },
    "diff_url": "https://github.com/psf/requests/pull/5922.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/5922",
    "title": "Always use value of session.verify when set, fixes #5921",
    "body": "This fixes https://github.com/psf/requests/issues/5921 where the value of REQUESTS_CA_BUNDLE is read even when certificate verification is disabled through the session.",
    "diff": "diff --git a/requests/sessions.py b/requests/sessions.py\nindex ae4bcc8e79..1674ef12b0 100644\n--- a/requests/sessions.py\n+++ b/requests/sessions.py\n@@ -714,7 +714,7 @@ def merge_environment_settings(self, url, proxies, stream, verify, cert):\n \n             # Look for requests environment configuration and be compatible\n             # with cURL.\n-            if verify is True or verify is None:\n+            if (verify is True or verify is None) and (self.verify is True or self.verify is None):\n                 verify = (os.environ.get('REQUESTS_CA_BUNDLE') or\n                           os.environ.get('CURL_CA_BUNDLE'))\n \ndiff --git a/tests/test_requests.py b/tests/test_requests.py\nindex b6d97dd9f4..f766816b8a 100644\n--- a/tests/test_requests.py\n+++ b/tests/test_requests.py\n@@ -860,6 +860,18 @@ def test_invalid_ca_certificate_path(self, httpbin_secure):\n             requests.get(httpbin_secure(), verify=INVALID_PATH)\n         assert str(e.value) == 'Could not find a suitable TLS CA certificate bundle, invalid path: {}'.format(INVALID_PATH)\n \n+    def test_invalid_ca_certificate_path_w_verif_false(self, httpbin_secure):\n+        INVALID_PATH = '/garbage'\n+        with override_environ(requests_ca_bundle=INVALID_PATH):\n+            requests.get(httpbin_secure(), verify=False)\n+\n+    def test_invalid_ca_certificate_path_w_session_verif_false(self, httpbin_secure):\n+        INVALID_PATH = '/garbage'\n+        with override_environ(requests_ca_bundle=INVALID_PATH):\n+            session = requests.Session()\n+            session.verify = False\n+            session.get(httpbin_secure())\n+\n     def test_invalid_ssl_certificate_files(self, httpbin_secure):\n         INVALID_PATH = '/garbage'\n         with pytest.raises(IOError) as e:\n"
  },
  {
    "id": 721074475,
    "number": 5915,
    "user": {
      "login": "theGOTOguy",
      "id": 1411050
    },
    "diff_url": "https://github.com/psf/requests/pull/5915.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/5915",
    "title": "Adds a Realistic WSGI Server for Testing",
    "body": "Sending of chunked-encoded data is not currently tested to be correct anywhere in requests' tests, and is called out as a needed improvement in #5906.  This is a hurdle to accepting #5664, a change which would both simplify requests' code and consistently use [urllib3 for retrying errors](https://urllib3.readthedocs.io/en/latest/reference/urllib3.util.html).  \r\n\r\nThe [existing test server](https://github.com/psf/requests/blob/main/tests/testserver/server.py) is a simple socket that can send and receive data and is not aware of HTTP protocols.  Therefore, using it to verify correct sending of chunked-encoded data, for instance, would ultimately become a change-detector test.  \r\n\r\nThis change seeks to remedy that while also taking a step towards both simplifying and strengthening requests' tests.\r\n\r\nWe strengthen requests' tests by introducing the popular [Werkzeug library](https://github.com/pallets/werkzeug) as an explicit test dependency ([Flask](https://github.com/pallets/flask), which uses Werkzeug, is already a test dependency).  By doing so, we leverage their authority as an implementer of [PEP 3333](https://www.python.org/dev/peps/pep-3333/) to validate our own implementation of chunked encoding.  This implicitly future-proofs against any hypothetical breaking changes to the standard by testing against Werkzeug's trusted implementation.\r\n\r\nWe also simplify requests' tests, since the implementation of the test [WerkzeugServer](https://github.com/psf/requests/commit/f1f838e36bc24fb40d7121e72298da3e5a7149ff#diff-a53b39ec2e425dad0c8387f8402839b2ec328e93494086ed66abb4b49708deeb) here is substantially more concise than the [current test server](https://github.com/psf/requests/blob/main/tests/testserver/server.py).  If this change is accepted, in another pull request I will refactor all of requests' tests to use the WerkzeugServer rather than the current test server.  ",
    "diff": "diff --git a/requirements-dev.txt b/requirements-dev.txt\nindex 6edee41781..1058ffb0ee 100644\n--- a/requirements-dev.txt\n+++ b/requirements-dev.txt\n@@ -5,6 +5,7 @@ pytest-httpbin==1.0.0\n pytest-mock==2.0.0\n httpbin==0.7.0\n trustme\n+Werkzeug\n wheel\n \n # Flask Stack\ndiff --git a/tests/test_lowlevel.py b/tests/test_lowlevel.py\nindex 859d07e8a5..2079b4d62e 100644\n--- a/tests/test_lowlevel.py\n+++ b/tests/test_lowlevel.py\n@@ -2,6 +2,7 @@\n \n import pytest\n from tests.testserver.server import Server, consume_socket_content\n+from tests.testserver.werkzeug_server import WerkzeugServer\n \n import requests\n from requests.compat import JSONDecodeError\n@@ -23,17 +24,16 @@ def echo_response_handler(sock):\n \n def test_chunked_upload():\n     \"\"\"can safely send generators\"\"\"\n-    close_server = threading.Event()\n-    server = Server.basic_response_server(wait_to_close_event=close_server)\n-    data = iter([b\"a\", b\"b\", b\"c\"])\n+    server = WerkzeugServer.echo_server()\n+    data = iter([b'a', b'b', b'c'])\n \n     with server as (host, port):\n-        url = f\"http://{host}:{port}/\"\n+        url = f'http://{host}:{port}/'\n         r = requests.post(url, data=data, stream=True)\n-        close_server.set()  # release server block\n \n+    assert r.content == b'abc'\n     assert r.status_code == 200\n-    assert r.request.headers[\"Transfer-Encoding\"] == \"chunked\"\n+    assert r.request.headers['Transfer-Encoding'] == 'chunked'\n \n \n def test_chunked_encoding_error():\ndiff --git a/tests/testserver/werkzeug_server.py b/tests/testserver/werkzeug_server.py\nnew file mode 100644\nindex 0000000000..ef8903a675\n--- /dev/null\n+++ b/tests/testserver/werkzeug_server.py\n@@ -0,0 +1,55 @@\n+import multiprocessing\n+import socket\n+from contextlib import closing\n+\n+from werkzeug import Request, Response, run_simple\n+\n+\n+@Request.application\n+def echo_application(request):\n+    return Response(request.get_data(), 200, content_type=request.content_type)\n+\n+\n+class WerkzeugServer:\n+    \"\"\"Realistic WSGI server for unit testing.\"\"\"\n+\n+    SOCKET_CONNECT_TIMEOUT = 2\n+\n+    def __init__(self, application, host=\"localhost\", port=0):\n+        super().__init__()\n+\n+        self.host = host\n+        self.port = port\n+\n+        # Werkzeug will not automatically pick a valid port for us.\n+        if not self.port:\n+            with closing(socket.socket()) as sock:\n+                sock.bind((self.host, self.port))\n+                self.port = sock.getsockname()[1]\n+\n+        self.process = multiprocessing.Process(\n+            target=run_simple, args=(self.host, self.port, application)\n+        )\n+\n+    @classmethod\n+    def echo_server(cls):\n+        return WerkzeugServer(echo_application)\n+\n+    def _socket_is_ready(self):\n+        with closing(socket.socket()) as sock:\n+            sock.settimeout(self.SOCKET_CONNECT_TIMEOUT)\n+            return sock.connect_ex((self.host, self.port)) == 0\n+\n+    def __enter__(self):\n+        self.process.start()\n+\n+        # Confirm that we can actually connect to the socket before we return.\n+        # This protects from flaky tests should the process come up too late.\n+        while not self._socket_is_ready():\n+            pass\n+\n+        return self.host, self.port\n+\n+    def __exit__(self, exc_type, exc_value, traceback):\n+        self.process.terminate()\n+        return False\n"
  },
  {
    "id": 684898170,
    "number": 5860,
    "user": {
      "login": "verhovsky",
      "id": 5687998
    },
    "diff_url": "https://github.com/psf/requests/pull/5860.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/5860",
    "title": "Document de-duplication of keys for params and data",
    "body": "and format the files dict format correctly",
    "diff": "diff --git a/requests/models.py b/requests/models.py\nindex aa6fb86e4e..24139498fd 100644\n--- a/requests/models.py\n+++ b/requests/models.py\n@@ -203,14 +203,14 @@ class Request(RequestHooksMixin):\n     :param method: HTTP method to use.\n     :param url: URL to send.\n     :param headers: dictionary of headers to send.\n-    :param files: dictionary of {filename: fileobject} files to multipart upload.\n+    :param files: dictionary of ``{filename: fileobject}`` files to multipart upload.\n     :param data: the body to attach to the request. If a dictionary or\n-        list of tuples ``[(key, value)]`` is provided, form-encoding will\n-        take place.\n+        list of tuples ``[(key, value)]`` or ``[(key, [values])]`` is provided, \n+        form-encoding will take place.\n     :param json: json for the body to attach to the request (if files or data is not specified).\n     :param params: URL parameters to append to the URL. If a dictionary or\n-        list of tuples ``[(key, value)]`` is provided, form-encoding will\n-        take place.\n+        list of tuples ``[(key, value)]`` or ``[(key, [values])]`` is provided, \n+        form-encoding will take place.\n     :param auth: Auth handler or (user, pass) tuple.\n     :param cookies: dictionary or CookieJar of cookies to attach to this request.\n     :param hooks: dictionary of callback hooks, for internal usage.\n"
  },
  {
    "id": 644523910,
    "number": 5816,
    "user": {
      "login": "Akasurde",
      "id": 633765
    },
    "diff_url": "https://github.com/psf/requests/pull/5816.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/5816",
    "title": "Warn user if environment variables are used",
    "body": "When session.verify=False, session.trust_env=True and\r\nREQUESTS_CA_BUNDLE or CURL_CA_BUNDLE is defined as environment\r\nvariables then, notify user that requests will use environment variables\r\nrather than silently failing.\r\n\r\npartially fixes: #3829\r\n\r\nSigned-off-by: Abhijeet Kasurde <akasurde@redhat.com>",
    "diff": "diff --git a/requests/sessions.py b/requests/sessions.py\nindex 6cb3b4dae3..4671b2ad67 100644\n--- a/requests/sessions.py\n+++ b/requests/sessions.py\n@@ -8,6 +8,7 @@\n import os\n import sys\n import time\n+import warnings\n from collections import OrderedDict\n from datetime import timedelta\n \n@@ -768,6 +769,10 @@ def merge_environment_settings(self, url, proxies, stream, verify, cert):\n                     or os.environ.get(\"CURL_CA_BUNDLE\")\n                     or verify\n                 )\n+                if os.environ.get('REQUESTS_CA_BUNDLE'):\n+                    warnings.warn('Setting verify value to environment value of REQUESTS_CA_BUNDLE')\n+                elif os.environ.get('CURL_CA_BUNDLE'):\n+                    warnings.warn('Setting verify value to environment value of CURL_CA_BUNDLE')\n \n         # Merge all the kwargs.\n         proxies = merge_setting(proxies, self.proxies)\n"
  },
  {
    "id": 596587408,
    "number": 5779,
    "user": {
      "login": "grintor",
      "id": 3444196
    },
    "diff_url": "https://github.com/psf/requests/pull/5779.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/5779",
    "title": "sets a default timeout and resolves #3070",
    "body": "This sets a (very high) default timeout which is guaranteed not to create any breaking changes while also fixing the longstanding issue of requests possibly hanging indefinitely in any script where the dev forgot to include a timeout. I would like to urge this commit make it into the next minor release version with the possibility of lowering the timeout to 300 seconds in the next major release version.\r\n\r\nPlease see the discussion at https://github.com/psf/requests/issues/3070",
    "diff": "diff --git a/requests/sessions.py b/requests/sessions.py\nindex ae4bcc8e79..7b8933bb20 100644\n--- a/requests/sessions.py\n+++ b/requests/sessions.py\n@@ -469,7 +469,7 @@ def prepare_request(self, request):\n \n     def request(self, method, url,\n             params=None, data=None, headers=None, cookies=None, files=None,\n-            auth=None, timeout=None, allow_redirects=True, proxies=None,\n+            auth=None, timeout=(131, None), allow_redirects=True, proxies=None,\n             hooks=None, stream=None, verify=None, cert=None, json=None):\n         \"\"\"Constructs a :class:`Request <Request>`, prepares it and sends it.\n         Returns :class:`Response <Response>` object.\n"
  },
  {
    "id": 594662758,
    "number": 5776,
    "user": {
      "login": "cpyberry",
      "id": 80456253
    },
    "diff_url": "https://github.com/psf/requests/pull/5776.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/5776",
    "title": "Added explanation why the timeout is doubled.(#5773)",
    "body": "## issue\r\n\r\n#5773",
    "diff": "diff --git a/docs/user/advanced.rst b/docs/user/advanced.rst\nindex 68f36999ef..b5d6161868 100644\n--- a/docs/user/advanced.rst\n+++ b/docs/user/advanced.rst\n@@ -1076,3 +1076,13 @@ coffee.\n     r = requests.get('https://github.com', timeout=None)\n \n .. _`connect()`: https://linux.die.net/man/2/connect\n+\n+**You need to be careful when using the timeout argument.**\n+\n+If the specified domain has multiple IP addresses, `urllib3`_ will continue to try another IP address when the specified connection timeout is reached. Also, the connection timeout is applied for each attempt.\n+Requests are affected by this because you are using `urllib3`_.\n+This phenomenon can wait longer than you expect. Maybe it's the time that doubled the connection timeout.\n+Of course there may be more.\n+You can also consider extreme solutions, such as having the specified domain name return only one IP address.\n+For example, if DNS returns both IPv4 DNS records (A) and IPv6 DNS records (AAAA), patch it so that it does not return IPv6.\n+However, new issues may arise.\n"
  },
  {
    "id": 571091787,
    "number": 5748,
    "user": {
      "login": "luckydenis",
      "id": 41421345
    },
    "diff_url": "https://github.com/psf/requests/pull/5748.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/5748",
    "title": "Fix: #4362 - Redirect resolved even though allow_redirects is set to False causing exception for unsupported connection adapter",
    "body": "Good evening\r\n\r\nThe problem was that when checking, we got data that passed the condition test: they contained `:` , and the request did not start with `http`. Example `0.0.0.0:8080`.\r\n\r\nDone:\r\n- [x] Fix #4362\r\n- [x] Add and fix tests\r\n- [x] make test\r\n- [x] flake8 (diff)",
    "diff": "diff --git a/AUTHORS.rst b/AUTHORS.rst\nindex c4e554d478..5baffd2753 100644\n--- a/AUTHORS.rst\n+++ b/AUTHORS.rst\n@@ -192,3 +192,4 @@ Patches and Suggestions\n - Alessio Izzo (`@aless10 <https://github.com/aless10>`_)\n - Sylvain Mari\u00e9 (`@smarie <https://github.com/smarie>`_)\n - Hod Bin Noon (`@hodbn <https://github.com/hodbn>`_)\n+- Denis Belavin (`@LuckyDenis <https://github.com/LuckyDenis>`_)\ndiff --git a/requests/models.py b/requests/models.py\nindex e7d292d580..3495a6ea23 100644\n--- a/requests/models.py\n+++ b/requests/models.py\n@@ -372,12 +372,6 @@ def prepare_url(self, url, params):\n         # Remove leading whitespaces from url\n         url = url.lstrip()\n \n-        # Don't do any URL preparation for non-HTTP schemes like `mailto`,\n-        # `data` etc to work around exceptions from `url_parse`, which\n-        # handles RFC 3986 only.\n-        if ':' in url and not url.lower().startswith('http'):\n-            self.url = url\n-            return\n \n         # Support for unicode domain names and paths.\n         try:\n@@ -385,6 +379,13 @@ def prepare_url(self, url, params):\n         except LocationParseError as e:\n             raise InvalidURL(*e.args)\n \n+        # Don't do any URL preparation for non-HTTP schemes like `mailto`,\n+        # `data` etc to work around exceptions from `url_parse`, which\n+        # handles RFC 3986 only.\n+        if scheme is not None and not scheme.startswith('http'):\n+            self.url = url\n+            return\n+\n         if not scheme:\n             error = (\"Invalid URL {0!r}: No schema supplied. Perhaps you meant http://{0}?\")\n             error = error.format(to_native_string(url, 'utf8'))\ndiff --git a/tests/test_requests.py b/tests/test_requests.py\nindex 463e8bf47a..dc98b683cc 100644\n--- a/tests/test_requests.py\n+++ b/tests/test_requests.py\n@@ -77,9 +77,14 @@ def test_entry_points(self):\n     @pytest.mark.parametrize(\n         'exception, url', (\n             (MissingSchema, 'hiwpefhipowhefopw'),\n+            (MissingSchema, 'localhost.localdomain:3128/'),\n+            (MissingSchema, '10.122.1.1:3128/'),\n+            (MissingSchema, '0.0.0.0:3128'),\n+            (InvalidSchema, 'httpp://localhost:3128'),\n+            (InvalidSchema, 'htps://localhost.localdomain:3128/'),\n+            (InvalidSchema, 'htp://10.122.1.1:3128/'),\n             (InvalidSchema, 'localhost:3128'),\n-            (InvalidSchema, 'localhost.localdomain:3128/'),\n-            (InvalidSchema, '10.122.1.1:3128/'),\n+            (InvalidURL, 'http:://10.122.1.1:3128/'),\n             (InvalidURL, 'http://'),\n         ))\n     def test_invalid_url(self, exception, url):\n"
  },
  {
    "id": 563464518,
    "number": 5735,
    "user": {
      "login": "mateusduboli",
      "id": 1688249
    },
    "diff_url": "https://github.com/psf/requests/pull/5735.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/5735",
    "title": "5677: Respect variable precedence in session",
    "body": "- Move `Session#merge_environment_variables` from `Session#request` to `Session#send` to make it consistent\r\n- On `Session#send` change variable precedence to (higher precedence first) `kwargs` -> `session args` -> `environment`.",
    "diff": "diff --git a/requests/sessions.py b/requests/sessions.py\nindex ae4bcc8e79..32ca0ec1c3 100644\n--- a/requests/sessions.py\n+++ b/requests/sessions.py\n@@ -527,18 +527,16 @@ def request(self, method, url,\n         )\n         prep = self.prepare_request(req)\n \n-        proxies = proxies or {}\n-\n-        settings = self.merge_environment_settings(\n-            prep.url, proxies, stream, verify, cert\n-        )\n-\n-        # Send the request.\n         send_kwargs = {\n             'timeout': timeout,\n             'allow_redirects': allow_redirects,\n+            'verify': verify,\n+            'proxies': proxies,\n+            'stream': stream,\n+            'cert': cert,\n         }\n-        send_kwargs.update(settings)\n+        \n+        # Send the request.\n         resp = self.send(prep, **send_kwargs)\n \n         return resp\n@@ -628,23 +626,28 @@ def send(self, request, **kwargs):\n \n         :rtype: requests.Response\n         \"\"\"\n-        # Set defaults that the hooks can utilize to ensure they always have\n-        # the correct parameters to reproduce the previous request.\n-        kwargs.setdefault('stream', self.stream)\n-        kwargs.setdefault('verify', self.verify)\n-        kwargs.setdefault('cert', self.cert)\n-        kwargs.setdefault('proxies', self.rebuild_proxies(request, self.proxies))\n-\n         # It's possible that users might accidentally send a Request object.\n         # Guard against that specific failure case.\n         if isinstance(request, Request):\n             raise ValueError('You can only send PreparedRequests.')\n \n+        # Set defaults that the hooks can utilize to ensure they always have\n+        # the correct parameters to reproduce the previous request.\n+        stream = kwargs.get('stream')\n+        verify = kwargs.get('verify')\n+        cert = kwargs.get('cert')\n+        proxies = kwargs.get('proxies')\n+\n         # Set up variables needed for resolve_redirects and dispatching of hooks\n         allow_redirects = kwargs.pop('allow_redirects', True)\n-        stream = kwargs.get('stream')\n         hooks = request.hooks\n \n+        # Merge with environment variables\n+        settings = self.merge_environment_settings(\n+            request.url, proxies, stream, verify, cert\n+        )\n+        kwargs.update(settings)\n+\n         # Get the appropriate adapter to use\n         adapter = self.get_adapter(url=request.url)\n \n@@ -704,6 +707,15 @@ def merge_environment_settings(self, url, proxies, stream, verify, cert):\n \n         :rtype: dict\n         \"\"\"\n+        \n+        # Merge all the kwargs.\n+        proxies = merge_setting(proxies, self.proxies)\n+        stream = merge_setting(stream, self.stream)\n+        verify = merge_setting(verify, self.verify)\n+        cert = merge_setting(cert, self.cert)\n+\n+        proxies = proxies or {}\n+\n         # Gather clues from the surrounding environment.\n         if self.trust_env:\n             # Set environment's proxies.\n@@ -716,13 +728,7 @@ def merge_environment_settings(self, url, proxies, stream, verify, cert):\n             # with cURL.\n             if verify is True or verify is None:\n                 verify = (os.environ.get('REQUESTS_CA_BUNDLE') or\n-                          os.environ.get('CURL_CA_BUNDLE'))\n-\n-        # Merge all the kwargs.\n-        proxies = merge_setting(proxies, self.proxies)\n-        stream = merge_setting(stream, self.stream)\n-        verify = merge_setting(verify, self.verify)\n-        cert = merge_setting(cert, self.cert)\n+                    os.environ.get('CURL_CA_BUNDLE') or verify)\n \n         return {'verify': verify, 'proxies': proxies, 'stream': stream,\n                 'cert': cert}\ndiff --git a/tests/test_requests.py b/tests/test_requests.py\nindex 223c36e363..1d68432c1f 100644\n--- a/tests/test_requests.py\n+++ b/tests/test_requests.py\n@@ -590,6 +590,33 @@ def test_respect_proxy_env_on_request(self, httpbin):\n                 session = requests.Session()\n                 session.request(method='GET', url=httpbin())\n \n+    def test_merge_environment_settings_precedence(self):\n+        session = requests.Session()\n+        merge_settings_args = {\n+            'url':'http://localhost:1',\n+            'proxies':{\n+                'http': 'http://argument'\n+            },\n+            'stream':None,\n+            'verify':None,\n+            'cert':None,\n+        }\n+        with override_environ(http_proxy='http://environment'):\n+            session.proxies.update(http='http://attribute')\n+            settings = session.merge_environment_settings(**merge_settings_args)\n+            assert settings['proxies']['http'] == 'http://argument'\n+\n+            merge_settings_args['proxies'] = None\n+            settings = session.merge_environment_settings(**merge_settings_args)\n+            assert settings['proxies']['http'] == 'http://attribute'\n+\n+            session.proxies = None\n+            settings = session.merge_environment_settings(**merge_settings_args)\n+            assert settings['proxies']['http'] == 'http://environment'\n+\n+        settings = session.merge_environment_settings(**merge_settings_args)\n+        assert settings['proxies'] == {}\n+\n     def test_basicauth_with_netrc(self, httpbin):\n         auth = ('user', 'pass')\n         wrong_auth = ('wronguser', 'wrongpass')\n"
  },
  {
    "id": 538329438,
    "number": 5693,
    "user": {
      "login": "sigmavirus24",
      "id": 240830
    },
    "diff_url": "https://github.com/psf/requests/pull/5693.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/5693",
    "title": "Pass urllib3.SKIP_HEADER when headers should be unset",
    "body": "urllib3 introduced some default headers and a way to skip them if\r\ndesired. Let's use that sentinel value to pass along information about\r\nRequests' users desire to skip those headers as well.\r\n\r\nCloses gh-5671\r\n\r\n---\r\nI'm pretty sure the tests will fail to start with, but wanted to get a sketch of something up for early review",
    "diff": "diff --git a/requests/compat.py b/requests/compat.py\nindex 5de0769f50..c522038929 100644\n--- a/requests/compat.py\n+++ b/requests/compat.py\n@@ -30,6 +30,16 @@\n except ImportError:\n     import json\n \n+\n+import urllib3\n+\n+try:\n+    SKIP_HEADER = urllib3.util.SKIP_HEADER\n+    SKIPPABLE_HEADERS = urllib3.util.SKIPPABLE_HEADERS\n+except AttributeError:\n+    SKIP_HEADER = None\n+    SKIPPABLE_HEADERS = frozenset([])\n+\n # ---------\n # Specifics\n # ---------\ndiff --git a/requests/models.py b/requests/models.py\nindex ec2edc20b5..a5c0c8b707 100644\n--- a/requests/models.py\n+++ b/requests/models.py\n@@ -15,6 +15,7 @@\n # such as in Embedded Python. See https://github.com/psf/requests/issues/3578.\n import encodings.idna\n \n+import urllib3\n from urllib3.fields import RequestField\n from urllib3.filepost import encode_multipart_formdata\n from urllib3.util import parse_url\n@@ -36,9 +37,21 @@\n     stream_decode_response_unicode, to_key_val_list, parse_header_links,\n     iter_slices, guess_json_utf, super_len, check_header_validity)\n from .compat import (\n-    Callable, Mapping,\n-    cookielib, urlunparse, urlsplit, urlencode, str, bytes,\n-    is_py2, chardet, builtin_str, basestring)\n+    SKIP_HEADER,\n+    SKIPPABLE_HEADERS,\n+    Callable,\n+    Mapping,\n+    cookielib,\n+    urlunparse,\n+    urlsplit,\n+    urlencode,\n+    str,\n+    bytes,\n+    is_py2,\n+    chardet,\n+    builtin_str,\n+    basestring,\n+)\n from .compat import json as complexjson\n from .status_codes import codes\n \n@@ -447,9 +460,14 @@ def prepare_headers(self, headers):\n         self.headers = CaseInsensitiveDict()\n         if headers:\n             for header in headers.items():\n+                name, value = header\n+                if value is None:\n+                    if name.lower() in SKIPPABLE_HEADERS:\n+                        value = SKIP_HEADER\n+                    else:\n+                        continue\n                 # Raise exception on invalid header value.\n                 check_header_validity(header)\n-                name, value = header\n                 self.headers[to_native_string(name)] = value\n \n     def prepare_body(self, data, files, json=None):\ndiff --git a/requests/sessions.py b/requests/sessions.py\nindex fdf7e9fe35..108be7bdea 100644\n--- a/requests/sessions.py\n+++ b/requests/sessions.py\n@@ -47,7 +47,9 @@\n     preferred_clock = time.time\n \n \n-def merge_setting(request_setting, session_setting, dict_class=OrderedDict):\n+def merge_setting(\n+    request_setting, session_setting, dict_class=OrderedDict, delete_none=True\n+):\n     \"\"\"Determines appropriate setting for a given request, taking into account\n     the explicit setting on that request, and the setting in the session. If a\n     setting is a dictionary, they will be merged together using `dict_class`\n@@ -69,11 +71,12 @@ def merge_setting(request_setting, session_setting, dict_class=OrderedDict):\n     merged_setting = dict_class(to_key_val_list(session_setting))\n     merged_setting.update(to_key_val_list(request_setting))\n \n-    # Remove keys that are set to None. Extract keys first to avoid altering\n-    # the dictionary during iteration.\n-    none_keys = [k for (k, v) in merged_setting.items() if v is None]\n-    for key in none_keys:\n-        del merged_setting[key]\n+    if delete_none:\n+        # Remove keys that are set to None. Extract keys first to avoid altering\n+        # the dictionary during iteration.\n+        none_keys = [k for (k, v) in merged_setting.items() if v is None]\n+        for key in none_keys:\n+            del merged_setting[key]\n \n     return merged_setting\n \n@@ -459,7 +462,12 @@ def prepare_request(self, request):\n             files=request.files,\n             data=request.data,\n             json=request.json,\n-            headers=merge_setting(request.headers, self.headers, dict_class=CaseInsensitiveDict),\n+            headers=merge_setting(\n+                request.headers,\n+                self.headers,\n+                dict_class=CaseInsensitiveDict,\n+                delete_none=False,\n+            ),\n             params=merge_setting(request.params, self.params),\n             auth=merge_setting(auth, self.auth),\n             cookies=merged_cookies,\ndiff --git a/requests/utils.py b/requests/utils.py\nindex db67938e67..905c933081 100644\n--- a/requests/utils.py\n+++ b/requests/utils.py\n@@ -947,6 +947,8 @@ def check_header_validity(header):\n     :param header: tuple, in the format (name, value).\n     \"\"\"\n     name, value = header\n+    if value is None:\n+        return\n \n     if isinstance(value, bytes):\n         pat = _CLEAN_HEADER_REGEX_BYTE\ndiff --git a/tests/test_requests.py b/tests/test_requests.py\nindex 5b6a7f5847..6a0727b8ad 100644\n--- a/tests/test_requests.py\n+++ b/tests/test_requests.py\n@@ -17,10 +17,15 @@\n from requests.adapters import HTTPAdapter\n from requests.auth import HTTPDigestAuth, _basic_auth_str\n from requests.compat import (\n-    Morsel, cookielib, getproxies, str, urlparse,\n-    builtin_str)\n-from requests.cookies import (\n-    cookiejar_from_dict, morsel_to_cookie)\n+    Morsel,\n+    cookielib,\n+    getproxies,\n+    str,\n+    urlparse,\n+    builtin_str,\n+    SKIP_HEADER,\n+)\n+from requests.cookies import cookiejar_from_dict, morsel_to_cookie\n from requests.exceptions import (\n     ConnectionError, ConnectTimeout, InvalidSchema, InvalidURL,\n     MissingSchema, ReadTimeout, Timeout, RetryError, TooManyRedirects,\n@@ -438,10 +443,13 @@ def test_history_is_always_a_list(self, httpbin):\n     def test_headers_on_session_with_None_are_not_sent(self, httpbin):\n         \"\"\"Do not send headers in Session.headers with None values.\"\"\"\n         ses = requests.Session()\n-        ses.headers['Accept-Encoding'] = None\n-        req = requests.Request('GET', httpbin('get'))\n+        ses.headers[\"Accept-Encoding\"] = None\n+        req = requests.Request(\"GET\", httpbin(\"get\"))\n         prep = ses.prepare_request(req)\n-        assert 'Accept-Encoding' not in prep.headers\n+        if not SKIP_HEADER:\n+            assert \"Accept-Encoding\" not in prep.headers\n+        else:\n+            assert SKIP_HEADER == prep.headers[\"Accept-Encoding\"]\n \n     def test_headers_preserve_order(self, httpbin):\n         \"\"\"Preserve order when headers provided as OrderedDict.\"\"\"\n"
  },
  {
    "id": 524165788,
    "number": 5665,
    "user": {
      "login": "jalopezsilva",
      "id": 1276443
    },
    "diff_url": "https://github.com/psf/requests/pull/5665.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/5665",
    "title": "Add support for HTTPS proxies.",
    "body": "Starting with 1.26 urllib3 now supports HTTPS proxies. To enable the support two changes are needed:\r\n\r\n* An additional proxy_kwargs argument that can be passed from the session. This dictionary will be used to pass any arguments needed to the underlying adapter. The parameter is optional.\r\n* The use_forwarding_for_https mode requires us to send the absolute URI when enabled.\r\n\r\nThe resulting API is very similar except it takes an additional parameter when needed. An example:\r\n\r\n```\r\n    session = requests.Session()\r\n    proxies = {\"http\": \"https://proxy.example\", \"https\": \"https://proxy.example\"}\r\n\r\n    proxies_kwargs = {\r\n        \"proxy_ssl_context\": proxy_ssl_context(),\r\n    }\r\n\r\n    response = session.get(\r\n       \"https://www.google.com\", proxies=proxies, proxies_kwargs=proxies_kwargs\r\n    )\r\n```\r\n",
    "diff": "diff --git a/requests/adapters.py b/requests/adapters.py\nindex fa4d9b3cc9..e6511b8f9b 100644\n--- a/requests/adapters.py\n+++ b/requests/adapters.py\n@@ -59,7 +59,7 @@ def __init__(self):\n         super(BaseAdapter, self).__init__()\n \n     def send(self, request, stream=False, timeout=None, verify=True,\n-             cert=None, proxies=None):\n+             cert=None, proxies=None, proxies_kwargs=None):\n         \"\"\"Sends PreparedRequest object. Returns Response object.\n \n         :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.\n@@ -73,6 +73,7 @@ def send(self, request, stream=False, timeout=None, verify=True,\n             to a CA bundle to use\n         :param cert: (optional) Any user-provided SSL certificate to be trusted.\n         :param proxies: (optional) The proxies dictionary to apply to the request.\n+        :param proxies_kwargs: (optional) Additional arguments to pass to urllib3 for proxies.\n         \"\"\"\n         raise NotImplementedError\n \n@@ -289,7 +290,7 @@ def build_response(self, req, resp):\n \n         return response\n \n-    def get_connection(self, url, proxies=None):\n+    def get_connection(self, url, proxies=None, proxies_kwargs=None):\n         \"\"\"Returns a urllib3 connection for the given URL. This should not be\n         called from user code, and is only exposed for use when subclassing the\n         :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n@@ -300,13 +301,16 @@ def get_connection(self, url, proxies=None):\n         \"\"\"\n         proxy = select_proxy(url, proxies)\n \n+        if not proxies_kwargs:\n+            proxies_kwargs = {}\n+\n         if proxy:\n             proxy = prepend_scheme_if_needed(proxy, 'http')\n             proxy_url = parse_url(proxy)\n             if not proxy_url.host:\n                 raise InvalidProxyURL(\"Please check proxy URL. It is malformed\"\n                                       \" and could be missing the host.\")\n-            proxy_manager = self.proxy_manager_for(proxy)\n+            proxy_manager = self.proxy_manager_for(proxy, **proxies_kwargs)\n             conn = proxy_manager.connection_from_url(url)\n         else:\n             # Only scheme should be lower case\n@@ -326,7 +330,7 @@ def close(self):\n         for proxy in self.proxy_manager.values():\n             proxy.clear()\n \n-    def request_url(self, request, proxies):\n+    def request_url(self, request, proxies, proxies_kwargs=None):\n         \"\"\"Obtain the url to use when making the final request.\n \n         If the message is being sent through a HTTP proxy, the full URL has to\n@@ -344,13 +348,20 @@ def request_url(self, request, proxies):\n         scheme = urlparse(request.url).scheme\n \n         is_proxied_http_request = (proxy and scheme != 'https')\n+        is_using_https_forwarding = (\n+            proxies_kwargs and proxies_kwargs.get('use_forwarding_for_https', False)\n+        )\n+\n+        is_using_https_proxy = False\n         using_socks_proxy = False\n         if proxy:\n             proxy_scheme = urlparse(proxy).scheme.lower()\n             using_socks_proxy = proxy_scheme.startswith('socks')\n+            is_using_https_proxy = proxy_scheme.startswith('https')\n \n         url = request.path_url\n-        if is_proxied_http_request and not using_socks_proxy:\n+        if (is_proxied_http_request and not using_socks_proxy or\n+                (is_using_https_proxy and is_using_https_forwarding)):\n             url = urldefragauth(request.url)\n \n         return url\n@@ -391,7 +402,8 @@ def proxy_headers(self, proxy):\n \n         return headers\n \n-    def send(self, request, stream=False, timeout=None, verify=True, cert=None, proxies=None):\n+    def send(self, request, stream=False, timeout=None, verify=True, cert=None,\n+             proxies=None, proxies_kwargs=None):\n         \"\"\"Sends PreparedRequest object. Returns Response object.\n \n         :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.\n@@ -405,16 +417,17 @@ def send(self, request, stream=False, timeout=None, verify=True, cert=None, prox\n             must be a path to a CA bundle to use\n         :param cert: (optional) Any user-provided SSL certificate to be trusted.\n         :param proxies: (optional) The proxies dictionary to apply to the request.\n+        :param proxies_kwargs: (optional) Dictionary with additional proxy kwargs.\n         :rtype: requests.Response\n         \"\"\"\n \n         try:\n-            conn = self.get_connection(request.url, proxies)\n+            conn = self.get_connection(request.url, proxies, proxies_kwargs)\n         except LocationValueError as e:\n             raise InvalidURL(e, request=request)\n \n         self.cert_verify(conn, request.url, verify, cert)\n-        url = self.request_url(request, proxies)\n+        url = self.request_url(request, proxies, proxies_kwargs)\n         self.add_headers(request, stream=stream, timeout=timeout, verify=verify, cert=cert, proxies=proxies)\n \n         chunked = not (request.body is None or 'Content-Length' in request.headers)\ndiff --git a/requests/api.py b/requests/api.py\nindex e978e20311..17b92fa5b4 100644\n--- a/requests/api.py\n+++ b/requests/api.py\n@@ -38,6 +38,7 @@ def request(method, url, **kwargs):\n     :param allow_redirects: (optional) Boolean. Enable/disable GET/OPTIONS/POST/PUT/PATCH/DELETE/HEAD redirection. Defaults to ``True``.\n     :type allow_redirects: bool\n     :param proxies: (optional) Dictionary mapping protocol to the URL of the proxy.\n+    :param proxies_kwargs: (optional) Dictionary with additional proxy kwargs.\n     :param verify: (optional) Either a boolean, in which case it controls whether we verify\n             the server's TLS certificate, or a string, in which case it must be a path\n             to a CA bundle to use. Defaults to ``True``.\ndiff --git a/requests/sessions.py b/requests/sessions.py\nindex fdf7e9fe35..a2990c6ee1 100644\n--- a/requests/sessions.py\n+++ b/requests/sessions.py\n@@ -470,7 +470,8 @@ def prepare_request(self, request):\n     def request(self, method, url,\n             params=None, data=None, headers=None, cookies=None, files=None,\n             auth=None, timeout=None, allow_redirects=True, proxies=None,\n-            hooks=None, stream=None, verify=None, cert=None, json=None):\n+                proxies_kwargs=None, hooks=None, stream=None, verify=None,\n+                cert=None, json=None):\n         \"\"\"Constructs a :class:`Request <Request>`, prepares it and sends it.\n         Returns :class:`Response <Response>` object.\n \n@@ -498,6 +499,7 @@ def request(self, method, url,\n         :type allow_redirects: bool\n         :param proxies: (optional) Dictionary mapping protocol or protocol and\n             hostname to the URL of the proxy.\n+        :param proxies_kwargs: (optional) Dictionary with additional proxy kwargs.\n         :param stream: (optional) whether to immediately download the response\n             content. Defaults to ``False``.\n         :param verify: (optional) Either a boolean, in which case it controls whether we verify\n@@ -537,6 +539,7 @@ def request(self, method, url,\n         send_kwargs = {\n             'timeout': timeout,\n             'allow_redirects': allow_redirects,\n+            'proxies_kwargs': proxies_kwargs,\n         }\n         send_kwargs.update(settings)\n         resp = self.send(prep, **send_kwargs)\ndiff --git a/tests/test_requests.py b/tests/test_requests.py\nindex 5b6a7f5847..96852c92a0 100644\n--- a/tests/test_requests.py\n+++ b/tests/test_requests.py\n@@ -1846,6 +1846,51 @@ def test_proxy_auth_empty_pass(self):\n         headers = adapter.proxy_headers(\"http://user:@httpbin.org\")\n         assert headers == {'Proxy-Authorization': 'Basic dXNlcjo='}\n \n+\n+    @pytest.mark.parametrize(\n+        'proxy_scheme, url_scheme, expected_value', (\n+            ('socks', 'http', '/'),\n+            ('socks', 'https', '/'),\n+            ('http', 'http', 'http://example.com/'),\n+            ('http', 'https', '/'),\n+            ('https', 'http', 'http://example.com/'),\n+            ('https', 'https', '/'),\n+        ))\n+    def test_proxy_scheme(self, proxy_scheme, url_scheme, expected_value):\n+        proxy_url = '{}://proxy'.format(proxy_scheme)\n+        url = '{}://example.com'.format(url_scheme)\n+        proxies = {'http': proxy_url, 'https': proxy_url}\n+        p = PreparedRequest()\n+        p.prepare(\n+            method='GET',\n+            url=url\n+        )\n+        adapter = HTTPAdapter()\n+        assert expected_value == adapter.request_url(p, proxies)\n+\n+    def test_proxy_scheme_https_forwarding(self):\n+        proxy_url = 'https://proxy.com'\n+        url = 'https://example.com/'\n+        proxies = {'http': proxy_url, 'https': proxy_url}\n+        p = PreparedRequest()\n+        p.prepare(\n+            method='GET',\n+            url=url\n+        )\n+        adapter = HTTPAdapter()\n+\n+        proxies_kwargs = {}\n+        expected_value = '/'\n+        assert expected_value == adapter.request_url(p, proxies, proxies_kwargs)\n+\n+        proxies_kwargs = {'use_forwarding_for_https': True}\n+        expected_value = url\n+        assert expected_value == adapter.request_url(p, proxies, proxies_kwargs)\n+\n+        proxies_kwargs = {'use_forwarding_for_https': False}\n+        expected_value = '/'\n+        assert expected_value == adapter.request_url(p, proxies, proxies_kwargs)\n+\n     def test_response_json_when_content_is_None(self, httpbin):\n         r = requests.get(httpbin('/status/204'))\n         # Make sure r.content is None\n"
  },
  {
    "id": 492483530,
    "number": 5596,
    "user": {
      "login": "Suika",
      "id": 2320837
    },
    "diff_url": "https://github.com/psf/requests/pull/5596.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/5596",
    "title": "Fix no_proxy and no, not being respected. Merge self.proxies and proxies",
    "body": "Since no_proxy is not working as intended, I tried fixing it with least intrsion into whole process.\r\nIt is contained to the `merge_environment_settings` and `request` function. In the `request` the proxy from request and proxy from session are merged. Having the request proxy overwrite the session proxy where needed.\r\n\r\nThe  `should_bypass_proxies` is applied outside `get_environ_proxies` in `merge_environment_settings`, since `no_proxy` is only applied to OS ENV and not the proxies set in `requests` or `Sessions`\r\n\r\nShould close #4871, I hope.",
    "diff": "diff --git a/requests/sessions.py b/requests/sessions.py\nindex 6cb3b4dae3..2474b8d87f 100644\n--- a/requests/sessions.py\n+++ b/requests/sessions.py\n@@ -574,6 +574,14 @@ def request(\n \n         proxies = proxies or {}\n \n+        # Update self.proxy with proxy and assing the result to proxies\n+        if isinstance(proxies,dict):\n+            slef_proxies_tmp = self.proxies.copy()\n+            slef_proxies_tmp.update(proxies)\n+            proxies = slef_proxies_tmp.copy()\n+        else:\n+            proxies = self.proxies.copy()\n+\n         settings = self.merge_environment_settings(\n             prep.url, proxies, stream, verify, cert\n         )\n@@ -769,8 +777,17 @@ def merge_environment_settings(self, url, proxies, stream, verify, cert):\n                     or verify\n                 )\n \n+        # Check for no_proxy and no since they could be loaded from environment\n+        no_proxy = proxies.get('no_proxy') if proxies is not None else None\n+        no = proxies.get('no') if proxies is not None else None\n+        if any([no_proxy, no]):\n+            no_proxy = ','.join(filter(None, (no_proxy, no)))\n+\n         # Merge all the kwargs.\n-        proxies = merge_setting(proxies, self.proxies)\n+        if should_bypass_proxies(url, no_proxy):\n+            proxies = {}\n+        else:\n+            proxies = merge_setting(proxies, self.proxies)\n         stream = merge_setting(stream, self.stream)\n         verify = merge_setting(verify, self.verify)\n         cert = merge_setting(cert, self.cert)\n"
  },
  {
    "id": 432414707,
    "number": 5486,
    "user": {
      "login": "blenq",
      "id": 22727090
    },
    "diff_url": "https://github.com/psf/requests/pull/5486.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/5486",
    "title": "Allow for autodection of encoding for scalar values",
    "body": "Since JSON RFC 7158 (and also in 7159 and 8259) scalar values are also valid JSON texts. The json decoder from the Python standard libs also accepts such scalar values.\r\n\r\nThe JSON Unicode detection algorithm in \"requests\" assumes that the first two characters are always ASCII. That is not true anymore.\r\nTherefore the current detection fails in at least these two edge cases\r\n\r\n- A single digit in UTF-16 encoding\r\n- A quoted string with the first character having a Unicode value higher than 255 in UTF-16 encoding\r\n\r\nAlso autodetection by chardet returns the wrong encoding.\r\n\r\nThis change detects the encoding in case of those two edge cases.\r\n",
    "diff": "diff --git a/requests/models.py b/requests/models.py\nindex ec2edc20b5..7d5be1d975 100644\n--- a/requests/models.py\n+++ b/requests/models.py\n@@ -880,11 +880,11 @@ def json(self, **kwargs):\n         :raises ValueError: If the response body does not contain valid json.\n         \"\"\"\n \n-        if not self.encoding and self.content and len(self.content) > 3:\n-            # No encoding set. JSON RFC 4627 section 3 states we should expect\n-            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n-            # decoding fails, fall back to `self.text` (using chardet to make\n-            # a best guess).\n+        if not self.encoding and self.content and len(self.content) > 1:\n+            # No encoding set. JSON RFC 4627 section 3 and RFC 7158 section 8.1\n+            # state we should expect UTF-8, -16 or -32. Detect which one to\n+            # use; If the detection or decoding fails, fall back to `self.text`\n+            # (using chardet to make a best guess).\n             encoding = guess_json_utf(self.content)\n             if encoding is not None:\n                 try:\ndiff --git a/requests/utils.py b/requests/utils.py\nindex 1aafd9cbf7..2eb2254a53 100644\n--- a/requests/utils.py\n+++ b/requests/utils.py\n@@ -855,7 +855,6 @@ def parse_header_links(value):\n \n # Null bytes; no need to recreate these on each call to guess_json_utf\n _null = '\\x00'.encode('ascii')  # encoding to ASCII for Python 3\n-_null2 = _null * 2\n _null3 = _null * 3\n \n \n@@ -863,7 +862,7 @@ def guess_json_utf(data):\n     \"\"\"\n     :rtype: str\n     \"\"\"\n-    # JSON always starts with two ASCII characters, so detection is as\n+    # JSON always starts with an ASCII character, so detection is as\n     # easy as counting the nulls and from their location and count\n     # determine the encoding. Also detect a BOM, if present.\n     sample = data[:4]\n@@ -876,12 +875,12 @@ def guess_json_utf(data):\n     nullcount = sample.count(_null)\n     if nullcount == 0:\n         return 'utf-8'\n-    if nullcount == 2:\n-        if sample[::2] == _null2:   # 1st and 3rd are null\n+    if nullcount < 3:\n+        if sample[:1] == _null and sample[1:2] != _null:   # 1st is null\n             return 'utf-16-be'\n-        if sample[1::2] == _null2:  # 2nd and 4th are null\n+        if sample[:1] != _null and sample[1:2] == _null:  # 2nd is null\n             return 'utf-16-le'\n-        # Did not detect 2 valid UTF-16 ascii-range characters\n+        # Did not detect a valid UTF-16 ascii-range character\n     if nullcount == 3:\n         if sample[:3] == _null3:\n             return 'utf-32-be'\ndiff --git a/tests/test_utils.py b/tests/test_utils.py\nindex 463516b2e5..3f7de00f17 100644\n--- a/tests/test_utils.py\n+++ b/tests/test_utils.py\n@@ -328,6 +328,24 @@ def test_encoded(self, encoding):\n         data = '{}'.encode(encoding)\n         assert guess_json_utf(data) == encoding\n \n+    @pytest.mark.parametrize(\n+        'encoding', (\n+            'utf-32', 'utf-8-sig', 'utf-16', 'utf-8', 'utf-16-be', 'utf-16-le',\n+            'utf-32-be', 'utf-32-le'\n+        ))\n+    def test_encoded_number(self, encoding):\n+        data = '0'.encode(encoding)\n+        assert guess_json_utf(data) == encoding\n+\n+    @pytest.mark.parametrize(\n+        'encoding', (\n+            'utf-32', 'utf-8-sig', 'utf-16', 'utf-8', 'utf-16-be', 'utf-16-le',\n+            'utf-32-be', 'utf-32-le'\n+        ))\n+    def test_encoded_euro_sign(self, encoding):\n+        data = '\"\\u20AC\"'.encode(encoding)\n+        assert guess_json_utf(data) == encoding\n+\n     def test_bad_utf_like_encoding(self):\n         assert guess_json_utf(b'\\x00\\x00\\x00\\x00') is None\n \n"
  },
  {
    "id": 420423129,
    "number": 5464,
    "user": {
      "login": "ibnesayeed",
      "id": 65147
    },
    "diff_url": "https://github.com/psf/requests/pull/5464.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/5464",
    "title": "Allow semicolon in the href of Link header",
    "body": "Semicolons are valid characters in URIs, splitting them while parsing the `Link` header results in unexpected token and wrong `url` attribute (notice `;oldid=934259284` portion the below example).\r\n\r\n```\r\n<https://en.wikipedia.org/w/index.php?title=COVID-19_pandemic&amp;oldid=934259284>; rel=\"original\", <https://web.archive.org/web/timemap/link/https://en.wikipedia.org/w/index.php?title=COVID-19_pandemic&amp;oldid=934259284>; rel=\"timemap\"; type=\"application/link-format\"\r\n```\r\n\r\nThis PR attempts to fix current parsing issue and also renames a variable to better reflect the purpose of it.\r\n\r\nI would note here that this parser still is far from perfect and can be better implemented using state machine.",
    "diff": "diff --git a/requests/utils.py b/requests/utils.py\nindex c1700d7fe8..c6509a6c61 100644\n--- a/requests/utils.py\n+++ b/requests/utils.py\n@@ -826,27 +826,24 @@ def parse_header_links(value):\n \n     links = []\n \n-    replace_chars = ' \\'\"'\n+    strip_chars = ' \\'\";'\n \n-    value = value.strip(replace_chars)\n+    value = value.strip(strip_chars)\n     if not value:\n         return links\n \n     for val in re.split(', *<', value):\n-        try:\n-            url, params = val.split(';', 1)\n-        except ValueError:\n-            url, params = val, ''\n+        url, _, params = val.partition('>')\n \n         link = {'url': url.strip('<> \\'\"')}\n \n-        for param in params.split(';'):\n+        for param in params.strip(strip_chars).split(';'):\n             try:\n                 key, value = param.split('=')\n             except ValueError:\n                 break\n \n-            link[key.strip(replace_chars)] = value.strip(replace_chars)\n+            link[key.strip(strip_chars)] = value.strip(strip_chars)\n \n         links.append(link)\n \ndiff --git a/tests/test_utils.py b/tests/test_utils.py\nindex 463516b2e5..db14d362c5 100644\n--- a/tests/test_utils.py\n+++ b/tests/test_utils.py\n@@ -556,6 +556,10 @@ def test_iter_slices(value, length):\n             '<http:/.../front.jpeg>; rel=front; type=\"image/jpeg\"',\n             [{'url': 'http:/.../front.jpeg', 'rel': 'front', 'type': 'image/jpeg'}]\n         ),\n+        (\n+            '<http:/.../front.jpeg?a=b;id=123>; rel=\"original\"',\n+            [{'url': 'http:/.../front.jpeg?a=b;id=123', 'rel': 'original'}]\n+        ),\n         (\n             '<http:/.../front.jpeg>',\n             [{'url': 'http:/.../front.jpeg'}]\n"
  },
  {
    "id": 409753140,
    "number": 5441,
    "user": {
      "login": "bluebird75",
      "id": 216653
    },
    "diff_url": "https://github.com/psf/requests/pull/5441.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/5441",
    "title": "Strip double-quotes on proxy environment variables and add test (issue #4613)",
    "body": "Issue #4613 details two problems :\r\n* proxy may contain white-space accidentally\r\n* proxy may be contain extra double-quotes on Windows\r\n\r\nA fix already exists for the white-space problem, this addresses the second problem.",
    "diff": "diff --git a/requests/utils.py b/requests/utils.py\nindex c1700d7fe8..9a9697f546 100644\n--- a/requests/utils.py\n+++ b/requests/utils.py\n@@ -613,6 +613,19 @@ def requote_uri(uri):\n         return quote(uri, safe=safe_without_percent)\n \n \n+def strip_double_quotes_from_url(url):\n+    \"\"\"Remove the double quotes around url if any are presents.\n+\n+    '\"http://www.myproxy.com\"' -> 'http://www.myproxy.com'\n+    'http://www.otherproxy.com' -> 'http://www.otherproxy.com'\n+\n+    :param url:\n+    :rtype: str\n+    \"\"\"\n+    if url and len(url) >= 2 and url[0] == '\"' and url[-1] =='\"':\n+        url = url[1:-1]\n+    return url\n+\n def address_in_network(ip, net):\n     \"\"\"This function allows you to check if an IP belongs to a network subnet\n \n@@ -766,7 +779,10 @@ def get_environ_proxies(url, no_proxy=None):\n     if should_bypass_proxies(url, no_proxy=no_proxy):\n         return {}\n     else:\n-        return getproxies()\n+        # strip the double-quotes for overquoted proxies (see end of issue #4613)\n+        # Typical case on Windows is created by: set http_proxy=\"http://www.myproxy.com\"\n+        # -> proxy value is set to '\"http://www.myproxy.com\"'\n+        return { var: strip_double_quotes_from_url(url) for (var,url) in getproxies().items() }\n \n \n def select_proxy(url, proxies):\ndiff --git a/tests/test_requests.py b/tests/test_requests.py\nindex 7d4a4eb510..176c90e0b3 100644\n--- a/tests/test_requests.py\n+++ b/tests/test_requests.py\n@@ -2285,6 +2285,25 @@ def test_requests_are_updated_each_time(httpbin):\n         assert session.calls[-1] == send_call\n \n \n+@pytest.mark.parametrize(\"var,url,proxy\", [\n+    ('http_proxy', 'http://example.com', '\"https://proxy.com:9876\"'),\n+    ('https_proxy', 'https://example.com', '\"http://proxy.com:9876\"'),\n+    ('all_proxy', 'http://example.com', '\"https://proxy.com:9876\"'),\n+])\n+def test_proxy_url_strips_quotes(var, url, proxy):\n+    session = requests.Session()\n+    prep = PreparedRequest()\n+    prep.prepare(method='GET', url=url)\n+\n+    kwargs = {\n+        var: proxy\n+    }\n+    scheme = urlparse(url).scheme\n+    with override_environ(**kwargs):\n+        proxies = session.rebuild_proxies(prep, {})\n+        assert scheme in proxies\n+        assert proxies[scheme] == proxy[1:-1]   # with quotes stripped\n+\n @pytest.mark.parametrize(\"var,url,proxy\", [\n     ('http_proxy', 'http://example.com', 'socks5://proxy.com:9876'),\n     ('https_proxy', 'https://example.com', 'socks5://proxy.com:9876'),\n"
  },
  {
    "id": 338551555,
    "number": 5257,
    "user": {
      "login": "jjlogu",
      "id": 1691611
    },
    "diff_url": "https://github.com/psf/requests/pull/5257.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/5257",
    "title": "Safely handle, when uri contains IPv6 link local with %zone index ",
    "body": "## Issue:\r\nWhen URI contains IPv6 link local with zone index (Ex: fe80::726f:8a26:222a:2bf3%eth0), requote_uri encodes all % to %25. Because of it, requests.get was requesting different resource.\r\n\r\nScreenshot: \r\n![image](https://user-images.githubusercontent.com/1691611/68458332-8b76fd80-0228-11ea-9c6f-8e439f257180.png)\r\n\r\nI'm proposing to ignore domain part in utils.unquote_unreserved.\r\n\r\n## Actual Result\r\n<pre>\r\n>>> requests.utils.requote_uri('http://[fe80::726f:8a26:222a:2bf3%eth0]:8080/a%20b/index.html')\r\n'http://[fe80::726f:8a26:222a:2bf3%25eth0]:8080/a%2520b/index.html'\r\n</pre>\r\n\r\n## Expected Result\r\n<pre>\r\n>>> requests.utils.requote_uri('http://[fe80::726f:8a26:222a:2bf3%eth0]:8080/a%20b/index.html')\r\n'http://[fe80::726f:8a26:222a:2bf3%eth0]:8080/a%20b/index.html'\r\n</pre>\r\n\r\n## System Information\r\n<pre>\r\n# python3 -m requests.help\r\n{\r\n  \"chardet\": {\r\n    \"version\": \"3.0.4\"\r\n  },\r\n  \"cryptography\": {\r\n    \"version\": \"2.2.2\"\r\n  },\r\n  \"idna\": {\r\n    \"version\": \"2.8\"\r\n  },\r\n  \"implementation\": {\r\n    \"name\": \"CPython\",\r\n    \"version\": \"3.5.2\"\r\n  },\r\n  \"platform\": {\r\n    \"release\": \"4.4.71-UNRELEASED-v4-00050-g76f27ecf9a52\",\r\n    \"system\": \"Linux\"\r\n  },\r\n  \"pyOpenSSL\": {\r\n    \"openssl_version\": \"1000207f\",\r\n    \"version\": \"18.0.0\"\r\n  },\r\n  \"requests\": {\r\n    \"version\": \"2.22.0\"\r\n  },\r\n  \"system_ssl\": {\r\n    \"version\": \"1000207f\"\r\n  },\r\n  \"urllib3\": {\r\n    \"version\": \"1.24.1\"\r\n  },\r\n  \"using_pyopenssl\": true\r\n}\r\n</pre>",
    "diff": "diff --git a/requests/utils.py b/requests/utils.py\nindex c1700d7fe8..c7083e3af9 100644\n--- a/requests/utils.py\n+++ b/requests/utils.py\n@@ -573,7 +573,15 @@ def unquote_unreserved(uri):\n \n     :rtype: str\n     \"\"\"\n-    parts = uri.split('%')\n+    delim = len(uri)\n+    start = uri.find(\"//\")+2\n+    for c in '/?#':\n+        wdelim = uri.find(c, start)\n+        if wdelim >= 0:\n+            delim = min(delim, wdelim)\n+    # IPv6 link local may have %zone_index. Safely ignore it.\n+    # Ex: http://[fe80::726f:8a26:222a:2bf3%eth0]:8080/a%20b/index.html\n+    parts = uri[delim:].split('%')\n     for i in range(1, len(parts)):\n         h = parts[i][0:2]\n         if len(h) == 2 and h.isalnum():\n@@ -588,7 +596,7 @@ def unquote_unreserved(uri):\n                 parts[i] = '%' + parts[i]\n         else:\n             parts[i] = '%' + parts[i]\n-    return ''.join(parts)\n+    return uri[:delim] + ''.join(parts)\n \n \n def requote_uri(uri):\n"
  },
  {
    "id": 329380812,
    "number": 5232,
    "user": {
      "login": "ofek",
      "id": 9677399
    },
    "diff_url": "https://github.com/psf/requests/pull/5232.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/5232",
    "title": "Add timeout as a default attribute for Session objects",
    "body": "We encountered a bug today wherein we mistakenly assumed it was already supported as an attribute like all the other kwargs, https://github.com/DataDog/integrations-core/pull/4811",
    "diff": "diff --git a/requests/sessions.py b/requests/sessions.py\nindex cd8a8ae6b6..184148711d 100644\n--- a/requests/sessions.py\n+++ b/requests/sessions.py\n@@ -355,7 +355,7 @@ class Session(SessionRedirectMixin):\n \n     __attrs__ = [\n         'headers', 'cookies', 'auth', 'proxies', 'hooks', 'params', 'verify',\n-        'cert', 'prefetch', 'adapters', 'stream', 'trust_env',\n+        'timeout', 'cert', 'prefetch', 'adapters', 'stream', 'trust_env',\n         'max_redirects',\n     ]\n \n@@ -370,6 +370,10 @@ def __init__(self):\n         #: :class:`Request <Request>`.\n         self.auth = None\n \n+        #: Default timeout tuple or int to attach to\n+        #: :class:`Request <Request>`.\n+        self.timeout = None\n+\n         #: Dictionary mapping protocol or protocol and host to the URL of the proxy\n         #: (e.g. {'http': 'foo.bar:3128', 'http://host.name': 'foo.bar:4012'}) to\n         #: be used on each :class:`Request <Request>`.\n@@ -518,12 +522,11 @@ def request(self, method, url,\n         proxies = proxies or {}\n \n         settings = self.merge_environment_settings(\n-            prep.url, proxies, stream, verify, cert\n+            prep.url, proxies, stream, verify, cert, timeout\n         )\n \n         # Send the request.\n         send_kwargs = {\n-            'timeout': timeout,\n             'allow_redirects': allow_redirects,\n         }\n         send_kwargs.update(settings)\n@@ -622,6 +625,7 @@ def send(self, request, **kwargs):\n         kwargs.setdefault('verify', self.verify)\n         kwargs.setdefault('cert', self.cert)\n         kwargs.setdefault('proxies', self.proxies)\n+        kwargs.setdefault('timeout', self.timeout)\n \n         # It's possible that users might accidentally send a Request object.\n         # Guard against that specific failure case.\n@@ -684,7 +688,7 @@ def send(self, request, **kwargs):\n \n         return r\n \n-    def merge_environment_settings(self, url, proxies, stream, verify, cert):\n+    def merge_environment_settings(self, url, proxies, stream, verify, cert, timeout):\n         \"\"\"\n         Check the environment and merge it with some settings.\n \n@@ -709,9 +713,10 @@ def merge_environment_settings(self, url, proxies, stream, verify, cert):\n         stream = merge_setting(stream, self.stream)\n         verify = merge_setting(verify, self.verify)\n         cert = merge_setting(cert, self.cert)\n+        timeout = merge_setting(timeout, self.timeout)\n \n         return {'verify': verify, 'proxies': proxies, 'stream': stream,\n-                'cert': cert}\n+                'cert': cert, 'timeout': timeout}\n \n     def get_adapter(self, url):\n         \"\"\"\ndiff --git a/tests/test_requests.py b/tests/test_requests.py\nindex 7d4a4eb510..4d8d5636a2 100644\n--- a/tests/test_requests.py\n+++ b/tests/test_requests.py\n@@ -2199,6 +2199,16 @@ def test_connect_timeout(self, timeout):\n             assert isinstance(e, ConnectionError)\n             assert isinstance(e, Timeout)\n \n+    def test_connect_timeout_session_attribute_default(self):\n+        try:\n+            s = requests.Session()\n+            s.timeout = 0.1\n+            s.get(TARPIT)\n+            pytest.fail('The connect() request should time out.')\n+        except ConnectTimeout as e:\n+            assert isinstance(e, ConnectionError)\n+            assert isinstance(e, Timeout)\n+\n     @pytest.mark.parametrize(\n         'timeout', (\n             (0.1, 0.1),\n@@ -2421,12 +2431,12 @@ class TestPreparingURLs(object):\n     def test_preparing_url(self, url, expected):\n \n         def normalize_percent_encode(x):\n-            # Helper function that normalizes equivalent \n+            # Helper function that normalizes equivalent\n             # percent-encoded bytes before comparisons\n             for c in re.findall(r'%[a-fA-F0-9]{2}', x):\n                 x = x.replace(c, c.upper())\n             return x\n-        \n+\n         r = requests.Request('GET', url=url)\n         p = r.prepare()\n         assert normalize_percent_encode(p.url) == expected\n"
  },
  {
    "id": 323892964,
    "number": 5221,
    "user": {
      "login": "Overv",
      "id": 285063
    },
    "diff_url": "https://github.com/psf/requests/pull/5221.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/5221",
    "title": "Change raise_for_status message to hide password in URL",
    "body": "This PR fixes #5021 by replacing a password in the URL with asterisks.\r\n\r\nExample:\r\n\r\n```python\r\nimport requests\r\n\r\nr = requests.get(\"http://foo:secret@httpbin.org/status/401\")\r\nr.raise_for_status()\r\n```\r\n\r\nOutput:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"5021.py\", line 4, in <module>\r\n    r.raise_for_status()\r\n  File \"***/python-requests/requests/requests/models.py\", line 968, in raise_for_status\r\n    raise HTTPError(http_error_msg, response=self)\r\nrequests.exceptions.HTTPError: 401 Client Error: UNAUTHORIZED for url: http://foo:***@httpbin.org/status/401\r\n```",
    "diff": "diff --git a/requests/models.py b/requests/models.py\nindex a60b5f4490..757abdcadd 100644\n--- a/requests/models.py\n+++ b/requests/models.py\n@@ -37,7 +37,7 @@\n     iter_slices, guess_json_utf, super_len, check_header_validity)\n from .compat import (\n     Callable, Mapping,\n-    cookielib, urlunparse, urlsplit, urlencode, str, bytes,\n+    cookielib, urlparse, urlunparse, urlsplit, urlencode, str, bytes,\n     is_py2, chardet, builtin_str, basestring)\n from .compat import json as complexjson\n from .status_codes import codes\n@@ -951,11 +951,19 @@ def raise_for_status(self):\n         else:\n             reason = self.reason\n \n+        # Strip any password from URL\n+        parsed_url = urlparse(self.url)\n+        if parsed_url.password is not None:\n+            hostname = parsed_url.netloc.split('@')[-1]\n+            redacted_url = urlunparse(parsed_url._replace(netloc=\"%s:%s@%s\" % (parsed_url.username, '***', hostname)))\n+        else:\n+            redacted_url = urlunparse(parsed_url)\n+\n         if 400 <= self.status_code < 500:\n-            http_error_msg = u'%s Client Error: %s for url: %s' % (self.status_code, reason, self.url)\n+            http_error_msg = u'%s Client Error: %s for url: %s' % (self.status_code, reason, redacted_url)\n \n         elif 500 <= self.status_code < 600:\n-            http_error_msg = u'%s Server Error: %s for url: %s' % (self.status_code, reason, self.url)\n+            http_error_msg = u'%s Server Error: %s for url: %s' % (self.status_code, reason, redacted_url)\n \n         if http_error_msg:\n             raise HTTPError(http_error_msg, response=self)\ndiff --git a/tests/test_requests.py b/tests/test_requests.py\nindex 7d4a4eb510..e7cf8fcd1e 100644\n--- a/tests/test_requests.py\n+++ b/tests/test_requests.py\n@@ -789,6 +789,24 @@ def test_status_raising(self, httpbin):\n         r = requests.get(httpbin('status', '500'))\n         assert not r.ok\n \n+    def test_status_raising_hides_password(self, httpbin):\n+        host = urlparse(httpbin()).netloc\n+\n+        r = requests.get('http://user:pass@' + host + '/status/404')\n+        with pytest.raises(requests.exceptions.HTTPError) as e:\n+            r.raise_for_status()\n+\n+        assert str(e.value) == '404 Client Error: NOT FOUND for url: http://user:***@' + host + '/status/404'\n+\n+    def test_status_raising_does_not_hide_users(self, httpbin):\n+        host = urlparse(httpbin()).netloc\n+\n+        r = requests.get('http://user@' + host + '/status/404')\n+        with pytest.raises(requests.exceptions.HTTPError) as e:\n+            r.raise_for_status()\n+\n+        assert str(e.value) == '404 Client Error: NOT FOUND for url: http://user@' + host + '/status/404'\n+\n     def test_decompress_gzip(self, httpbin):\n         r = requests.get(httpbin('gzip'))\n         r.content.decode('ascii')\n@@ -2421,12 +2439,12 @@ class TestPreparingURLs(object):\n     def test_preparing_url(self, url, expected):\n \n         def normalize_percent_encode(x):\n-            # Helper function that normalizes equivalent \n+            # Helper function that normalizes equivalent\n             # percent-encoded bytes before comparisons\n             for c in re.findall(r'%[a-fA-F0-9]{2}', x):\n                 x = x.replace(c, c.upper())\n             return x\n-        \n+\n         r = requests.Request('GET', url=url)\n         p = r.prepare()\n         assert normalize_percent_encode(p.url) == expected\n"
  },
  {
    "id": 323878017,
    "number": 5220,
    "user": {
      "login": "Overv",
      "id": 285063
    },
    "diff_url": "https://github.com/psf/requests/pull/5220.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/5220",
    "title": "Fix connection pool managers not taking varying TLS parameters into account",
    "body": "This PR fixes #5140 and #4325 by returning different connection and proxy pools based on the connection parameters for the current request.\r\n\r\nI tested it with the following code:\r\n\r\n```python\r\nimport requests\r\n\r\nsession = requests.Session()\r\n\r\nprint(session.get('https://client.badssl.com/'))\r\nprint(session.get('https://client.badssl.com/', cert=('client.crt', 'client.key')))\r\nprint(session.get('https://client.badssl.com/', cert=('client.crt', 'client.key')))\r\nprint(session.get('https://client.badssl.com/'))\r\nprint(session.get('https://client.badssl.com/'))\r\n```\r\n\r\nWhich correctly outputs:\r\n\r\n```\r\n<Response [400]>\r\n<Response [200]>\r\n<Response [200]>\r\n<Response [400]>\r\n<Response [400]>\r\n```\r\n\r\nInstead of the previous output:\r\n\r\n```\r\n<Response [400]>\r\n<Response [400]>\r\n<Response [400]>\r\n<Response [400]>\r\n<Response [400]>\r\n```",
    "diff": "diff --git a/requests/adapters.py b/requests/adapters.py\nindex 97ea25b42a..73c3263517 100644\n--- a/requests/adapters.py\n+++ b/requests/adapters.py\n@@ -119,6 +119,7 @@ def __init__(self, pool_connections=DEFAULT_POOLSIZE,\n             self.max_retries = Retry.from_int(max_retries)\n         self.config = {}\n         self.proxy_manager = {}\n+        self.pool_manager = {}\n \n         super(HTTPAdapter, self).__init__()\n \n@@ -126,8 +127,6 @@ def __init__(self, pool_connections=DEFAULT_POOLSIZE,\n         self._pool_maxsize = pool_maxsize\n         self._pool_block = pool_block\n \n-        self.init_poolmanager(pool_connections, pool_maxsize, block=pool_block)\n-\n     def __getstate__(self):\n         return {attr: getattr(self, attr, None) for attr in self.__attrs__}\n \n@@ -135,83 +134,93 @@ def __setstate__(self, state):\n         # Can't handle by adding 'proxy_manager' to self.__attrs__ because\n         # self.poolmanager uses a lambda function, which isn't pickleable.\n         self.proxy_manager = {}\n+        self.pool_manager = {}\n         self.config = {}\n \n         for attr, value in state.items():\n             setattr(self, attr, value)\n \n-        self.init_poolmanager(self._pool_connections, self._pool_maxsize,\n-                              block=self._pool_block)\n-\n-    def init_poolmanager(self, connections, maxsize, block=DEFAULT_POOLBLOCK, **pool_kwargs):\n-        \"\"\"Initializes a urllib3 PoolManager.\n+    def pool_manager_for(self, **pool_kwargs):\n+        \"\"\"Return urllib3 PoolManager for the given parameters.\n \n         This method should not be called from user code, and is only\n         exposed for use when subclassing the\n         :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n \n-        :param connections: The number of urllib3 connection pools to cache.\n-        :param maxsize: The maximum number of connections to save in the pool.\n-        :param block: Block when no free connections are available.\n-        :param pool_kwargs: Extra keyword arguments used to initialize the Pool Manager.\n+        :param pool_kwargs: Extra keyword arguments used to configure the Pool Manager.\n+        :returns: PoolManager\n+        :rtype urllib3.PoolManager\n         \"\"\"\n-        # save these values for pickling\n-        self._pool_connections = connections\n-        self._pool_maxsize = maxsize\n-        self._pool_block = block\n+        key = frozenset(pool_kwargs.items())\n \n-        self.poolmanager = PoolManager(num_pools=connections, maxsize=maxsize,\n-                                       block=block, strict=True, **pool_kwargs)\n+        if key in self.pool_manager:\n+            manager = self.pool_manager[key]\n+        else:\n+            manager = self.pool_manager[key] = PoolManager(\n+                num_pools=self._pool_connections,\n+                maxsize=self._pool_maxsize,\n+                block=self._pool_block,\n+                strict=True,\n+                **pool_kwargs\n+            )\n+\n+        return manager\n \n-    def proxy_manager_for(self, proxy, **proxy_kwargs):\n-        \"\"\"Return urllib3 ProxyManager for the given proxy.\n+    def proxy_manager_for(self, proxy, **pool_kwargs):\n+        \"\"\"Return urllib3 ProxyManager for the given proxy and parameters.\n \n         This method should not be called from user code, and is only\n         exposed for use when subclassing the\n         :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n \n         :param proxy: The proxy to return a urllib3 ProxyManager for.\n-        :param proxy_kwargs: Extra keyword arguments used to configure the Proxy Manager.\n+        :param pool_kwargs: Extra keyword arguments used to configure the Proxy Manager.\n         :returns: ProxyManager\n         :rtype: urllib3.ProxyManager\n         \"\"\"\n-        if proxy in self.proxy_manager:\n-            manager = self.proxy_manager[proxy]\n+        key = (proxy, frozenset(pool_kwargs.items()))\n+\n+        if key in self.proxy_manager:\n+            manager = self.proxy_manager[key]\n         elif proxy.lower().startswith('socks'):\n             username, password = get_auth_from_url(proxy)\n-            manager = self.proxy_manager[proxy] = SOCKSProxyManager(\n+            manager = self.proxy_manager[key] = SOCKSProxyManager(\n                 proxy,\n                 username=username,\n                 password=password,\n                 num_pools=self._pool_connections,\n                 maxsize=self._pool_maxsize,\n                 block=self._pool_block,\n-                **proxy_kwargs\n+                **pool_kwargs\n             )\n         else:\n             proxy_headers = self.proxy_headers(proxy)\n-            manager = self.proxy_manager[proxy] = proxy_from_url(\n+            manager = self.proxy_manager[key] = proxy_from_url(\n                 proxy,\n                 proxy_headers=proxy_headers,\n                 num_pools=self._pool_connections,\n                 maxsize=self._pool_maxsize,\n                 block=self._pool_block,\n-                **proxy_kwargs)\n+                **pool_kwargs)\n \n         return manager\n \n-    def cert_verify(self, conn, url, verify, cert):\n-        \"\"\"Verify a SSL certificate. This method should not be called from user\n-        code, and is only exposed for use when subclassing the\n-        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n+    def get_pool_args(self, url, verify, cert):\n+        \"\"\"Return connection pool parameters for verifying an SSL certificate.\n+        This method should not be called from user code, and is only exposed for\n+        use when subclassing the :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n \n-        :param conn: The urllib3 connection object associated with the cert.\n         :param url: The requested URL.\n         :param verify: Either a boolean, in which case it controls whether we verify\n             the server's TLS certificate, or a string, in which case it must be a path\n             to a CA bundle to use\n         :param cert: The SSL certificate to verify.\n+        :returns: Pool parameters.\n+        :rtype: dict\n         \"\"\"\n+\n+        parameters = {}\n+\n         if url.lower().startswith('https') and verify:\n \n             cert_loc = None\n@@ -227,30 +236,32 @@ def cert_verify(self, conn, url, verify, cert):\n                 raise IOError(\"Could not find a suitable TLS CA certificate bundle, \"\n                               \"invalid path: {}\".format(cert_loc))\n \n-            conn.cert_reqs = 'CERT_REQUIRED'\n+            parameters['cert_reqs'] = 'CERT_REQUIRED'\n \n             if not os.path.isdir(cert_loc):\n-                conn.ca_certs = cert_loc\n+                parameters['ca_certs'] = cert_loc\n             else:\n-                conn.ca_cert_dir = cert_loc\n+                parameters['.ca_cert_dir'] = cert_loc\n         else:\n-            conn.cert_reqs = 'CERT_NONE'\n-            conn.ca_certs = None\n-            conn.ca_cert_dir = None\n+            parameters['cert_reqs'] = 'CERT_NONE'\n+            parameters['ca_certs'] = None\n+            parameters['ca_cert_dir'] = None\n \n         if cert:\n             if not isinstance(cert, basestring):\n-                conn.cert_file = cert[0]\n-                conn.key_file = cert[1]\n+                parameters['cert_file'] = cert[0]\n+                parameters['key_file'] = cert[1]\n             else:\n-                conn.cert_file = cert\n-                conn.key_file = None\n-            if conn.cert_file and not os.path.exists(conn.cert_file):\n+                parameters['cert_file'] = cert\n+                parameters['key_file'] = None\n+            if parameters['cert_file'] and not os.path.exists(parameters['cert_file']):\n                 raise IOError(\"Could not find the TLS certificate file, \"\n-                              \"invalid path: {}\".format(conn.cert_file))\n-            if conn.key_file and not os.path.exists(conn.key_file):\n+                              \"invalid path: {}\".format(parameters['cert_file']))\n+            if parameters['key_file'] and not os.path.exists(parameters['key_file']):\n                 raise IOError(\"Could not find the TLS key file, \"\n-                              \"invalid path: {}\".format(conn.key_file))\n+                              \"invalid path: {}\".format(parameters['key_file']))\n+\n+        return parameters\n \n     def build_response(self, req, resp):\n         \"\"\"Builds a :class:`Response <requests.Response>` object from a urllib3\n@@ -289,13 +300,14 @@ def build_response(self, req, resp):\n \n         return response\n \n-    def get_connection(self, url, proxies=None):\n+    def get_connection(self, url, proxies=None, **pool_kwargs):\n         \"\"\"Returns a urllib3 connection for the given URL. This should not be\n         called from user code, and is only exposed for use when subclassing the\n         :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n \n         :param url: The URL to connect to.\n         :param proxies: (optional) A Requests-style dictionary of proxies used on this request.\n+        :param pool_kwargs: Parameters for connection pool to use.\n         :rtype: urllib3.ConnectionPool\n         \"\"\"\n         proxy = select_proxy(url, proxies)\n@@ -306,13 +318,15 @@ def get_connection(self, url, proxies=None):\n             if not proxy_url.host:\n                 raise InvalidProxyURL(\"Please check proxy URL. It is malformed\"\n                                       \" and could be missing the host.\")\n-            proxy_manager = self.proxy_manager_for(proxy)\n+            proxy_manager = self.proxy_manager_for(proxy, **pool_kwargs)\n             conn = proxy_manager.connection_from_url(url)\n         else:\n+            pool_manager = self.pool_manager_for(**pool_kwargs)\n+\n             # Only scheme should be lower case\n             parsed = urlparse(url)\n             url = parsed.geturl()\n-            conn = self.poolmanager.connection_from_url(url)\n+            conn = pool_manager.connection_from_url(url)\n \n         return conn\n \n@@ -322,7 +336,8 @@ def close(self):\n         Currently, this closes the PoolManager and any active ProxyManager,\n         which closes any pooled connections.\n         \"\"\"\n-        self.poolmanager.clear()\n+        for pool in self.pool_manager.values():\n+            pool.clear()\n         for proxy in self.proxy_manager.values():\n             proxy.clear()\n \n@@ -408,12 +423,13 @@ def send(self, request, stream=False, timeout=None, verify=True, cert=None, prox\n         :rtype: requests.Response\n         \"\"\"\n \n+        pool_kwargs = self.get_pool_args(request.url, verify, cert)\n+\n         try:\n-            conn = self.get_connection(request.url, proxies)\n+            conn = self.get_connection(request.url, proxies, **pool_kwargs)\n         except LocationValueError as e:\n             raise InvalidURL(e, request=request)\n \n-        self.cert_verify(conn, request.url, verify, cert)\n         url = self.request_url(request, proxies)\n         self.add_headers(request, stream=stream, timeout=timeout, verify=verify, cert=cert, proxies=proxies)\n \n"
  },
  {
    "id": 311767019,
    "number": 5179,
    "user": {
      "login": "epenet",
      "id": 6771947
    },
    "diff_url": "https://github.com/psf/requests/pull/5179.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/5179",
    "title": "HTTPDigestAuth thread fix",
    "body": "Ensure thread-state is initialised on handle_401\r\nRefs #5178",
    "diff": "diff --git a/requests/auth.py b/requests/auth.py\nindex eeface39ae..7af1b58715 100644\n--- a/requests/auth.py\n+++ b/requests/auth.py\n@@ -122,7 +122,7 @@ def init_per_thread_state(self):\n             self._thread_local.nonce_count = 0\n             self._thread_local.chal = {}\n             self._thread_local.pos = None\n-            self._thread_local.num_401_calls = None\n+            self._thread_local.num_401_calls = 0\n \n     def build_digest_header(self, method, url):\n         \"\"\"\n@@ -238,6 +238,9 @@ def handle_401(self, r, **kwargs):\n         :rtype: requests.Response\n         \"\"\"\n \n+        # Initialize per-thread state, if needed\n+        self.init_per_thread_state()\n+\n         # If response is not 4xx, do not auth\n         # See https://github.com/psf/requests/issues/3772\n         if not 400 <= r.status_code < 500:\n"
  },
  {
    "id": 309714151,
    "number": 5172,
    "user": {
      "login": "AndTheDaysGoBy",
      "id": 11529426
    },
    "diff_url": "https://github.com/psf/requests/pull/5172.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/5172",
    "title": "Fix Issue of Ignoring Session-level Settings",
    "body": "A re-write of https://github.com/psf/requests/pull/4935\r\nAddresses: https://github.com/psf/requests/issues/4938\r\nand perhaps others.\r\n\r\nIn short, there are three types of values: the instance values (e.g. those passed in via the `get(...)`, `request(...)`, etc.), the session values (e.g. those set for the session such as `s.verify = \"/some/cert\"`), and system or environment level values (e.g. the environment variable for the CA_BUNDLE that, at times, becomes the value for the verify flag).\r\n\r\nWorking with @sorech02, we determined that the best course of action was to just move the session level merging before the environment merging. I.e., the current flow is as follows: one of the simple request calls is made, e.g. `request`, `get`, `delete`, etc., then, the `merge_environment_settings` method is called wherein environment values are merged in (e.g. the CA_BUNDLE value) and then the session level settings are merged in via the `merge_settings` calls in `merge_environment_settings`.\r\n\r\nThis is too late because, by default, trust_env is set to True, which will result in the `verify` value passed into the session merging to not be none. One solution is to make sure you set trust_env to False before using the verify flag with a custom certificate, but we deemed this to only be a work around.\r\n\r\nThe true fix, as described above, is to ensure that the settings which are constructed give the proper precedence to the values given. I.e. passed > session > environment.\r\nWe were forced to leave the `setdefaults` (which effectively reapply the session values if there are no values set) in the `def send` alone because, at times, one might use `send` itself when constructing prepared requests (i.e. not via `request`, `get`, etc.). In which  case, the function must still ensure to use the session level settings.",
    "diff": "diff --git a/requests/sessions.py b/requests/sessions.py\nindex fdf7e9fe35..ca0d93e133 100644\n--- a/requests/sessions.py\n+++ b/requests/sessions.py\n@@ -704,6 +704,12 @@ def merge_environment_settings(self, url, proxies, stream, verify, cert):\n \n         :rtype: dict\n         \"\"\"\n+        # Merge all the kwargs.\n+        proxies = merge_setting(proxies, self.proxies)\n+        stream = merge_setting(stream, self.stream)\n+        verify = merge_setting(verify, self.verify)\n+        cert = merge_setting(cert, self.cert)\n+\n         # Gather clues from the surrounding environment.\n         if self.trust_env:\n             # Set environment's proxies.\n@@ -716,13 +722,7 @@ def merge_environment_settings(self, url, proxies, stream, verify, cert):\n             # with cURL.\n             if verify is True or verify is None:\n                 verify = (os.environ.get('REQUESTS_CA_BUNDLE') or\n-                          os.environ.get('CURL_CA_BUNDLE'))\n-\n-        # Merge all the kwargs.\n-        proxies = merge_setting(proxies, self.proxies)\n-        stream = merge_setting(stream, self.stream)\n-        verify = merge_setting(verify, self.verify)\n-        cert = merge_setting(cert, self.cert)\n+                          os.environ.get('CURL_CA_BUNDLE')) or True\n \n         return {'verify': verify, 'proxies': proxies, 'stream': stream,\n                 'cert': cert}\ndiff --git a/tests/test_requests.py b/tests/test_requests.py\nindex e730f7648b..5cba44e7fc 100644\n--- a/tests/test_requests.py\n+++ b/tests/test_requests.py\n@@ -2527,3 +2527,14 @@ def test_parameters_for_nonstandard_schemes(self, input, params, expected):\n         r = requests.Request('GET', url=input, params=params)\n         p = r.prepare()\n         assert p.url == expected\n+\n+    def test_default_to_session_settings(self, mocker):\n+        send = mocker.patch.object(requests.adapters.HTTPAdapter, \"send\")\n+        s = requests.session()\n+        s.verify = \"/some/cert\"\n+        try:\n+            s.request(\"GET\", \"http://example.com\")\n+        except Exception:\n+            pass\n+        _, kwargs = send.call_args\n+        assert kwargs[\"verify\"] == \"/some/cert\"\n"
  },
  {
    "id": 241389273,
    "number": 4920,
    "user": {
      "login": "alex-wenzel",
      "id": 32204469
    },
    "diff_url": "https://github.com/psf/requests/pull/4920.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/4920",
    "title": "iter_content() checks for < 1 chunk_size",
    "body": "With requests v2.20.1, supplying a value of 0 for `chunk_size` causes `iter_content()` to hang. This made debugging code that dynamically computes a value for `chunk_size` difficult. These two lines explicitly enforce `chunk_size` being a positive integer.",
    "diff": "diff --git a/requests/models.py b/requests/models.py\nindex 62dcd0b7c8..c81e918e54 100644\n--- a/requests/models.py\n+++ b/requests/models.py\n@@ -769,6 +769,8 @@ def generate():\n             raise StreamConsumedError()\n         elif chunk_size is not None and not isinstance(chunk_size, int):\n             raise TypeError(\"chunk_size must be an int, it is instead a %s.\" % type(chunk_size))\n+        elif chunk_size is not None and chunk_size < 1:\n+            raise ValueError(\"chunk_size must be a positive integer, got \"+str(chunk_size))\n         # simulate reading small chunks of the content\n         reused_chunks = iter_slices(self._content, chunk_size)\n \n"
  },
  {
    "id": 219427631,
    "number": 4812,
    "user": {
      "login": "gilbsgilbs",
      "id": 3407667
    },
    "diff_url": "https://github.com/psf/requests/pull/4812.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/4812",
    "title": "Fix no_proxy being too greedy.",
    "body": "**Summary:** Setting `no_proxy` on `requests.com` used to enable it on `somerequests.com` also, which was unexpected and potentially dangerous. (see #4795)\r\n\r\nNote that this delegates to urllib more often, as stated [here](https://github.com/requests/requests/issues/4795#issuecomment-425886286). I don't have the history behind `should_bypass_proxies` implementation, therefore my change is the tiniest possible (minimum thing to fix the greedy behavior). I don't know exactly why requests tries some things first and then delegates to urllib though.\r\n\r\nFixes #4795 ",
    "diff": "diff --git a/Pipfile b/Pipfile\nindex 3e0fd729eb..dc62929f33 100644\n--- a/Pipfile\n+++ b/Pipfile\n@@ -4,7 +4,7 @@ verify_ssl = true\n name = \"pypi\"\n \n [dev-packages]\n-pytest = \">=2.8.0\"\n+pytest = \">=3.1.0\"\n codecov = \"*\"\n pytest-httpbin = \">=0.0.7\"\n pytest-mock = \"*\"\ndiff --git a/requests/utils.py b/requests/utils.py\nindex 671973127f..c3058f5d87 100644\n--- a/requests/utils.py\n+++ b/requests/utils.py\n@@ -727,12 +727,16 @@ def should_bypass_proxies(url, no_proxy):\n                     # matches the IP of the index\n                     return True\n         else:\n-            host_with_port = parsed.hostname\n+            host_without_port = '.' + parsed.hostname\n+            host_with_port = host_without_port\n             if parsed.port:\n                 host_with_port += ':{0}'.format(parsed.port)\n \n             for host in no_proxy:\n-                if parsed.hostname.endswith(host) or host_with_port.endswith(host):\n+                if not host.startswith('.'):\n+                    host = '.' + host\n+\n+                if host_without_port.endswith(host) or host_with_port.endswith(host):\n                     # The URL does match something in no_proxy, so we don't want\n                     # to apply the proxies on this URL.\n                     return True\ndiff --git a/tests/test_utils.py b/tests/test_utils.py\nindex f34c630f07..d9e90745b7 100644\n--- a/tests/test_utils.py\n+++ b/tests/test_utils.py\n@@ -6,6 +6,7 @@\n from io import BytesIO\n import zipfile\n from collections import deque\n+import sys\n \n import pytest\n from requests import compat\n@@ -182,6 +183,7 @@ def test_not_bypass(self, url):\n             'http://192.168.1.1:5000/',\n             'http://192.168.1.1/',\n             'http://www.requests.com/',\n+            'http://requests.com/',\n         ))\n     def test_bypass_no_proxy_keyword(self, url):\n         no_proxy = '192.168.1.1,requests.com'\n@@ -194,6 +196,13 @@ def test_bypass_no_proxy_keyword(self, url):\n             'http://172.16.1.1/',\n             'http://172.16.1.1:5000/',\n             'http://localhost.localdomain:5000/v1.0/',\n+            pytest.param(\n+                'http://somerequests.com',\n+                marks=pytest.mark.skipif(\n+                    (sys.version_info[0], sys.version_info[1]) == (3, 4),\n+                    reason='Python 3.4 implementation of no_proxy is greedy',\n+                ),\n+            ),\n         ))\n     def test_not_bypass_no_proxy_keyword(self, url, monkeypatch):\n         # This is testing that the 'no_proxy' argument overrides the\n@@ -619,6 +628,7 @@ def test_urldefragauth(url, expected):\n             ('http://172.16.1.1:5000/', True),\n             ('http://localhost.localdomain:5000/v1.0/', True),\n             ('http://google.com:6000/', True),\n+            ('http://zgoogle.com:6000/', False),\n             ('http://172.16.1.12/', False),\n             ('http://172.16.1.12:5000/', False),\n             ('http://google.com:5000/v1.0/', False),\n"
  },
  {
    "id": 150688691,
    "number": 4372,
    "user": {
      "login": "mjpieters",
      "id": 46775
    },
    "diff_url": "https://github.com/psf/requests/pull/4372.diff",
    "issue_url": "https://api.github.com/repos/psf/requests/issues/4372",
    "title": "When Location decoding fails, fall back to original",
    "body": "Issue #3888 correctly identified Location headers as *usually* containing UTF-8\r\ncodepoints (when not correctly URL encoded), but this is not always the case.\r\nFor example the URL\r\nhttp://www.finanzen.net/suchergebnis.asp?strSuchString=DE0005933931 redirects\r\nto `b'/etf/ishares_core_dax\\xae_ucits_etf_de'`, containing the Latin-1 byte for\r\nthe \u00ae character.\r\n\r\nIf UTF-8 decoding fails, it is better to fall back to the original.\r\n\r\nThis issue was found via https://stackoverflow.com/questions/47113376/python-3-x-requests-redirect-with-unicode-character",
    "diff": "diff --git a/requests/sessions.py b/requests/sessions.py\nindex 2cedaa8f5f..0b3390cc6a 100644\n--- a/requests/sessions.py\n+++ b/requests/sessions.py\n@@ -113,7 +113,11 @@ def get_redirect_target(self, resp):\n             # To solve this, we re-encode the location in latin1.\n             if is_py3:\n                 location = location.encode('latin1')\n-            return to_native_string(location, 'utf8')\n+            try:\n+                return to_native_string(location, 'utf8')\n+            except UnicodeDecodeError:\n+                # The header was definitely not using UTF-8, retain original\n+                return resp.headers['location']\n         return None\n \n     def resolve_redirects(self, resp, req, stream=False, timeout=None,\ndiff --git a/tests/test_lowlevel.py b/tests/test_lowlevel.py\nindex c87234bc37..0e088efdd5 100644\n--- a/tests/test_lowlevel.py\n+++ b/tests/test_lowlevel.py\n@@ -205,7 +205,7 @@ def test_use_proxy_from_environment(httpbin, var, scheme):\n         assert len(fake_proxy.handler_results[0]) > 0\n \n \n-def test_redirect_rfc1808_to_non_ascii_location():\n+def test_redirect_rfc1808_to_utf8_location():\n     path = u'\u0161'\n     expected_path = b'%C5%A1'\n     redirect_request = []  # stores the second request to the server\n@@ -235,3 +235,34 @@ def redirect_resp_handler(sock):\n         assert r.url == u'{0}/{1}'.format(url, expected_path.decode('ascii'))\n \n         close_server.set()\n+\n+def test_redirect_rfc1808_to_latin1_location():\n+    path = u'\u00e5'\n+    expected_path = b'%C3%A5'\n+    redirect_request = []  # stores the second request to the server\n+\n+    def redirect_resp_handler(sock):\n+        consume_socket_content(sock, timeout=0.5)\n+        location = u'//{0}:{1}/{2}'.format(host, port, path)\n+        sock.send(\n+            b'HTTP/1.1 301 Moved Permanently\\r\\n'\n+            b'Content-Length: 0\\r\\n'\n+            b'Location: ' + location.encode('latin-1') + b'\\r\\n'\n+            b'\\r\\n'\n+        )\n+        redirect_request.append(consume_socket_content(sock, timeout=0.5))\n+        sock.send(b'HTTP/1.1 200 OK\\r\\n\\r\\n')\n+\n+    close_server = threading.Event()\n+    server = Server(redirect_resp_handler, wait_to_close_event=close_server)\n+\n+    with server as (host, port):\n+        url = u'http://{0}:{1}'.format(host, port)\n+        r = requests.get(url=url, allow_redirects=True)\n+        assert r.status_code == 200\n+        assert len(r.history) == 1\n+        assert r.history[0].status_code == 301\n+        assert redirect_request[0].startswith(b'GET /' + expected_path + b' HTTP/1.1')\n+        assert r.url == u'{0}/{1}'.format(url, expected_path.decode('ascii'))\n+\n+        close_server.set()\n"
  }
]