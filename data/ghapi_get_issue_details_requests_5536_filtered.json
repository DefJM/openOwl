{
  "issue": {
    "html_url": "https://github.com/psf/requests/issues/5536",
    "id": 660816593,
    "number": 5536,
    "title": "When stream=True iter_content(chunk_size=None) reads the input as a single big chunk",
    "created_at": "2020-07-19T12:45:58Z",
    "updated_at": "2023-11-09T15:26:01Z",
    "closed_at": null,
    "body": "[According to the documentation](https://2.python-requests.org/en/master/api/#requests.Response.iter_content) when stream=True iter_content(chunk_size=None) \"will read data as it arrives in whatever size the chunks are received\", But it actually collects all input into a single big bytes object consuming large amounts of memory and entirely defeating the purpose of iter_content().\r\n\r\n## Expected Result\r\n\r\niter_content(chunk_size=None) yields \"data as it arrives in whatever size the chunks are received\".\r\n\r\n## Actual Result\r\n\r\nA single big chunk\r\n\r\n## Reproduction Steps\r\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/hexagonrecursion/requests-bug/c34439734aca44a8b5df85d606b2ac63ca0510f0?filepath=repro.ipynb)\r\n```python\r\nfrom requests import get\r\nURL = 'https://dl.fedoraproject.org/pub/alt/iot/32/IoT/x86_64/images/Fedora-IoT-32-20200603.0.x86_64.raw.xz'\r\nr = get(URL, stream=True)\r\nfor b in r.iter_content(chunk_size=None):\r\n    print(len(b))\r\n```\r\nprints\r\n```\r\n533830860\r\n```\r\n\r\n## System Information\r\n\r\n    $ python -m requests.help\r\n\r\n```\r\n{\r\n  \"chardet\": {\r\n    \"version\": \"3.0.4\"\r\n  },\r\n  \"cryptography\": {\r\n    \"version\": \"2.9.2\"\r\n  },\r\n  \"idna\": {\r\n    \"version\": \"2.9\"\r\n  },\r\n  \"implementation\": {\r\n    \"name\": \"CPython\",\r\n    \"version\": \"3.7.6\"\r\n  },\r\n  \"platform\": {\r\n    \"release\": \"4.19.104+\",\r\n    \"system\": \"Linux\"\r\n  },\r\n  \"pyOpenSSL\": {\r\n    \"openssl_version\": \"1010107f\",\r\n    \"version\": \"19.1.0\"\r\n  },\r\n  \"requests\": {\r\n    \"version\": \"2.23.0\"\r\n  },\r\n  \"system_ssl\": {\r\n    \"version\": \"1010107f\"\r\n  },\r\n  \"urllib3\": {\r\n    \"version\": \"1.25.9\"\r\n  },\r\n  \"using_pyopenssl\": true\r\n}\r\n```",
    "author_association": "NONE",
    "comments": 15,
    "state": "open",
    "user": {
      "login": "hexagonrecursion",
      "id": 52621858,
      "site_admin": false
    },
    "reactions": {
      "url": "https://api.github.com/repos/psf/requests/issues/5536/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "assignees": []
  },
  "comments": [
    {
      "url": "https://api.github.com/repos/psf/requests/issues/comments/660660298",
      "id": 660660298,
      "created_at": "2020-07-19T15:10:13Z",
      "updated_at": "2020-07-19T15:10:13Z",
      "author_association": "CONTRIBUTOR",
      "body": "`chunk_size=None` as you've quoted relies on the size of the data as sent by the server. If there server is sending everything all at once and it's all on the socket, what do you expect the library to do differently?",
      "reactions": {
        "url": "https://api.github.com/repos/psf/requests/issues/comments/660660298/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null,
      "user": {
        "login": "sigmavirus24",
        "id": 240830,
        "site_admin": false
      }
    },
    {
      "url": "https://api.github.com/repos/psf/requests/issues/comments/660667947",
      "id": 660667947,
      "created_at": "2020-07-19T16:00:13Z",
      "updated_at": "2020-07-19T16:00:13Z",
      "author_association": "NONE",
      "body": "@sigmavirus24 I don't think the server sends the file all at once. The example above produces no output for ~30 seconds and then prints 533830860. This starts printing right away:\r\n```python3\r\nfrom requests import get\r\nURL = 'https://dl.fedoraproject.org/pub/alt/iot/32/IoT/x86_64/images/Fedora-IoT-32-20200603.0.x86_64.raw.xz'\r\nr = get(URL, stream=True)\r\nfor b in r.iter_content(chunk_size=2**23):\r\n    print(len(b))\r\n```",
      "reactions": {
        "url": "https://api.github.com/repos/psf/requests/issues/comments/660667947/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null,
      "user": {
        "login": "hexagonrecursion",
        "id": 52621858,
        "site_admin": false
      }
    },
    {
      "url": "https://api.github.com/repos/psf/requests/issues/comments/664434720",
      "id": 664434720,
      "created_at": "2020-07-27T14:35:54Z",
      "updated_at": "2020-07-27T14:43:06Z",
      "author_association": "NONE",
      "body": "I have the same issue with 2.24.0\r\n\r\nWhen I use a `chunk_size` of `1` I get the expected output but with a huge overhead.",
      "reactions": {
        "url": "https://api.github.com/repos/psf/requests/issues/comments/664434720/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null,
      "user": {
        "login": "laktak",
        "id": 959858,
        "site_admin": false
      }
    },
    {
      "url": "https://api.github.com/repos/psf/requests/issues/comments/739646016",
      "id": 739646016,
      "created_at": "2020-12-07T03:49:04Z",
      "updated_at": "2020-12-07T03:49:04Z",
      "author_association": "NONE",
      "body": "Can confirm the same is occurring.\r\n\r\nWorks as with `chunk_size=1`, hangs with `None`.\r\n\r\nCan try to put together a reproducible example if that's helpful?",
      "reactions": {
        "url": "https://api.github.com/repos/psf/requests/issues/comments/739646016/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null,
      "user": {
        "login": "djrobstep",
        "id": 2441221,
        "site_admin": false
      }
    },
    {
      "url": "https://api.github.com/repos/psf/requests/issues/comments/751926306",
      "id": 751926306,
      "created_at": "2020-12-29T02:42:31Z",
      "updated_at": "2020-12-29T02:42:31Z",
      "author_association": "NONE",
      "body": "As promised, here's a reproducible example against httpbin.org:\r\n\r\n```\r\nimport requests\r\nchunk_size = None\r\n\r\nURL = 'https://httpbin.org/drip?duration=2'\r\n\r\nr = requests.get(URL, stream=True)\r\n\r\nfor x in r.iter_content(chunk_size=chunk_size):\r\n    print(f'response: {x}')\r\n```\r\n\r\nRun this and you'll see that `iter_content` waits until the request is fully complete to return anything.\r\n\r\nChange the chunk_size to 1 and everything works nicely (albeit with high overhead).\r\n\r\nIf somebody can point me in the right direction, I'm happy to investigate this and do what is required to fix it.",
      "reactions": {
        "url": "https://api.github.com/repos/psf/requests/issues/comments/751926306/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null,
      "user": {
        "login": "djrobstep",
        "id": 2441221,
        "site_admin": false
      }
    },
    {
      "url": "https://api.github.com/repos/psf/requests/issues/comments/1083354701",
      "id": 1083354701,
      "created_at": "2022-03-30T16:23:37Z",
      "updated_at": "2022-03-30T16:23:37Z",
      "author_association": "NONE",
      "body": "Any resolution to this? I am also still seeing this on v2.25.1",
      "reactions": {
        "url": "https://api.github.com/repos/psf/requests/issues/comments/1083354701/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null,
      "user": {
        "login": "stephen-goveia",
        "id": 27365528,
        "site_admin": false
      }
    },
    {
      "url": "https://api.github.com/repos/psf/requests/issues/comments/1083394590",
      "id": 1083394590,
      "created_at": "2022-03-30T17:04:33Z",
      "updated_at": "2022-03-30T17:04:33Z",
      "author_association": "MEMBER",
      "body": "Hi @stephen-goveia, this is a behavior in urllib3 as noted in urllib3/urllib3#2123. We aren't able to change it in Requests, so the outcome will be determined by whether this makes it into the urllib3 v2 release.",
      "reactions": {
        "url": "https://api.github.com/repos/psf/requests/issues/comments/1083394590/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null,
      "user": {
        "login": "nateprewitt",
        "id": 5271761,
        "site_admin": false
      }
    },
    {
      "url": "https://api.github.com/repos/psf/requests/issues/comments/1083436628",
      "id": 1083436628,
      "created_at": "2022-03-30T17:47:50Z",
      "updated_at": "2022-03-30T17:47:50Z",
      "author_association": "NONE",
      "body": "thanks @nateprewitt!",
      "reactions": {
        "url": "https://api.github.com/repos/psf/requests/issues/comments/1083436628/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null,
      "user": {
        "login": "stephen-goveia",
        "id": 27365528,
        "site_admin": false
      }
    },
    {
      "url": "https://api.github.com/repos/psf/requests/issues/comments/1214813157",
      "id": 1214813157,
      "created_at": "2022-08-15T09:26:52Z",
      "updated_at": "2022-08-18T17:25:54Z",
      "author_association": "NONE",
      "body": "Hi. I don't understand why this issue is still open.\r\nHere is a link to [the official documentation](https://requests.readthedocs.io/en/latest/api/#requests.Response.iter_content).\r\n\r\n> chunk_size must be of type int or None. A value of None will function differently depending on the value of stream. stream=True will read data as it arrives in whatever size the chunks are received. If stream=False, data is returned as a single chunk.\r\n",
      "reactions": {
        "url": "https://api.github.com/repos/psf/requests/issues/comments/1214813157/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null,
      "user": {
        "login": "Karmavil",
        "id": 2354060,
        "site_admin": false
      }
    },
    {
      "url": "https://api.github.com/repos/psf/requests/issues/comments/1221861363",
      "id": 1221861363,
      "created_at": "2022-08-22T05:41:54Z",
      "updated_at": "2022-08-22T05:41:54Z",
      "author_association": "NONE",
      "body": "Even after setting stream=True this is still an issue:\r\n\r\n```py3\r\nimport requests\r\nimport time\r\n\r\nchunk_size = None\r\n\r\nURL = 'https://httpbin.org/drip?duration=20&numbytes=4'\r\n\r\nr = requests.get(URL, stream=True)\r\n\r\nt = time.monotonic()\r\nfor x in r.iter_content(chunk_size=chunk_size):\r\n    t2 = time.monotonic()\r\n    print(f'{t2 - t}')\r\n    t = time.monotonic()    \r\n```\r\nprints:\r\n```\r\n15.593049310147762\r\n```",
      "reactions": {
        "url": "https://api.github.com/repos/psf/requests/issues/comments/1221861363/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null,
      "user": {
        "login": "hexagonrecursion",
        "id": 52621858,
        "site_admin": false
      }
    },
    {
      "url": "https://api.github.com/repos/psf/requests/issues/comments/1222289180",
      "id": 1222289180,
      "created_at": "2022-08-22T12:28:17Z",
      "updated_at": "2022-08-22T12:28:17Z",
      "author_association": "NONE",
      "body": "Please keep in mind that I'm making this comment as a user, not as a contributor.\r\n\r\nYou're right. It is.. but please [read the documentation](https://requests.readthedocs.io/en/latest/api/#requests.Response.iter_content). \r\nAll I'm saying is that the documentation is clear enough, (**or at least it is today**):\r\n>  When stream=True is set on the request, this avoids reading the content at once into memory for large responses\r\n\r\nWhat should the module do when you ask not to download everything at once but to download \"Nothing\"? \r\nShould it throw an error? \r\nShould it not download anything at all?\r\n\r\nJust check the _content-length_ **header** and set a suitable chunk size when dealing with large files",
      "reactions": {
        "url": "https://api.github.com/repos/psf/requests/issues/comments/1222289180/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null,
      "user": {
        "login": "Karmavil",
        "id": 2354060,
        "site_admin": false
      }
    },
    {
      "url": "https://api.github.com/repos/psf/requests/issues/comments/1368028851",
      "id": 1368028851,
      "created_at": "2022-12-30T17:43:57Z",
      "updated_at": "2022-12-30T17:43:57Z",
      "author_association": "NONE",
      "body": "It is not only about large files, it is also about SSE (server sent events). They are streamed, and clients expect them to arrive directly after the server sends them.",
      "reactions": {
        "url": "https://api.github.com/repos/psf/requests/issues/comments/1368028851/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null,
      "user": {
        "login": "bablokb",
        "id": 9638355,
        "site_admin": false
      }
    },
    {
      "url": "https://api.github.com/repos/psf/requests/issues/comments/1673299166",
      "id": 1673299166,
      "created_at": "2023-08-10T14:10:37Z",
      "updated_at": "2023-08-10T14:10:37Z",
      "author_association": "NONE",
      "body": "No movement on this in ~8 months... Any update?",
      "reactions": {
        "url": "https://api.github.com/repos/psf/requests/issues/comments/1673299166/reactions",
        "total_count": 2,
        "+1": 0,
        "-1": 2,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null,
      "user": {
        "login": "dmyoung9",
        "id": 101432365,
        "site_admin": false
      }
    },
    {
      "url": "https://api.github.com/repos/psf/requests/issues/comments/1676582026",
      "id": 1676582026,
      "created_at": "2023-08-14T02:27:04Z",
      "updated_at": "2023-08-14T02:27:04Z",
      "author_association": "NONE",
      "body": "Possible workaround using the `Response.raw.stream()`, seems to work on my end:\r\n\r\n```python\r\nresp = requests.get(\"something\", stream=True)\r\nfor chunk in resp.raw.stream():\r\n   print(f\"chunk size: {len(chunk)}\")\r\n```",
      "reactions": {
        "url": "https://api.github.com/repos/psf/requests/issues/comments/1676582026/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 1,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null,
      "user": {
        "login": "mbhynes",
        "id": 10452129,
        "site_admin": false
      }
    },
    {
      "url": "https://api.github.com/repos/psf/requests/issues/comments/1804043749",
      "id": 1804043749,
      "created_at": "2023-11-09T15:26:01Z",
      "updated_at": "2023-11-09T15:26:01Z",
      "author_association": "NONE",
      "body": "@mbhynes Not sure what you were doing to have that \"work\", but it certainly doesn't do what I'd expect...\r\n\r\n```python\r\nimport requests\r\nurl = \"https://httpbin.org/drip?duration=2&numbytes=8\"\r\nresp = requests.get(url, stream=True)\r\nfor chunk in resp.raw.stream():\r\n   print(f\"chunk size: {len(chunk)}\")\r\n```\r\njust gives me a single 8-byte chunk back after 2 seconds, rather than 8 single byte chunks every few hundred milliseconds.\r\n\r\nI'd assume your endpoint happens to be returning the data via a \"chunked transfer encoding\" which has been able to handle streaming data in chunks for a long time already, but you could check by doing:\r\n```python\r\nprint(resp.headers.get(\"transfer-encoding\"))\r\n```\r\n---\r\nThat said, I've created a pull-request with `urllib3` (https://github.com/urllib3/urllib3/pull/3186) that can be built on to enable streaming in cases like this and I'd hope would allow the normal `iter_content` method to yield data in appropriately sized chunks.",
      "reactions": {
        "url": "https://api.github.com/repos/psf/requests/issues/comments/1804043749/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null,
      "user": {
        "login": "smason",
        "id": 1433005,
        "site_admin": false
      }
    }
  ]
}